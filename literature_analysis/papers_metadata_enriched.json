[
  {
    "paperId": "a6dc30b886c5132aaa716a0494d043b9ebb8cb07",
    "externalIds": {
      "DBLP": "journals/access/TiwaryNFT24",
      "DOI": "10.1109/ACCESS.2024.3422416",
      "CorpusId": 271003412
    },
    "url": "https://www.semanticscholar.org/paper/a6dc30b886c5132aaa716a0494d043b9ebb8cb07",
    "title": "A Review of Explainable Recommender Systems Utilizing Knowledge Graphs and Reinforcement Learning",
    "venue": "IEEE Access",
    "year": 2024,
    "referenceCount": 102,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3422416?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2024.3422416, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2294449489",
        "name": "Neeraj Tiwary"
      },
      {
        "authorId": "118523123",
        "name": "Shahrul Azman Mohd Noah"
      },
      {
        "authorId": "2294423020",
        "name": "Fariza Fauzi"
      },
      {
        "authorId": "9287076",
        "name": "Tan Siok Yee"
      }
    ],
    "abstract": "This review paper addresses the research question of the significance of explainability in AI and the role of integrating KG and RL to enhance Explainable Recommender Systems (XRS). It surveys articles published from January 2015 to March 2024 on XRS, focusing on knowledge graphs (KGs) and reinforcement learning (RL) for achieving explainability in recommender systems. Employing a systematic methodology, it introduces a custom Python-based web scraper to efficiently navigate and extract relevant academic research papers from IEEE, ScienceDirect (Elsevier), ACM, and Springer online databases. The study encompasses the PRISMA methodology to conduct a thorough analysis and identify pertinent research works. This systematic literature review aims to provide a unified view of the field by reviewing eight existing XRS literature reviews and 29 pertinent XRS studies involving KG and RL from the specified period. It categorizes and analyses relevant research papers based on their implementation methodologies and explores significant contributions, encompassing perspectives on model-agnostic and model-intrinsic explanations.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "75841526f5f793aabc1c8895fda6201043fb46e8",
    "externalIds": {
      "DOI": "10.3233/jifs-219416",
      "CorpusId": 269421631
    },
    "url": "https://www.semanticscholar.org/paper/75841526f5f793aabc1c8895fda6201043fb46e8",
    "title": "Mitigating filter bubbles: Diverse and explainable recommender systems",
    "venue": "Journal of Intelligent &amp; Fuzzy Systems",
    "year": 2024,
    "referenceCount": 11,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3233/jifs-219416?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3233/jifs-219416, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-26",
    "authors": [
      {
        "authorId": "2298681384",
        "name": "Umar Tahir Kidwai"
      },
      {
        "authorId": "2220166482",
        "name": "Nadeem Akhtar"
      },
      {
        "authorId": "2298651522",
        "name": "Mohammad Nadeem"
      },
      {
        "authorId": "49004436",
        "name": "Roobaea Alroobaea"
      }
    ],
    "abstract": "In recent years, the surge in online content has necessitated the development of intelligent recommender systems capable of offering personalized suggestions to users. However, these systems often encapsulate users within a “filter bubble”, limiting their exposure to a narrow range of content. This study introduces a novel approach to address this issue by integrating a novel diversity module into a knowledge graph-based explainable recommender system. Utilizing the Movie Lens 1M dataset, this research pioneers in fostering a more nuanced and transparent user experience, thereby enhancing user trust and broadening the spectrum of recommendations. Looking ahead, we aim to further refine this system by incorporating an explicit feedback loop and leveraging Natural Language Processing (NLP) techniques to provide users with insightful explanations of recommendations, including a comprehensive analysis of filter bubbles. This initiative marks a significant stride towards creating a more inclusive and informed recommendation landscape, promising users not only a wider array of content but also a deeper understanding of the recommendation mechanisms at play.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "481442415a7610e9da7ee3c8603d07b8f6b2df3d",
    "externalIds": {
      "DBLP": "conf/recsys/Ariza-CasabonaB24",
      "DOI": "10.1145/3640457.3688069",
      "CorpusId": 273217851
    },
    "url": "https://www.semanticscholar.org/paper/481442415a7610e9da7ee3c8603d07b8f6b2df3d",
    "title": "A Comparative Analysis of Text-Based Explainable Recommender Systems",
    "venue": "ACM Conference on Recommender Systems",
    "year": 2024,
    "referenceCount": 51,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3640457.3688069",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3640457.3688069?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3640457.3688069, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2024-10-08",
    "authors": [
      {
        "authorId": "2240551453",
        "name": "Alejandro Ariza-Casabona"
      },
      {
        "authorId": "1824224",
        "name": "Ludovico Boratto"
      },
      {
        "authorId": "2035668",
        "name": "Maria Salamó"
      }
    ],
    "abstract": "One way to increase trust among users towards recommender systems is to provide the recommendation along with a textual explanation. In the literature, extraction-based, generation-based, and, more recently, hybrid solutions based on retrieval-augmented generation have been proposed to tackle the problem of text-based explainable recommendation. However, the use of different datasets, preprocessing steps, target explanations, baselines, and evaluation metrics complicates the reproducibility and state-of-the-art assessment of previous work among different model categories for successful advancements in the field. Our aim is to provide a comprehensive analysis of text-based explainable recommender systems by setting up a well-defined benchmark that accommodates generation-based, extraction-based, and hybrid approaches. Also, we enrich the existing evaluation of explainability and text quality of the explanations with a novel definition of feature hallucination. Our experiments on three real-world datasets unveil hidden behaviors and confirm several claims about model patterns. Our source code and preprocessed datasets are available at https://github.com/alarca94/text-exp-recsys24.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "1cb0d0397404f2d94e4b2d201034f108ec7fde6a",
    "externalIds": {
      "DBLP": "conf/vissym/HazwaniAWIEB23",
      "DOI": "10.2312/evp.20231062",
      "CorpusId": 259282199
    },
    "url": "https://www.semanticscholar.org/paper/1cb0d0397404f2d94e4b2d201034f108ec7fde6a",
    "title": "Interaction Tasks for Explainable Recommender Systems",
    "venue": "Eurographics Conference on Visualization",
    "year": 2023,
    "referenceCount": 28,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.2312/evp.20231062?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2312/evp.20231062, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "33828586",
        "name": "M. Krone"
      },
      {
        "authorId": "2220681380",
        "name": "S. Lenti"
      },
      {
        "authorId": "2217797563",
        "name": "I. Al-Hazwani"
      },
      {
        "authorId": "41078761",
        "name": "Turki S Alahmadi"
      },
      {
        "authorId": "2325155946",
        "name": "Kathrin Wardatzky"
      },
      {
        "authorId": "2692909",
        "name": "Oana Inel"
      },
      {
        "authorId": "1401917601",
        "name": "Mennatallah El-Assady"
      },
      {
        "authorId": "2052846593",
        "name": "J. Bernard"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 8
  },
  {
    "paperId": "5bb0150f1a73e174642675610038cc89bd4b3ca6",
    "externalIds": {
      "DBLP": "journals/kais/CaroMartinezJR23",
      "DOI": "10.1007/s10115-023-01903-9",
      "CorpusId": 258925478
    },
    "url": "https://www.semanticscholar.org/paper/5bb0150f1a73e174642675610038cc89bd4b3ca6",
    "title": "A graph-based approach for minimising the knowledge requirement of explainable recommender systems",
    "venue": "Knowledge and Information Systems",
    "year": 2023,
    "referenceCount": 64,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://link.springer.com/content/pdf/10.1007/s10115-023-01903-9.pdf",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10115-023-01903-9?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10115-023-01903-9, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-25",
    "authors": [
      {
        "authorId": "1410523548",
        "name": "Marta Caro-Martínez"
      },
      {
        "authorId": "83898594",
        "name": "Guillermo Jiménez-Díaz"
      },
      {
        "authorId": "1389962946",
        "name": "Juan A. Recio-García"
      }
    ],
    "abstract": "Traditionally, recommender systems use collaborative filtering or content-based approaches based on ratings and item descriptions. However, this information is unavailable in many domains and applications, and recommender systems can only tackle the problem using information about interactions or implicit knowledge. Within this scenario, this work proposes a novel approach based on link prediction techniques over graph structures that exclusively considers interactions between users and items to provide recommendations. We present and evaluate two alternative recommendation methods: one item-based and one user-based that apply the edge weight , common neighbours , Jaccard neighbours , Adar/Adamic , and Preferential Attachment link prediction techniques. This approach has two significant advantages, which are the novelty of our proposal. First, it is suitable for minimal knowledge scenarios where explicit data such as ratings or preferences are not available. However, as our evaluation demonstrates, this approach outperforms state-of-the-art techniques using a similar level of interaction knowledge. Second, our approach has another relevant feature regarding one of the most significant concerns in current artificial intelligence research: the recommendation methods presented in this paper are easily interpretable for the users, improving their trust in the recommendations.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "618ed65eee6f1e67a60f6213431f4a6cf261c4ff",
    "externalIds": {
      "DBLP": "journals/cim/CobaCZ22",
      "DOI": "10.1109/MCI.2021.3129958",
      "CorpusId": 245881430
    },
    "url": "https://www.semanticscholar.org/paper/618ed65eee6f1e67a60f6213431f4a6cf261c4ff",
    "title": "RecoXplainer: A Library for Development and Offline Evaluation of Explainable Recommender Systems",
    "venue": "IEEE Computational Intelligence Magazine",
    "year": 2022,
    "referenceCount": 0,
    "citationCount": 11,
    "openAccessPdf": {
      "url": "https://www.research.unipd.it/bitstream/11577/3471609/1/2022_recoXplainer_A_Library_for_Development_and_Offline_Explainable_Recommender_Systems.pdf",
      "status": "GREEN",
      "license": "other-oa",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/MCI.2021.3129958?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/MCI.2021.3129958, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-02-01",
    "authors": [
      {
        "authorId": "51161531",
        "name": "Ludovik Çoba"
      },
      {
        "authorId": "144833154",
        "name": "R. Confalonieri"
      },
      {
        "authorId": "1694617",
        "name": "M. Zanker"
      }
    ],
    "abstract": "Since recommender systems play an important role in our online experience today and are involved in a wide range of decisions, multiple stakeholders are requesting explanations for the corresponding algorithmic predictions. These demands—together with the benefits of explanations (e.g., trust, efficiency, and sometimes even persuasion)—have triggered significant interest from researchers in academia and in industry. Nonetheless, to the best of our knowledge, no comprehensive toolkit for development and evaluation of explainable recommender systems is available to the community yet. Instead, researchers are frequently faced with the challenge of re-implementing prior algorithms when creating and evaluating new approaches. This paper introduces recoXplainer, an easy-to-use, unified and extendable library that supports the development and evaluation of explainable recommender systems. recoXplainer includes several state-of-the-art black box algorithms, model-based and post-hoc explainability techniques, as well as offline evaluation metrics in order to assess the quality of the explanation algorithms.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "c82f338abf4a981c8e884dd048dfce927bec9f87",
    "externalIds": {
      "DBLP": "conf/extraamas/HulstijnTNA23",
      "DOI": "10.1007/978-3-031-40878-6_12",
      "CorpusId": 261894142
    },
    "url": "https://www.semanticscholar.org/paper/c82f338abf4a981c8e884dd048dfce927bec9f87",
    "title": "Metrics for Evaluating Explainable Recommender Systems",
    "venue": "EXTRAAMAS",
    "year": 2023,
    "referenceCount": 0,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-031-40878-6_12?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-031-40878-6_12, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "145539910",
        "name": "J. Hulstijn"
      },
      {
        "authorId": "51221370",
        "name": "I. Tchappi"
      },
      {
        "authorId": "2297347",
        "name": "A. Najjar"
      },
      {
        "authorId": "2060698",
        "name": "Reyhan Aydoğan"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "1be7b168f9921134be97640c474f748a71dc25ad",
    "externalIds": {
      "DBLP": "conf/recsys/BalloccuBFM22",
      "DOI": "10.1145/3523227.3547374",
      "CorpusId": 252216461
    },
    "url": "https://www.semanticscholar.org/paper/1be7b168f9921134be97640c474f748a71dc25ad",
    "title": "Hands on Explainable Recommender Systems with Knowledge Graphs",
    "venue": "ACM Conference on Recommender Systems",
    "year": 2022,
    "referenceCount": 26,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3523227.3547374",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3523227.3547374?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3523227.3547374, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Review"
    ],
    "publicationDate": "2022-09-18",
    "authors": [
      {
        "authorId": "2163452228",
        "name": "Giacomo Balloccu"
      },
      {
        "authorId": "1824224",
        "name": "Ludovico Boratto"
      },
      {
        "authorId": "40433308",
        "name": "G. Fenu"
      },
      {
        "authorId": "28922901",
        "name": "Mirko Marras"
      }
    ],
    "abstract": "The goal of this tutorial is to present the RecSys community with recent advances on explainable recommender systems with knowledge graphs. We will first introduce conceptual foundations, by surveying the state of the art and describing real-world examples of how knowledge graphs are being integrated into the recommendation pipeline, also for the purpose of providing explanations. This tutorial will continue with a systematic presentation of algorithmic solutions to model, integrate, train, and assess a recommender system with knowledge graphs, with particular attention to the explainability perspective. A practical part will then provide attendees with concrete implementations of recommender systems with knowledge graphs, leveraging open-source tools and public datasets; in this part, tutorial participants will be engaged in the design of explanations accompanying the recommendations and in articulating their impact. We conclude the tutorial by analyzing emerging open issues and future directions. Website: https://explainablerecsys.github.io/recsys2022/.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "1379d7b6bab33147c2cf4e1e55fc5f73b4a00dcb",
    "externalIds": {
      "DBLP": "conf/icde/WangLCLFLXSLGC22",
      "DOI": "10.1109/icde53745.2022.00309",
      "CorpusId": 251289647
    },
    "url": "https://www.semanticscholar.org/paper/1379d7b6bab33147c2cf4e1e55fc5f73b4a00dcb",
    "title": "Tower Bridge Net (TB-Net): Bidirectional Knowledge Graph Aware Embedding Propagation for Explainable Recommender Systems",
    "venue": "IEEE International Conference on Data Engineering",
    "year": 2022,
    "referenceCount": 24,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/icde53745.2022.00309?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/icde53745.2022.00309, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-05-01",
    "authors": [
      {
        "authorId": "2151225434",
        "name": "Shendi Wang"
      },
      {
        "authorId": "2382824304",
        "name": "Haoyang Li"
      },
      {
        "authorId": "3151540",
        "name": "Caleb Chen Cao"
      },
      {
        "authorId": "2141165316",
        "name": "Xiao-Hui Li"
      },
      {
        "authorId": "101750640",
        "name": "Ng Ngai Fai"
      },
      {
        "authorId": "2180326741",
        "name": "Jianxin Liu"
      },
      {
        "authorId": "2111808280",
        "name": "Xun Xue"
      },
      {
        "authorId": "2115232192",
        "name": "H. Song"
      },
      {
        "authorId": "2180539149",
        "name": "Jinyu Li"
      },
      {
        "authorId": "2180234851",
        "name": "Guangye Gu"
      },
      {
        "authorId": "2593276",
        "name": "Lei Chen"
      }
    ],
    "abstract": "Recently, neural networks based models have been widely used for recommender systems (RS). Unfortunately, the existing neural network based RS solutions are often treated as black-boxes, which gain little trust and confidence from users. Thus, there is an increasing demand of explainability. Several explainable recommendation methods have been introduced to RS. However, there is a trade-off between explainability and performance among these methods. In this paper, we propose a novel framework, the Tower Bridge Net (TB-Net), using the proposed bidirectional embedding propagation approach to achieve both superior recommendation and explainability performances. Extensive validation on three public datasets shows that the performance of TB-Net dominates the state-of-the-art models. We quantitatively evaluate the explainability by using numerical metrics and experimentally prove that TB-Net achieves a significant improvement on explainability compared with existing methods. More importantly, TB-Net has been deployed and offers explainable recommendation service for the largest bank in China, Industrial and Commercial Bank of China Limited (ICBC). Results on a billion-scale dataset (1.2 billion nodes and edges) from ICBC show that TB-Net can provide both accurate recommendations and semantic explanations, and is very effective and deployable in practice.",
    "affiliations": [],
    "countries": [],
    "num_authors": 11
  },
  {
    "paperId": "0a6f0e30cca5fa3a291a55a4ddff44e569c89e97",
    "externalIds": {
      "DBLP": "journals/jair/Caro-MartinezJR21",
      "DOI": "10.1613/jair.1.12789",
      "CorpusId": 236469004
    },
    "url": "https://www.semanticscholar.org/paper/0a6f0e30cca5fa3a291a55a4ddff44e569c89e97",
    "title": "Conceptual Modeling of Explainable Recommender Systems: An Ontological Formalization to Guide Their Design and Development",
    "venue": "Journal of Artificial Intelligence Research",
    "year": 2021,
    "referenceCount": 85,
    "citationCount": 13,
    "openAccessPdf": {
      "url": "https://jair.org/index.php/jair/article/download/12789/26699",
      "status": "GOLD",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1613/jair.1.12789?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1613/jair.1.12789, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1410523548",
        "name": "Marta Caro-Martínez"
      },
      {
        "authorId": "83898594",
        "name": "Guillermo Jiménez-Díaz"
      },
      {
        "authorId": "1389962946",
        "name": "Juan A. Recio-García"
      }
    ],
    "abstract": "With the increasing importance of e-commerce and the immense variety of products, users need help to decide which ones are the most interesting to them. This is one of the main goals of recommender systems. However, users’ trust may be compromised if they do not understand how or why the recommendation was achieved. Here, explanations are essential to improve user confidence in recommender systems and to make the recommendation useful. \nProviding explanation capabilities into recommender systems is not an easy task as their success depends on several aspects such as the explanation’s goal, the user’s expectation, the knowledge available, or the presentation method. Therefore, this work proposes a conceptual model to alleviate this problem by defining the requirements of explanations for recommender systems. Our goal is to provide a model that guides the development of effective explanations for recommender systems as they are correctly designed and suited to the user’s needs. Although earlier explanation taxonomies sustain this work, our model includes new concepts not considered in previous works. Moreover, we make a novel contribution regarding the formalization of this model as an ontology that can be integrated into the development of proper explanations for recommender systems.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "177427d07f2e7b54dbf9657d565cd51734eb0d10",
    "externalIds": {
      "MAG": "3081144008",
      "ArXiv": "2008.09316",
      "DBLP": "journals/corr/abs-2008-09316",
      "DOI": "10.1145/3340531.3411919",
      "CorpusId": 221245973
    },
    "url": "https://www.semanticscholar.org/paper/177427d07f2e7b54dbf9657d565cd51734eb0d10",
    "title": "Explainable Recommender Systems via Resolving Learning Representations",
    "venue": "International Conference on Information and Knowledge Management",
    "year": 2020,
    "referenceCount": 57,
    "citationCount": 24,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2008.09316",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2008.09316, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2020-08-21",
    "authors": [
      {
        "authorId": "47717322",
        "name": "Ninghao Liu"
      },
      {
        "authorId": "144143228",
        "name": "Yong Ge"
      },
      {
        "authorId": "2156060357",
        "name": "Li Li"
      },
      {
        "authorId": "2283100826",
        "name": "X. Hu"
      },
      {
        "authorId": "1562383795",
        "name": "Rui Chen"
      },
      {
        "authorId": "2108645988",
        "name": "Soo-Hyun Choi"
      }
    ],
    "abstract": "Recommender systems play a fundamental role in web applications in filtering massive information and matching user interests. While many efforts have been devoted to developing more effective models in various scenarios, the exploration on the explainability of recommender systems is running behind. Explanations could help improve user experience and discover system defects. In this paper, after formally introducing the elements that are related to model explainability, we propose a novel explainable recommendation model through improving the transparency of the representation learning process. Specifically, to overcome the representation entangling problem in traditional models, we revise traditional graph convolution to discriminate information from different layers. Also, each representation vector is factorized into several segments, where each segment relates to one semantic aspect in data. Different from previous work, in our model, factor discovery and representation learning are simultaneously conducted, and we are able to handle extra attribute information and knowledge. In this way, the proposed model can learn interpretable and meaningful representations for users and items. Unlike traditional methods that need to make a trade-off between explainability and effectiveness, the performance of our proposed explainable model is not negatively affected after considering explainability. Finally, comprehensive experiments are conducted to validate the performance of our model as well as explanation faithfulness.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "a1eb6dd5bf57162f365f98717b9fc3882fe94025",
    "externalIds": {
      "MAG": "2805655805",
      "DBLP": "series/hci/AbdollahiN18",
      "DOI": "10.1007/978-3-319-90403-0_2",
      "CorpusId": 52196572
    },
    "url": "https://www.semanticscholar.org/paper/a1eb6dd5bf57162f365f98717b9fc3882fe94025",
    "title": "Transparency in Fair Machine Learning: the Case of Explainable Recommender Systems",
    "venue": "Human and Machine Learning",
    "year": 2018,
    "referenceCount": 48,
    "citationCount": 68,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-90403-0_2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-90403-0_2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "4175388",
        "name": "B. Abdollahi"
      },
      {
        "authorId": "2423522",
        "name": "O. Nasraoui"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "6b82110ecae66a8bc32ff3f928cd2c1e40294db0",
    "externalIds": {
      "MAG": "2946816105",
      "DBLP": "conf/icaisc/RutkowskiLNNG19",
      "DOI": "10.1007/978-3-030-20912-4_34",
      "CorpusId": 167205114
    },
    "url": "https://www.semanticscholar.org/paper/6b82110ecae66a8bc32ff3f928cd2c1e40294db0",
    "title": "On Explainable Recommender Systems Based on Fuzzy Rule Generation Techniques",
    "venue": "International Conference on Artificial Intelligence and Soft Computing",
    "year": 2019,
    "referenceCount": 21,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-030-20912-4_34?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-030-20912-4_34, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2019-06-16",
    "authors": [
      {
        "authorId": "2057189914",
        "name": "Tomasz Rutkowski"
      },
      {
        "authorId": "2127620",
        "name": "Krystian Lapa"
      },
      {
        "authorId": "145652012",
        "name": "R. Nowicki"
      },
      {
        "authorId": "1726788",
        "name": "R. Nielek"
      },
      {
        "authorId": "2455940",
        "name": "K. Grzanek"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "903542ef41d5e33c82aceef692c85437e5dcafe5",
    "externalIds": {
      "MAG": "2998919392",
      "DBLP": "conf/bdiot2/SamihAB19",
      "DOI": "10.1145/3372938.3372959",
      "CorpusId": 211040827
    },
    "url": "https://www.semanticscholar.org/paper/903542ef41d5e33c82aceef692c85437e5dcafe5",
    "title": "Towards a knowledge based Explainable Recommender Systems",
    "venue": "International Conference on Big Data and Internet of Things",
    "year": 2019,
    "referenceCount": 15,
    "citationCount": 27,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3372938.3372959?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3372938.3372959, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2019-10-23",
    "authors": [
      {
        "authorId": "2342422936",
        "name": "Amina Samih"
      },
      {
        "authorId": "9139705",
        "name": "Amina Adadi"
      },
      {
        "authorId": "50487490",
        "name": "M. Berrada"
      }
    ],
    "abstract": "Most current Machine Learning based recommender systems act like black boxes, not offering the user any insight into the system logic or justification for the recommendations. Thus, risking losing trust with users and failing to achieve acceptance. The goal of this work is to improve the explainability of recommender systems by using a knowledge extraction method.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "4fbc63683b4d79c53488a8f98aa1e397e89426ed",
    "externalIds": {
      "MAG": "2883679850",
      "DBLP": "conf/recsys/BelliniSNRS18",
      "ArXiv": "1807.06300",
      "DOI": "10.1145/3270323.3270327",
      "CorpusId": 49867535
    },
    "url": "https://www.semanticscholar.org/paper/4fbc63683b4d79c53488a8f98aa1e397e89426ed",
    "title": "Knowledge-aware Autoencoders for Explainable Recommender Systems",
    "venue": "DLRS@RecSys",
    "year": 2018,
    "referenceCount": 25,
    "citationCount": 41,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.06300, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2018-07-17",
    "authors": [
      {
        "authorId": "31614629",
        "name": "Vito Bellini"
      },
      {
        "authorId": "10668128",
        "name": "Angelo Schiavone"
      },
      {
        "authorId": "1737962",
        "name": "T. D. Noia"
      },
      {
        "authorId": "1738932",
        "name": "Azzurra Ragone"
      },
      {
        "authorId": "1738818",
        "name": "E. Sciascio"
      }
    ],
    "abstract": "Recommender Systems have been widely used to help users in finding what they are looking for thus tackling the information overload problem. After several years of research and industrial findings looking after better algorithms to improve accuracy and diversity metrics, explanation services for recommendation are gaining momentum as a tool to provide a human-understandable feedback to results computed, in most of the cases, by black-box machine learning techniques. As a matter of fact, explanations may guarantee users satisfaction, trust, and loyalty in a system. In this paper, we evaluate how different information encoded in a Knowledge Graph are perceived by users when they are adopted to show them an explanation. More precisely, we compare how the use of categorical information, factual one or a mixture of them both in building explanations, affect explanatory criteria for a recommender system. Experimental results are validated through an A/B testing platform which uses a recommendation engine based on a Semantics-Aware Autoencoder to build users profiles which are in turn exploited to compute recommendation lists and to provide an explanation.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "adb465db0731e405c7c7d7a588cf2ea1e48074e1",
    "externalIds": {
      "CorpusId": 212735902
    },
    "url": "https://www.semanticscholar.org/paper/adb465db0731e405c7c7d7a588cf2ea1e48074e1",
    "title": "PhD position in Explainable Recommender Systems",
    "venue": "",
    "year": 2020,
    "referenceCount": 0,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null
    },
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2024649509",
        "name": "A. Chauvin"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 1
  },
  {
    "paperId": "04304f657d01c09ff0f59f02de1d8e116c296450",
    "externalIds": {
      "DBLP": "journals/umuai/CamposFH24",
      "DOI": "10.1007/s11257-024-09400-6",
      "CorpusId": 270326628
    },
    "url": "https://www.semanticscholar.org/paper/04304f657d01c09ff0f59f02de1d8e116c296450",
    "title": "An explainable content-based approach for recommender systems: a case study in journal recommendation for paper submission",
    "venue": "User Model. User Adapt. Interact.",
    "year": 2024,
    "referenceCount": 50,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "https://link.springer.com/content/pdf/10.1007/s11257-024-09400-6.pdf",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11257-024-09400-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11257-024-09400-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-06",
    "authors": [
      {
        "authorId": "2314816151",
        "name": "Luis M. de Campos"
      },
      {
        "authorId": "1397395901",
        "name": "J. M. Fernández-Luna"
      },
      {
        "authorId": "1690193",
        "name": "J. Huete"
      }
    ],
    "abstract": "Explainable artificial intelligence is becoming increasingly important in new artificial intelligence developments since it enables users to understand and consequently trust system output. In the field of recommender systems, explanation is necessary not only for such understanding and trust but also because if users understand why the system is making certain suggestions, they are more likely to consume the recommended product. This paper proposes a novel approach for explaining content-based recommender systems by specifically focusing on publication venue recommendation. In this problem, the authors of a new research paper receive recommendations about possible journals (or other publication venues) to which they could submit their article based on content similarity, while the recommender system simultaneously explains its decisions. The proposed explanation ecosystem is based on various elements that support the explanation (topics, related articles, relevant terms, etc.) and is fully integrated with the underlying recommendation model. The proposed method is evaluated through a user study in the biomedical field, where transparency, satisfaction, trust, and scrutability are assessed. The obtained results suggest that the proposed approach is effective and useful for explaining the output of the recommender system to users.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "4e5385d6d4afd2f2b00a8d5998b6b61339e191f9",
    "externalIds": {
      "DBLP": "journals/corr/abs-2410-00654",
      "ArXiv": "2410.00654",
      "DOI": "10.1145/3640457.3688014",
      "CorpusId": 273022801
    },
    "url": "https://www.semanticscholar.org/paper/4e5385d6d4afd2f2b00a8d5998b6b61339e191f9",
    "title": "Explainable Multi-Stakeholder Job Recommender Systems",
    "venue": "ACM Conference on Recommender Systems",
    "year": 2024,
    "referenceCount": 19,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3640457.3688014",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.00654, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2024-10-01",
    "authors": [
      {
        "authorId": "2187199259",
        "name": "Roan Schellingerhout"
      }
    ],
    "abstract": "Public opinion on recommender systems has become increasingly wary in recent years. In line with this trend, lawmakers have also started to become more critical of such systems, resulting in the introduction of new laws focusing on aspects such as privacy, fairness, and explainability for recommender systems and AI at large. These concepts are especially crucial in high-risk domains such as recruitment. In recruitment specifically, decisions carry substantial weight, as the outcomes can significantly impact individuals’ careers and companies’ success. Additionally, there is a need for a multi-stakeholder approach, as these systems are used by job seekers, recruiters, and companies simultaneously, each with its own requirements and expectations. In this paper, I summarize my current research on the topic of explainable, multi-stakeholder job recommender systems and set out a number of future research directions.",
    "affiliations": [],
    "countries": [],
    "num_authors": 1
  },
  {
    "paperId": "72986cada4a8269041f1b153b08c51f8707afd91",
    "externalIds": {
      "ArXiv": "2305.18363",
      "DBLP": "conf/sigir/GuoZSRCR23",
      "DOI": "10.1145/3539618.3591884",
      "CorpusId": 258967706
    },
    "url": "https://www.semanticscholar.org/paper/72986cada4a8269041f1b153b08c51f8707afd91",
    "title": "Towards Explainable Conversational Recommender Systems",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2023,
    "referenceCount": 47,
    "citationCount": 20,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2305.18363",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.18363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-27",
    "authors": [
      {
        "authorId": "2119112577",
        "name": "Shuyu Guo"
      },
      {
        "authorId": "2108032328",
        "name": "Shuo Zhang"
      },
      {
        "authorId": "2153198380",
        "name": "Weiwei Sun"
      },
      {
        "authorId": "1749477",
        "name": "Pengjie Ren"
      },
      {
        "authorId": "1721165",
        "name": "Zhumin Chen"
      },
      {
        "authorId": "2780667",
        "name": "Z. Ren"
      }
    ],
    "abstract": "Explanations in conventional recommender systems have demonstrated benefits in helping the user understand the rationality of the recommendations and improving the system's efficiency, transparency, and trustworthiness. In the conversational environment, multiple contextualized explanations need to be generated, which poses further challenges for explanations. To better measure explainability in CRS, we propose ten evaluation perspectives based on the concepts from conventional recommender systems together with the characteristics of CRS. We assess five existing CRS benchmark datasets using these metrics and observe the necessity of improving the explanation quality of CRS. To achieve this, we conduct manual and automatic approaches to extend these dialogues and construct a new CRS dataset, namely Explainable Recommendation Dialogues (E-ReDial). It includes 756 dialogues with over 2,000 high-quality rewritten explanations. We compare two baseline approaches to perform explanation generation based on E-ReDial. Experimental results suggest that models trained on E-ReDial can significantly improve explainability while introducing knowledge into the models can further improve the performance. GPT-3 in the in-context learning setting can generate more realistic and diverse movie descriptions. In contrast, T5 training on E-Redial can better generate clear reasons for recommendations based on user preferences. E-ReDial is available at https://github.com/Superbooming/E-ReDial.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "5f7e543af1fa5f5e836e9789335f9b23e576d2bc",
    "externalIds": {
      "DOI": "10.5206/cjils-rcsib.v46i2.15723",
      "CorpusId": 265081254
    },
    "url": "https://www.semanticscholar.org/paper/5f7e543af1fa5f5e836e9789335f9b23e576d2bc",
    "title": "Using folk theories of recommender systems to inform human-centered explainable AI (HCXAI)",
    "venue": "Canadian journal of information and library science",
    "year": 2023,
    "referenceCount": 0,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://ojs.lib.uwo.ca/index.php/cjils/article/download/15723/13030",
      "status": "HYBRID",
      "license": "CCBYNCND",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5206/cjils-rcsib.v46i2.15723?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5206/cjils-rcsib.v46i2.15723, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-06",
    "authors": [
      {
        "authorId": "2264939653",
        "name": "Michael Ridley"
      }
    ],
    "abstract": "This study uses folk theories of the Spotify music recommender system to inform the principles of human-centered explainable AI (HCXAI). The results show that folk theories can reinforce, challenge, and augment these principles facilitating the development of more transparent and explainable recommender systems for the non-expert, lay public.",
    "affiliations": [],
    "countries": [],
    "num_authors": 1
  },
  {
    "paperId": "bfc22ef20dee9cbbeabe1363c044cb498e417880",
    "externalIds": {
      "DBLP": "journals/tors/MarkchomLF25",
      "DOI": "10.1145/3625828",
      "CorpusId": 263147330
    },
    "url": "https://www.semanticscholar.org/paper/bfc22ef20dee9cbbeabe1363c044cb498e417880",
    "title": "Explainable Meta-Path Based Recommender Systems",
    "venue": "Trans. Recomm. Syst.",
    "year": 2023,
    "referenceCount": 56,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3625828",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3625828?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3625828, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-28",
    "authors": [
      {
        "authorId": "52196310",
        "name": "Thanet Markchom"
      },
      {
        "authorId": "2266015545",
        "name": "Huizhi Liang"
      },
      {
        "authorId": "2248238572",
        "name": "James Ferryman"
      }
    ],
    "abstract": "Meta-paths have been popularly used to provide explainability in recommendations. Although long/complicated meta-paths could represent complex user-item connectivity, they are not easy to interpret. This work tackles this problem by introducing a meta-path translation task. The objective is to translate a meta-path to its comparable explainable meta-paths that perform similarly in terms of recommendation but have higher explainability compared to the given one. We propose a definition of meta-path explainability to determine comparable explainable meta-paths and a meta-path grammar that allows comparable explainable meta-paths to be formed in a similar way as sentences in human languages. Based on this grammar, we propose a meta-path translation model, a sequence-to-sequence (Seq2Seq) model to translate a long and complicated meta-path to its comparable explainable meta-paths. Two novel datasets for meta-path translation were generated based on two real-world recommendation datasets. The experiments were conducted on these generated datasets. The results show that our model outperformed state-of-the-art Seq2Seq baselines regarding meta-path translation and maintained a better trade-off between accuracy and diversity/readability in predicting comparable explainable meta-paths. These results indicate that our model can effectively generate a group of explainable meta-paths as alternative explanations for those recommendations based on any given long/complicated meta-path.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "7491eb4fa2cb27daa4dbd3d0ec838cc7b6863692",
    "externalIds": {
      "DBLP": "journals/kbs/MarkchomLF23",
      "DOI": "10.1016/j.knosys.2023.110258",
      "CorpusId": 255904948
    },
    "url": "https://www.semanticscholar.org/paper/7491eb4fa2cb27daa4dbd3d0ec838cc7b6863692",
    "title": "Scalable and explainable visually-aware recommender systems",
    "venue": "Knowledge-Based Systems",
    "year": 2023,
    "referenceCount": 44,
    "citationCount": 13,
    "openAccessPdf": {
      "url": "https://eprints.ncl.ac.uk/file_store/production/288862/9A3DC25D-87FD-4A72-A74D-89F229D08788.pdf",
      "status": "GREEN",
      "license": "other-oa",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.knosys.2023.110258?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.knosys.2023.110258, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-03-01",
    "authors": [
      {
        "authorId": "52196310",
        "name": "Thanet Markchom"
      },
      {
        "authorId": "34727566",
        "name": "Huizhi Liang"
      },
      {
        "authorId": "113273965",
        "name": "James T Ferryman"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "63a632afd5027eeb23fb18357c6eec414c4f4818",
    "externalIds": {
      "DBLP": "conf/webi/GaoZ22",
      "DOI": "10.1109/WI-IAT55865.2022.00115",
      "CorpusId": 258327156
    },
    "url": "https://www.semanticscholar.org/paper/63a632afd5027eeb23fb18357c6eec414c4f4818",
    "title": "A Survey of Explainable E-Commerce Recommender Systems",
    "venue": "2022 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)",
    "year": 2022,
    "referenceCount": 44,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/WI-IAT55865.2022.00115?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/WI-IAT55865.2022.00115, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Conference",
      "Review"
    ],
    "publicationDate": "2022-11-01",
    "authors": [
      {
        "authorId": "1832848446",
        "name": "Huaqi Gao"
      },
      {
        "authorId": "2215370366",
        "name": "Shunke Zhou"
      }
    ],
    "abstract": "Since the growing information overload has become more and more serious on the Web, especially in the field of e-commerce. The explainable recommender system plays a crucial role in providing users with explanations of recommendations to enhance customer satisfaction and loyalty. In recent years, various explainable recommender approaches have been proposed and applied in e-commerce. This survey will review the development of explainable recommender systems, existing methods to generate explainable recommendations, applications in the e-commerce field, and further discuss future directions that can be incorporated and implemented to improve the quality of explainable recommender systems.",
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "e00e06683dba1b5bda23b8cdb78a31f1d345c018",
    "externalIds": {
      "ArXiv": "2202.06466",
      "DBLP": "journals/corr/abs-2202-06466",
      "CorpusId": 246822806
    },
    "url": "https://www.semanticscholar.org/paper/e00e06683dba1b5bda23b8cdb78a31f1d345c018",
    "title": "Measuring \"Why\" in Recommender Systems: a Comprehensive Survey on the Evaluation of Explainable Recommendation",
    "venue": "arXiv.org",
    "year": 2022,
    "referenceCount": 69,
    "citationCount": 37,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.06466, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2022-02-14",
    "authors": [
      {
        "authorId": "49795005",
        "name": "Xu Chen"
      },
      {
        "authorId": "1591136873",
        "name": "Yongfeng Zhang"
      },
      {
        "authorId": "2113341875",
        "name": "Jingxuan Wen"
      }
    ],
    "abstract": "Explainable recommendation has shown its great advantages for improving recommendation persuasiveness, user satisfaction, system transparency, among others. A fundamental problem of explainable recommendation is how to evaluate the explanations. In the past few years, various evaluation strategies have been proposed. However, they are scattered in different papers, and there lacks a systematic and detailed comparison between them. To bridge this gap, in this paper, we comprehensively review the previous work, and provide different taxonomies for them according to the evaluation perspectives and evaluation methods. Beyond summarizing the previous work, we also analyze the (dis)advantages of existing evaluation methods and provide a series of guidelines on how to select them. The contents of this survey are based on more than 100 papers from top-tier conferences like IJCAI, AAAI, TheWebConf, Recsys, UMAP, and IUI, and their complete summarization are presented at https://shimo.im/sheets/VKrpYTcwVH6KXgdy/MODOC/. With this survey, we finally aim to provide a clear and comprehensive review on the evaluation of explainable recommendation.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "1767850776facae214e643dcb3d1a1aa99aea8c4",
    "externalIds": {
      "DOI": "10.1109/i2ct54291.2022.9824265",
      "CorpusId": 250648722
    },
    "url": "https://www.semanticscholar.org/paper/1767850776facae214e643dcb3d1a1aa99aea8c4",
    "title": "A Comparative Review of Expert Systems, Recommender Systems, and Explainable AI",
    "venue": "2022 IEEE 7th International conference for Convergence in Technology (I2CT)",
    "year": 2022,
    "referenceCount": 0,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/i2ct54291.2022.9824265?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/i2ct54291.2022.9824265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference",
      "Review"
    ],
    "publicationDate": "2022-04-07",
    "authors": [
      {
        "authorId": "152866889",
        "name": "M. Ravi"
      },
      {
        "authorId": "1728262",
        "name": "A. Negi"
      },
      {
        "authorId": "2155000143",
        "name": "Sanjay Chitnis"
      }
    ],
    "abstract": "Previously Expert Systems (ES) dominated Artificial Intelligence (AI) applications and various ES were developed in multiple domains. However, due to knowledge acquisition bottlenecks, these systems fell out of use. With the rise in Machine Learning (ML) and Deep Learning (DL) approaches, another category of systems called Recommender Systems (RS) is now developed for various application domains. As ML/DL systems acted like black boxes, explainable AI (XAI) came into the picture to provide explanations for the recommendations or predictions made. In this paper, we review the architectural similarities and differences between these three approaches along with applications and future directions. It is important to study these to predict the future of RS and any possible resurgence of ES, developments in XAI and application domains.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "21be2210c4a4de7a9ce0cb59e4d0fc28921d68a5",
    "externalIds": {
      "DBLP": "conf/iui/SilvaMSW24",
      "DOI": "10.1145/3640543.3645171",
      "CorpusId": 268984484
    },
    "url": "https://www.semanticscholar.org/paper/21be2210c4a4de7a9ce0cb59e4d0fc28921d68a5",
    "title": "Leveraging ChatGPT for Automated Human-centered Explanations in Recommender Systems",
    "venue": "International Conference on Intelligent User Interfaces",
    "year": 2024,
    "referenceCount": 24,
    "citationCount": 33,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3640543.3645171",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3640543.3645171?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3640543.3645171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2024-03-18",
    "authors": [
      {
        "authorId": "2269452111",
        "name": "Ítallo Silva"
      },
      {
        "authorId": "2159497665",
        "name": "L. Marinho"
      },
      {
        "authorId": "2186119813",
        "name": "Alan Said"
      },
      {
        "authorId": "1918235",
        "name": "M. Willemsen"
      }
    ],
    "abstract": "The adoption of recommender systems (RSs) in various domains has become increasingly popular, but concerns have been raised about their lack of transparency and interpretability. While significant advancements have been made in creating explainable RSs, there is still a shortage of automated approaches that can deliver meaningful and contextual human-centered explanations. Numerous researchers have evaluated explanations based on human-generated recommendations and explanations to address this gap. However, such approaches do not scale for real-world systems. Building on recent research that exploits Large Language Models (LLMs) for RSs, we propose leveraging the conversational capabilities of ChatGPT to provide users with personalized, human-like, and meaningful explanations for recommended items. Our paper presents one of the first user studies that measure users’ perceptions of ChatGPT-generated explanations while acting as an RS. Regarding recommendations, we assess whether users prefer ChatGPT over random (but popular) recommendations. Concerning explanations, we assess users’ perceptions of personalization, effectiveness, and persuasiveness. Our findings reveal that users tend to prefer ChatGPT-generated recommendations over popular ones. Additionally, personalized rather than generic explanations prove to be more effective when the recommended item is unfamiliar.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "c25dd5b15f2225322148c0e29d406c9957f72674",
    "externalIds": {
      "DBLP": "journals/data/DohZATD22",
      "DOI": "10.3390/data7070094",
      "CorpusId": 250544670
    },
    "url": "https://www.semanticscholar.org/paper/c25dd5b15f2225322148c0e29d406c9957f72674",
    "title": "A Systematic Review of Deep Knowledge Graph-Based Recommender Systems, with Focus on Explainable Embeddings",
    "venue": "International Conference on Data Technologies and Applications",
    "year": 2022,
    "referenceCount": 93,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://www.mdpi.com/2306-5729/7/7/94/pdf?version=1658389978",
      "status": "GOLD",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/data7070094?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/data7070094, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2022-07-12",
    "authors": [
      {
        "authorId": "70326112",
        "name": "R. Doh"
      },
      {
        "authorId": "2110616186",
        "name": "Conghua Zhou"
      },
      {
        "authorId": "2059615608",
        "name": "John Kingsley Arthur"
      },
      {
        "authorId": "2379399679",
        "name": "Isaac Tawiah"
      },
      {
        "authorId": "1404440022",
        "name": "Benjamin Doh"
      }
    ],
    "abstract": "Recommender systems (RS) have been developed to make personalized suggestions and enrich users’ preferences in various online applications to address the information explosion problems. However, traditional recommender-based systems act as black boxes, not presenting the user with insights into the system logic or reasons for recommendations. Recently, generating explainable recommendations with deep knowledge graphs (DKG) has attracted significant attention. DKG is a subset of explainable artificial intelligence (XAI) that utilizes the strengths of deep learning (DL) algorithms to learn, provide high-quality predictions, and complement the weaknesses of knowledge graphs (KGs) in the explainability of recommendations. DKG-based models can provide more meaningful, insightful, and trustworthy justifications for recommended items and alleviate the information explosion problems. Although several studies have been carried out on RS, only a few papers have been published on DKG-based methodologies, and a review in this new research direction is still insufficiently explored. To fill this literature gap, this paper uses a systematic literature review framework to survey the recently published papers from 2018 to 2022 in the landscape of DKG and XAI. We analyze how the methods produced in these papers extract essential information from graph-based representations to improve recommendations’ accuracy, explainability, and reliability. From the perspective of the leveraged knowledge-graph related information and how the knowledge-graph or path embeddings are learned and integrated with the DL methods, we carefully select and classify these published works into four main categories: the Two-stage explainable learning methods, the Joint-stage explainable learning methods, the Path-embedding explainable learning methods, and the Propagation explainable learning methods. We further summarize these works according to the characteristics of the approaches and the recommendation scenarios to facilitate the ease of checking the literature. We finally conclude by discussing some open challenges left for future research in this vibrant field.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "229ac921a1ef8a9683821f024a45163f6dcfda94",
    "externalIds": {
      "DBLP": "journals/tiis/AlhejailiF22",
      "DOI": "10.1145/3530299",
      "CorpusId": 248497547
    },
    "url": "https://www.semanticscholar.org/paper/229ac921a1ef8a9683821f024a45163f6dcfda94",
    "title": "Expressive Latent Feature Modelling for Explainable Matrix Factorisation-based Recommender Systems",
    "venue": "ACM Trans. Interact. Intell. Syst.",
    "year": 2022,
    "referenceCount": 43,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://figshare.com/articles/journal_contribution/Expressive_latent_feature_modelling_for_explainable_matrix_factorisation-based_recommender_systems/19497506/1/files/34647659.pdf",
      "status": "GREEN",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3530299?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3530299, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-05-02",
    "authors": [
      {
        "authorId": "114468283",
        "name": "Abdullah Alhejaili"
      },
      {
        "authorId": "2164040526",
        "name": "Shaheen Fatima"
      }
    ],
    "abstract": "The traditional matrix factorisation (MF)-based recommender system methods, despite their success in making the recommendation, lack explainable recommendations as the produced latent features are meaningless and cannot explain the recommendation. This article introduces an MF-based explainable recommender system framework that utilises the user-item rating data and the available item information to model meaningful user and item latent features. These features are exploited to enhance the rating prediction accuracy and the recommendation explainability. Our proposed feature-based explainable recommender system framework utilises these meaningful user and item latent features to explain the recommendation without relying on private or outer data. The recommendations are explained to the user using text message and bar chart. Our proposed model has been evaluated in terms of the rating prediction accuracy and the reasonableness of the explanation using six real-world benchmark datasets for movies, books, video games, and fashion recommendation systems. The results show that the proposed model can produce accurate explainable recommendations.",
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "32566e07da8ae95325238eea068ed8c7fe01578a",
    "externalIds": {
      "ArXiv": "2403.06465",
      "DBLP": "conf/www/LianLHYXX24",
      "DOI": "10.1145/3589335.3651242",
      "CorpusId": 268351536
    },
    "url": "https://www.semanticscholar.org/paper/32566e07da8ae95325238eea068ed8c7fe01578a",
    "title": "RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems",
    "venue": "The Web Conference",
    "year": 2024,
    "referenceCount": 6,
    "citationCount": 15,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3589335.3651242",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.06465, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-11",
    "authors": [
      {
        "authorId": "2813328",
        "name": "Jianxun Lian"
      },
      {
        "authorId": "2226475380",
        "name": "Yuxuan Lei"
      },
      {
        "authorId": "2236672137",
        "name": "Xu Huang"
      },
      {
        "authorId": "2237129499",
        "name": "Jing Yao"
      },
      {
        "authorId": "2267336219",
        "name": "Wei Xu"
      },
      {
        "authorId": "2261275112",
        "name": "Xing Xie"
      }
    ],
    "abstract": "This paper introduces RecAI, a practical toolkit designed to augment or even revolutionize recommender systems with the advanced capabilities of Large Language Models (LLMs). RecAI provides a suite of tools, including Recommender AI Agent, Recommendation-oriented Language Models, Knowledge Plugin, RecExplainer, and Evaluator, to facilitate the integration of LLMs into recommender systems from multifaceted perspectives. The new generation of recommender systems, empowered by LLMs, are expected to be more versatile, explainable, conversational, and controllable, paving the way for more intelligent and user-centric recommendation experiences. We hope the open-source of RecAI can help accelerate evolution of new advanced recommender systems. The source code of RecAI is available at https://github.com/microsoft/RecAI.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "491d4e2f0449f1dfd377a8936f2ccb01a17e6f56",
    "externalIds": {
      "DBLP": "conf/inista/Vultureanu-Albisi21",
      "DOI": "10.1109/INISTA52262.2021.9548125",
      "CorpusId": 238241986
    },
    "url": "https://www.semanticscholar.org/paper/491d4e2f0449f1dfd377a8936f2ccb01a17e6f56",
    "title": "Recommender Systems: An Explainable AI Perspective",
    "venue": "International Symposium on INnovations in Intelligent SysTems and Applications",
    "year": 2021,
    "referenceCount": 0,
    "citationCount": 31,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/INISTA52262.2021.9548125?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/INISTA52262.2021.9548125, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Conference",
      "Review"
    ],
    "publicationDate": "2021-08-25",
    "authors": [
      {
        "authorId": "2091486093",
        "name": "Alexandra Vultureanu-Albişi"
      },
      {
        "authorId": "1740341",
        "name": "C. Bǎdicǎ"
      }
    ],
    "abstract": "In recent years, in the era of information overload development, the need for recommender systems that make personalized suggestion systems has become a very exciting field for researchers. To develop models that generate high-quality recommendations, the explainable recommendation has been introduced, proposing to develop intuitive and trustworthy explanations. The problem that the explainable recommendation wants to solve is to let people understand why certain elements rather than other are recommended by the system. This paper briefly overviews the short history of explainable AI and then it presents its role and applicability in the domain of recommender systems. Our work contributes to understanding the concept of explainable recommendation and what it should accomplish to increase its acceptability and to enable its accurate evaluation.",
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "0cfdd655100055f234fd23ebecd915504b8e00e3",
    "externalIds": {
      "DBLP": "journals/corr/abs-2303-14524",
      "ArXiv": "2303.14524",
      "CorpusId": 257766541
    },
    "url": "https://www.semanticscholar.org/paper/0cfdd655100055f234fd23ebecd915504b8e00e3",
    "title": "Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 28,
    "citationCount": 342,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.14524, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-03-25",
    "authors": [
      {
        "authorId": "1390781327",
        "name": "Yunfan Gao"
      },
      {
        "authorId": "2169937292",
        "name": "Tao Sheng"
      },
      {
        "authorId": "2152318443",
        "name": "Youlin Xiang"
      },
      {
        "authorId": "2212411539",
        "name": "Yun Xiong"
      },
      {
        "authorId": "2256769434",
        "name": "Haofen Wang"
      },
      {
        "authorId": "2144138716",
        "name": "Jiawei Zhang"
      }
    ],
    "abstract": "Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks. However, traditional recommender systems continue to face great challenges such as poor interactivity and explainability, which actually also hinder their broad deployment in real-world systems. To address these limitations, this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augments LLMs for building conversational recommender systems by converting user profiles and historical interactions into prompts. Chat-Rec is demonstrated to be effective in learning user preferences and establishing connections between users and products through in-context learning, which also makes the recommendation process more interactive and explainable. What's more, within the Chat-Rec framework, user's preferences can transfer to different products for cross-domain recommendations, and prompt-based injection of information into LLMs can also handle the cold-start scenarios with new items. In our experiments, Chat-Rec effectively improve the results of top-k recommendations and performs better in zero-shot rating prediction task. Chat-Rec offers a novel approach to improving recommender systems and presents new practical scenarios for the implementation of AIGC (AI generated content) in recommender system studies.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "c4318ad89e8fe85674c2f6e10fe2d2446b7c7578",
    "externalIds": {
      "DBLP": "conf/recsys/TsaiB20",
      "MAG": "3097125659",
      "CorpusId": 221969158
    },
    "url": "https://www.semanticscholar.org/paper/c4318ad89e8fe85674c2f6e10fe2d2446b7c7578",
    "title": "User Feedback in Controllable and Explainable Social Recommender Systems: a Linguistic Analysis",
    "venue": "IntRS@RecSys",
    "year": 2020,
    "referenceCount": 60,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "40166850",
        "name": "Chun-Hua Tsai"
      },
      {
        "authorId": "1804693",
        "name": "Peter Brusilovsky"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "413a87342b5a7dfb1ba2877549aad80e338f89da",
    "externalIds": {
      "DBLP": "conf/sigir-ap/YuSJ23",
      "DOI": "10.1145/3624918.3625331",
      "CorpusId": 265409430
    },
    "url": "https://www.semanticscholar.org/paper/413a87342b5a7dfb1ba2877549aad80e338f89da",
    "title": "AdaReX: Cross-Domain, Adaptive, and Explainable Recommender System",
    "venue": "SIGIR-AP",
    "year": 2023,
    "referenceCount": 43,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3624918.3625331",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3624918.3625331?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3624918.3625331, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-11-26",
    "authors": [
      {
        "authorId": "2268284436",
        "name": "Yi Yu"
      },
      {
        "authorId": "2268098628",
        "name": "Kazunari Sugiyama"
      },
      {
        "authorId": "2239340856",
        "name": "Adam Jatowt"
      }
    ],
    "abstract": "Explainability is an inherent issue of recommender systems and has received a lot of attention recently. Generative explainable recommendation, which provides personalized explanations by generating textual rationales, is emerging as an effective solution. Despite promising, current methods face limitations in their reliance on dense training data, which hinders the generalizability of explainable recommender systems. Our work tackles a novel problem of cross-domain explainable recommendation aiming to extend the generalizability of explainable recommender systems. To solve this, we propose a novel approach that models aspects extracted from past reviews, to empower the explainable recommender systems by leveraging knowledge from other domains. Specifically, we propose AdaReX (Adaptive eXplainable Recommendation), to model auxiliary and target domains simultaneously. By performing specific tasks in respective domains and their interconnection via a discriminator model, AdaReX allows the aspect sequences to learn common knowledge across different domains and tasks. Furthermore, through our proposed optimization objective, the learning of aspect sequence is deeply cross-interacted with in-domain users and items’ latent factors, enabling the enhanced sharing of knowledge between domains. Our extensive experiments on real datasets demonstrate that our approach not only generates better explanations and recommendations for sparse users but also improves performance for general users.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "1aeb8cce292705824987983a20dd29584f722749",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-05331",
      "ArXiv": "2305.05331",
      "DOI": "10.1109/TKDE.2024.3350447",
      "CorpusId": 258564243
    },
    "url": "https://www.semanticscholar.org/paper/1aeb8cce292705824987983a20dd29584f722749",
    "title": "Explainable Recommender With Geometric Information Bottleneck",
    "venue": "IEEE Transactions on Knowledge and Data Engineering",
    "year": 2023,
    "referenceCount": 57,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2305.05331",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.05331, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-05-09",
    "authors": [
      {
        "authorId": "1830443015",
        "name": "Hanqi Yan"
      },
      {
        "authorId": "145096580",
        "name": "Lin Gui"
      },
      {
        "authorId": "2381913941",
        "name": "Menghan Wang"
      },
      {
        "authorId": "2156563711",
        "name": "Kun Zhang"
      },
      {
        "authorId": "1390509967",
        "name": "Yulan He"
      }
    ],
    "abstract": "Explainable recommender systems can explain their recommendation decisions, enhancing user trust in the systems. Most explainable recommender systems either rely on human-annotated rationales to train models for explanation generation or leverage the attention mechanism to extract important text spans from reviews as explanations. The extracted rationales are often confined to an individual review and may fail to identify the implicit features beyond the review text. To avoid the expensive human annotation process and to generate explanations beyond individual reviews, we propose to incorporate a geometric prior learnt from user-item interactions into a variational network which infers latent factors from user-item reviews. The latent factors from an individual user-item pair can be used for both recommendation and explanation generation, which naturally inherit the global characteristics encoded in the prior knowledge. Experimental results on three e-commerce datasets show that our model significantly improves the interpretability of a variational recommender using the Wasserstein distance while achieving performance comparable to existing content-based recommender systems in terms of recommendation behaviours.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "0f2d11ac3202997b914efafaedb56b564ff9d3a7",
    "externalIds": {
      "ArXiv": "2305.07961",
      "DBLP": "journals/corr/abs-2305-07961",
      "CorpusId": 258685399
    },
    "url": "https://www.semanticscholar.org/paper/0f2d11ac3202997b914efafaedb56b564ff9d3a7",
    "title": "Leveraging Large Language Models in Conversational Recommender Systems",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 118,
    "citationCount": 111,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.07961, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-13",
    "authors": [
      {
        "authorId": "2217253911",
        "name": "Luke Friedman"
      },
      {
        "authorId": "2416263",
        "name": "Sameer Ahuja"
      },
      {
        "authorId": "2217251792",
        "name": "David Allen"
      },
      {
        "authorId": "6840539",
        "name": "Zhenning Tan"
      },
      {
        "authorId": "39041127",
        "name": "Hakim Sidahmed"
      },
      {
        "authorId": "2217232763",
        "name": "Changbo Long"
      },
      {
        "authorId": "2109935759",
        "name": "Jun Xie"
      },
      {
        "authorId": "2370193442",
        "name": "Gabriel Schubiner"
      },
      {
        "authorId": "2109171018",
        "name": "Ajay Patel"
      },
      {
        "authorId": "2196537401",
        "name": "Harsh Lara"
      },
      {
        "authorId": "2055638940",
        "name": "Brian Chu"
      },
      {
        "authorId": "2211981539",
        "name": "Zexiang Chen"
      },
      {
        "authorId": "2074227766",
        "name": "Manoj Tiwari"
      }
    ],
    "abstract": "A Conversational Recommender System (CRS) offers increased transparency and control to users by enabling them to engage with the system through a real-time multi-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to converse naturally and incorporate world knowledge and common-sense reasoning into language understanding, unlocking the potential of this paradigm. However, effectively leveraging LLMs within a CRS introduces new technical challenges, including properly understanding and controlling a complex conversation and retrieving from external sources of information. These issues are exacerbated by a large, evolving item corpus and a lack of conversational data for training. In this paper, we provide a roadmap for building an end-to-end large-scale CRS using LLMs. In particular, we propose new implementations for user preference understanding, flexible dialogue management and explainable recommendations as part of an integrated architecture powered by LLMs. For improved personalization, we describe how an LLM can consume interpretable natural language user profiles and use them to modulate session-level context. To overcome conversational data limitations in the absence of an existing production CRS, we propose techniques for building a controllable LLM-based user simulator to generate synthetic conversations. As a proof of concept we introduce RecLLM, a large-scale CRS for YouTube videos built on LaMDA, and demonstrate its fluency and diverse functionality through some illustrative example conversations.",
    "affiliations": [],
    "countries": [],
    "num_authors": 13
  },
  {
    "paperId": "4cc1093e13fb03aa844e9d2e9d8793d9c10f9453",
    "externalIds": {
      "MAG": "2892994974",
      "DBLP": "conf/webist/CostaD18",
      "DOI": "10.5220/0006893700350045",
      "CorpusId": 52898334
    },
    "url": "https://www.semanticscholar.org/paper/4cc1093e13fb03aa844e9d2e9d8793d9c10f9453",
    "title": "Neural Explainable Collective Non-negative Matrix Factorization for Recommender Systems",
    "venue": "International Conference on Web Information Systems and Technologies",
    "year": 2018,
    "referenceCount": 20,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://doi.org/10.5220/0006893700350045",
      "status": "HYBRID",
      "license": "CCBYNCND",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5220/0006893700350045?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5220/0006893700350045, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "145331063",
        "name": "F. Costa"
      },
      {
        "authorId": "1805892",
        "name": "Peter Dolog"
      }
    ],
    "abstract": "Explainable recommender systems aim to generate explanations for users according to their predicted scores, the user’s history and their similarity to other users. Recently, researchers have proposed explainable recommender models using topic models and sentiment analysis methods providing explanations based on user’s reviews. However, such methods have neglected improvements in natural language processing, even if these methods are known to improve user satisfaction. In this paper, we propose a neural explainable collective nonnegative matrix factorization (NECoNMF) to predict ratings based on users’ feedback, for example, ratings and reviews. To do so, we use collective non-negative matrix factorization to predict user preferences according to different features and a natural language model to explain the prediction. Empirical experiments were conducted in two datasets, showing the model’s efficiency for predicting ratings and generating explanations. The results present that NECoNMF improves the accuracy for explainable recommendations in comparison with the state-of-art method in 18.3% for NDCG@5, 12.2% for HitRatio@5, 17.1% for NDCG@10, and 12.2% for HitRatio@10 in the Yelp dataset. A similar performance has been observed in the Amazon dataset 7.6% for NDCG@5, 1.3% for HitRatio@5, 7.9% for NDCG@10, and 3.9% for HitRatio@10.",
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "afbd4b202dcd599a6906bfc0dd219b5abbf9f0b8",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-00574",
      "ArXiv": "2305.00574",
      "DOI": "10.1145/3539618.3592070",
      "CorpusId": 258427017
    },
    "url": "https://www.semanticscholar.org/paper/afbd4b202dcd599a6906bfc0dd219b5abbf9f0b8",
    "title": "The Dark Side of Explanations: Poisoning Recommender Systems with Counterfactual Examples",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2023,
    "referenceCount": 33,
    "citationCount": 28,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.00574",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.00574, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-04-30",
    "authors": [
      {
        "authorId": "2157281262",
        "name": "Ziheng Chen"
      },
      {
        "authorId": "2192306989",
        "name": "Fabrizio Silvestri"
      },
      {
        "authorId": "2144547216",
        "name": "Jia Wang"
      },
      {
        "authorId": "1591136873",
        "name": "Yongfeng Zhang"
      },
      {
        "authorId": "2651748",
        "name": "Gabriele Tolomei"
      }
    ],
    "abstract": "Deep learning-based recommender systems have become an integral part of several online platforms. However, their black-box nature emphasizes the need for explainable artificial intelligence (XAI) approaches to provide human-understandable reasons why a specific item gets recommended to a given user. One such method is counterfactual explanation (CF). While CFs can be highly beneficial for users and system designers, malicious actors may also exploit these explanations to undermine the system's security. In this work, we propose H-CARS, a novel strategy to poison recommender systems via CFs. Specifically, we first train a logical-reasoning-based surrogate model on training data derived from counterfactual explanations. By reversing the learning process of the recommendation model, we thus develop a proficient greedy algorithm to generate fabricated user profiles and their associated interaction records for the aforementioned surrogate model. Our experiments, which employ a well-known CF generation method and are conducted on two distinct datasets, show that H-CARS yields significant and successful attack performance.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "56cbac15c07f6a52b0bdd59c1d2f968aaea52395",
    "externalIds": {
      "DBLP": "journals/fdata/FelfernigWTELMGL24",
      "PubMedCentral": "10642936",
      "ArXiv": "2412.03620",
      "DOI": "10.3389/fdata.2023.1284511",
      "CorpusId": 264823201,
      "PubMed": "37965497"
    },
    "url": "https://www.semanticscholar.org/paper/56cbac15c07f6a52b0bdd59c1d2f968aaea52395",
    "title": "Recommender systems for sustainability: overview and research issues",
    "venue": "Frontiers Big Data",
    "year": 2023,
    "referenceCount": 133,
    "citationCount": 24,
    "openAccessPdf": {
      "url": "https://www.frontiersin.org/articles/10.3389/fdata.2023.1284511/pdf?isPublishedV2=False",
      "status": "GOLD",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.03620, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Review",
      "JournalArticle"
    ],
    "publicationDate": "2023-10-30",
    "authors": [
      {
        "authorId": "2264307908",
        "name": "Alexander Felfernig"
      },
      {
        "authorId": "2097794289",
        "name": "Manfred Wundara"
      },
      {
        "authorId": "13278227",
        "name": "Thi Ngoc Trang Tran"
      },
      {
        "authorId": "2135115901",
        "name": "Seda Polat-Erdeniz"
      },
      {
        "authorId": "2182737348",
        "name": "Sebastian Lubos"
      },
      {
        "authorId": "2264438563",
        "name": "Merfat El Mansi"
      },
      {
        "authorId": "2233477799",
        "name": "Damian Garber"
      },
      {
        "authorId": "144335284",
        "name": "Viet-Man Le"
      }
    ],
    "abstract": "Sustainability development goals (SDGs) are regarded as a universal call to action with the overall objectives of planet protection, ending of poverty, and ensuring peace and prosperity for all people. In order to achieve these objectives, different AI technologies play a major role. Specifically, recommender systems can provide support for organizations and individuals to achieve the defined goals. Recommender systems integrate AI technologies such as machine learning, explainable AI (XAI), case-based reasoning, and constraint solving in order to find and explain user-relevant alternatives from a potentially large set of options. In this article, we summarize the state of the art in applying recommender systems to support the achievement of sustainability development goals. In this context, we discuss open issues for future research.",
    "affiliations": [],
    "countries": [],
    "num_authors": 8
  },
  {
    "paperId": "f6e936ee147639b3d694e076896ea6756fcc399e",
    "externalIds": {
      "ArXiv": "2306.05809",
      "DBLP": "journals/corr/abs-2306-05809",
      "DOI": "10.1080/10447318.2023.2262797",
      "CorpusId": 259129445
    },
    "url": "https://www.semanticscholar.org/paper/f6e936ee147639b3d694e076896ea6756fcc399e",
    "title": "Interactive Explanation with Varying Level of Details in an Explainable Scientific Literature Recommender System",
    "venue": "International journal of human computer interactions",
    "year": 2023,
    "referenceCount": 86,
    "citationCount": 16,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2306.05809",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.05809, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-09",
    "authors": [
      {
        "authorId": "1729232457",
        "name": "Mouadh Guesmi"
      },
      {
        "authorId": "1724546",
        "name": "Mohamed Amine Chatti"
      },
      {
        "authorId": "2114534153",
        "name": "Shoeb Joarder"
      },
      {
        "authorId": "2167427642",
        "name": "Qurat Ul Ain"
      },
      {
        "authorId": "2023361368",
        "name": "R. Alatrash"
      },
      {
        "authorId": "2214581453",
        "name": "Clara Siepmann"
      },
      {
        "authorId": "2219861226",
        "name": "Tannaz Vahidi"
      }
    ],
    "abstract": "Abstract Explainable recommender systems (RS) have traditionally followed a one-size-fits-all approach, delivering the same explanation level of detail to each user, without considering their individual needs and goals. Further, explanations in RS have so far been presented mostly in a static and non-interactive manner. To fill these research gaps, we aim in this paper to adopt a user-centered, interactive explanation model that provides explanations with different levels of detail and empowers users to interact with, control, and personalize the explanations based on their needs and preferences. We followed a user-centered approach to design interactive explanations with three levels of detail (basic, intermediate, and advanced) and implemented them in the transparent Recommendation and Interest Modeling Application (RIMA). We conducted a qualitative user study (N = 14) to investigate the impact of providing interactive explanations with varying level of details on the users’ perception of the explainable RS. Our study showed qualitative evidence that fostering interaction and giving users control in deciding which explanation they would like to see can meet the demands of users with different needs, preferences, and goals, and consequently can have positive effects on different crucial aspects in explainable recommendation, including transparency, trust, satisfaction, and user experience.",
    "affiliations": [],
    "countries": [],
    "num_authors": 7
  },
  {
    "paperId": "b20c87e2925a81f57d9c913423e9e74c14de5341",
    "externalIds": {
      "DBLP": "journals/tors/GeLFTLXLXZ25",
      "ArXiv": "2207.12515",
      "DOI": "10.1145/3652891",
      "CorpusId": 251067091
    },
    "url": "https://www.semanticscholar.org/paper/b20c87e2925a81f57d9c913423e9e74c14de5341",
    "title": "A Survey on Trustworthy Recommender Systems",
    "venue": "Trans. Recomm. Syst.",
    "year": 2022,
    "referenceCount": 372,
    "citationCount": 89,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3652891",
      "status": "HYBRID",
      "license": "other-oa",
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.12515, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2022-07-25",
    "authors": [
      {
        "authorId": "152988336",
        "name": "Yingqiang Ge"
      },
      {
        "authorId": "50152132",
        "name": "Shuchang Liu"
      },
      {
        "authorId": "2011378",
        "name": "Zuohui Fu"
      },
      {
        "authorId": "2110449137",
        "name": "Juntao Tan"
      },
      {
        "authorId": "2109968285",
        "name": "Zelong Li"
      },
      {
        "authorId": "2111044480",
        "name": "Shuyuan Xu"
      },
      {
        "authorId": "48515097",
        "name": "Yunqi Li"
      },
      {
        "authorId": "2885287",
        "name": "Yikun Xian"
      },
      {
        "authorId": "2145038716",
        "name": "Yongfeng Zhang"
      }
    ],
    "abstract": "Recommender systems (RS), serving at the forefront of Human-centered AI, are widely deployed in almost every corner of the web and facilitate the human decision-making process. However, despite their enormous capabilities and potential, RS may also lead to undesired effects on users, items, producers, platforms, or even the society at large, such as compromised user trust due to non-transparency, unfair treatment of different consumers, or producers, privacy concerns due to extensive use of user’s private data for personalization, just to name a few. All of these create an urgent need for Trustworthy Recommender Systems (TRS) so as to mitigate or avoid such adverse impacts and risks. In this survey, we will introduce techniques related to trustworthy recommendation, including but not limited to explainable recommendation, fairness in recommendation, privacy-aware recommendation, robustness in recommendation, user-controllable recommendation, as well as the relationship between these different perspectives in terms of trustworthy recommendation. Through this survey, we hope to deliver readers with a comprehensive view of the research area and raise attention to the community about the importance, existing research achievements, and future research directions on trustworthy recommendation.",
    "affiliations": [],
    "countries": [],
    "num_authors": 9
  },
  {
    "paperId": "41f052ccaa811731f68f0abbef2c8a978a374f83",
    "externalIds": {
      "DBLP": "journals/corr/abs-2204-11241",
      "ArXiv": "2204.11241",
      "DOI": "10.1145/3477495.3532041",
      "CorpusId": 248377350
    },
    "url": "https://www.semanticscholar.org/paper/41f052ccaa811731f68f0abbef2c8a978a374f83",
    "title": "Post Processing Recommender Systems with Knowledge Graphs for Recency, Popularity, and Diversity of Explanations",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2022,
    "referenceCount": 41,
    "citationCount": 41,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3477495.3532041",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.11241, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2022-04-24",
    "authors": [
      {
        "authorId": "2163452228",
        "name": "Giacomo Balloccu"
      },
      {
        "authorId": "1824224",
        "name": "Ludovico Boratto"
      },
      {
        "authorId": "40433308",
        "name": "G. Fenu"
      },
      {
        "authorId": "28922901",
        "name": "Mirko Marras"
      }
    ],
    "abstract": "Existing explainable recommender systems have mainly modeled relationships between recommended and already experienced products, and shaped explanation types accordingly (e.g., movie \"x\" starred by actress \"y\" recommended to a user because that user watched other movies with \"y\" as an actress). However, none of these systems has investigated the extent to which properties of a single explanation (e.g., the recency of interaction with that actress) and of a group of explanations for a recommended list (e.g., the diversity of the explanation types) can influence the perceived explaination quality. In this paper, we conceptualized three novel properties that model the quality of the explanations (linking interaction recency, shared entity popularity, and explanation type diversity) and proposed re-ranking approaches able to optimize for these properties. Experiments on two public data sets showed that our approaches can increase explanation quality according to the proposed properties, fairly across demographic groups, while preserving recommendation utility. The source code and data are available at https://github.com/giacoballoccu/explanation-quality-recsys.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "d39b71bd3bcf53e12129323f8521f25ed604efab",
    "externalIds": {
      "DBLP": "journals/concurrency/Vultureanu-Albisi22",
      "DOI": "10.1002/cpe.6834",
      "CorpusId": 246129497
    },
    "url": "https://www.semanticscholar.org/paper/d39b71bd3bcf53e12129323f8521f25ed604efab",
    "title": "A survey on effects of adding explanations to recommender systems",
    "venue": "Concurrency and Computation",
    "year": 2022,
    "referenceCount": 64,
    "citationCount": 24,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1002/cpe.6834?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/cpe.6834, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2022-01-19",
    "authors": [
      {
        "authorId": "2091486093",
        "name": "Alexandra Vultureanu-Albişi"
      },
      {
        "authorId": "1740341",
        "name": "C. Bǎdicǎ"
      }
    ],
    "abstract": "Explainable recommendations become essential when we need to improve the performance of recommendations and to increase user confidence. Explanations are effective when end users can build a complete and correct mental representation of the inferential process of a recommender system. This paper presents our view on the background regarding the implications of explainability applied to recommender systems. Our work contributes to the better understanding of the concept of explainable recommendation and it offers a broader picture of the development of further research in this field. Additionally, we contribute by providing a better understanding of the concept of human‐centered evaluation of explainable recommender systems.",
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "d4bec7afedeb15bd1a3b45e369a86c8eeebbe384",
    "externalIds": {
      "DBLP": "conf/iui/AinCGJ22",
      "CorpusId": 248301731
    },
    "url": "https://www.semanticscholar.org/paper/d4bec7afedeb15bd1a3b45e369a86c8eeebbe384",
    "title": "A Multi-Dimensional Conceptualization Framework for Personalized Explanations in Recommender Systems 11-23",
    "venue": "IUI Workshops",
    "year": 2022,
    "referenceCount": 47,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2167427642",
        "name": "Qurat Ul Ain"
      },
      {
        "authorId": "2163104962",
        "name": "Mohamed Anime Chati"
      },
      {
        "authorId": "1729232457",
        "name": "Mouadh Guesmi"
      },
      {
        "authorId": "2114534153",
        "name": "Shoeb Joarder"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "f6e64ef1094c9a6d546ca05b431556521d38b689",
    "externalIds": {
      "DBLP": "conf/ACMdis/ViswanathanGCGR22",
      "DOI": "10.1145/3532106.3533491",
      "CorpusId": 249579014
    },
    "url": "https://www.semanticscholar.org/paper/f6e64ef1094c9a6d546ca05b431556521d38b689",
    "title": "Addressing Hiccups in Conversations with Recommender Systems",
    "venue": "Conference on Designing Interactive Systems",
    "year": 2022,
    "referenceCount": 89,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3532106.3533491?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3532106.3533491, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-06-13",
    "authors": [
      {
        "authorId": "1678382",
        "name": "Sruthi Viswanathan"
      },
      {
        "authorId": "1811419139",
        "name": "Fabien Guillot"
      },
      {
        "authorId": "10741225",
        "name": "Minsuk Chang"
      },
      {
        "authorId": "2167612",
        "name": "A. Grasso"
      },
      {
        "authorId": "2822140",
        "name": "J. Renders"
      }
    ],
    "abstract": "Conversational Agents (CAs) employing voice as their main interaction mode produce natural language utterances with the aim of mimicking human conversations. To unveil hiccups in conversations with recommender systems, we observed users interacting with CAs. Our findings suggest that those occur as users struggle to start the session, as CAs do not appear exploratory, and as CAs remained silent after offering recommendation(s) or after reporting errors. Users enacted mental models derived from years of experience with Graphical User Interfaces, but also expected human-like characteristics such as explanations and proactivity. Anchoring on these, we designed a dialogue model for a multimodal Conversational Recommender System (CRS) mimicking humans and GUIs. We probed the state of hiccups further with a Wizard-of-Oz prototype implementing this dialogue model. Our findings suggest that participants rapidly adopted GUI mimicries, cooperated for error resolution, appreciated explainable recommendations, and provided insights to improve persisting hiccups in proactivity and navigation. Based on these, we provide implications for design to address hiccups in CRS.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "9ef432c5432c98587af495b819a594f1c4989711",
    "externalIds": {
      "DBLP": "conf/ic3k/RostamiFBFA022",
      "DOI": "10.5220/0011561700003335",
      "CorpusId": 253198554
    },
    "url": "https://www.semanticscholar.org/paper/9ef432c5432c98587af495b819a594f1c4989711",
    "title": "A Novel Explainable and Health-aware Food Recommender System",
    "venue": "International Conference on Knowledge Discovery and Information Retrieval",
    "year": 2022,
    "referenceCount": 21,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://doi.org/10.5220/0011561700003335",
      "status": "HYBRID",
      "license": "CCBYNCND",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5220/0011561700003335?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5220/0011561700003335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2189179175",
        "name": "Merhrdad Rostami"
      },
      {
        "authorId": "2189179927",
        "name": "Vahid Farahi"
      },
      {
        "authorId": "52024680",
        "name": "K. Berahmand"
      },
      {
        "authorId": "3103483",
        "name": "Saman Forouzandeh"
      },
      {
        "authorId": "1901767",
        "name": "S. Ahmadian"
      },
      {
        "authorId": "2051449930",
        "name": "M. Oussalah"
      }
    ],
    "abstract": ": Food recommendation systems are increasingly being used by online food services to make recommendations. Health factors are often ignored in most of these systems, despite the fact that unhealthy diets are connected to a wide range of non-communicable diseases. Furthermore, if users do not receive compelling explanations about the recommended healthy foods, they may become hesitant to try them. In this paper, a novel explainable and health-aware food recommender system is developed to address these challenges. For this purpose, user’s preferences and food health factors are taken into account simultaneously and then a rule-based mechanism is employed for ﬁnal healthy and explainable recommendations. Five performance metrics were used to compare our system with different new recommender systems. Using a dataset crawled from ”Allrecipes.com”, the proposed model is shown to perform best.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "4da28c66713a5038114911b387b549b7ce9e0090",
    "externalIds": {
      "DBLP": "journals/corr/abs-2502-10050",
      "ArXiv": "2502.10050",
      "DOI": "10.48550/arXiv.2502.10050",
      "CorpusId": 276395083
    },
    "url": "https://www.semanticscholar.org/paper/4da28c66713a5038114911b387b549b7ce9e0090",
    "title": "A Survey on LLM-powered Agents for Recommender Systems",
    "venue": "arXiv.org",
    "year": 2025,
    "referenceCount": 48,
    "citationCount": 15,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.10050, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2025-02-14",
    "authors": [
      {
        "authorId": "152405445",
        "name": "Qiyao Peng"
      },
      {
        "authorId": "2108972050",
        "name": "Hongtao Liu"
      },
      {
        "authorId": "2334596658",
        "name": "Hua Huang"
      },
      {
        "authorId": "2260805189",
        "name": "Qing Yang"
      },
      {
        "authorId": "19066746",
        "name": "Minglai Shao"
      }
    ],
    "abstract": "Recommender systems are essential components of many online platforms, yet traditional approaches still struggle with understanding complex user preferences and providing explainable recommendations. The emergence of Large Language Model (LLM)-powered agents offers a promising approach by enabling natural language interactions and interpretable reasoning, potentially transforming research in recommender systems. This survey provides a systematic review of the emerging applications of LLM-powered agents in recommender systems. We identify and analyze three key paradigms in current research: (1) Recommender-oriented approaches, which leverage intelligent agents to enhance the fundamental recommendation mechanisms; (2) Interaction-oriented approaches, which facilitate dynamic user engagement through natural dialogue and interpretable suggestions; and (3) Simulation-oriented approaches, which employ multi-agent frameworks to model complex user-item interactions and system dynamics. Beyond paradigm categorization, we analyze the architectural foundations of LLM-powered recommendation agents, examining their essential components: profile construction, memory management, strategic planning, and action execution. Our investigation extends to a comprehensive analysis of benchmark datasets and evaluation frameworks in this domain. This systematic examination not only illuminates the current state of LLM-powered agent recommender systems but also charts critical challenges and promising research directions in this transformative field.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "960f194d1b129843de4a99c5eea5a535f142fcef",
    "externalIds": {
      "ArXiv": "2109.10665",
      "DBLP": "journals/corr/abs-2109-10665",
      "DOI": "10.1109/TNNLS.2023.3280161",
      "CorpusId": 237592904,
      "PubMed": "37279123"
    },
    "url": "https://www.semanticscholar.org/paper/960f194d1b129843de4a99c5eea5a535f142fcef",
    "title": "A Survey on Reinforcement Learning for Recommender Systems",
    "venue": "IEEE Transactions on Neural Networks and Learning Systems",
    "year": 2021,
    "referenceCount": 206,
    "citationCount": 53,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2109.10665",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.10665, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2021-09-22",
    "authors": [
      {
        "authorId": "113310851",
        "name": "Y. Lin"
      },
      {
        "authorId": "2144385006",
        "name": "Yong Liu"
      },
      {
        "authorId": "145957468",
        "name": "Fan Lin"
      },
      {
        "authorId": "2240533680",
        "name": "Lixin Zou"
      },
      {
        "authorId": "2111194421",
        "name": "Pengcheng Wu"
      },
      {
        "authorId": "3265605",
        "name": "Wenhua Zeng"
      },
      {
        "authorId": "2257328854",
        "name": "Huanhuan Chen"
      },
      {
        "authorId": "1679209",
        "name": "C. Miao"
      }
    ],
    "abstract": "Recommender systems have been widely applied in different real-life scenarios to help us find useful information. In particular, reinforcement learning (RL)-based recommender systems have become an emerging research topic in recent years, owing to the interactive nature and autonomous learning ability. Empirical results show that RL-based recommendation methods often surpass supervised learning methods. Nevertheless, there are various challenges in applying RL in recommender systems. To understand the challenges and relevant solutions, there should be a reference for researchers and practitioners working on RL-based recommender systems. To this end, we first provide a thorough overview, comparisons, and summarization of RL approaches applied in four typical recommendation scenarios, including interactive recommendation, conversational recommendation, sequential recommendation, and explainable recommendation. Furthermore, we systematically analyze the challenges and relevant solutions on the basis of existing literature. Finally, under discussion for open issues of RL and its limitations of recommender systems, we highlight some potential research directions in this field.",
    "affiliations": [],
    "countries": [],
    "num_authors": 8
  },
  {
    "paperId": "e77ca077480349e8f8cb2a12c9e64e724b4001bd",
    "externalIds": {
      "DBLP": "journals/corr/abs-2110-14844",
      "ArXiv": "2110.14844",
      "CorpusId": 240070310
    },
    "url": "https://www.semanticscholar.org/paper/e77ca077480349e8f8cb2a12c9e64e724b4001bd",
    "title": "From Intrinsic to Counterfactual: On the Explainability of Contextualized Recommender Systems",
    "venue": "arXiv.org",
    "year": 2021,
    "referenceCount": 51,
    "citationCount": 15,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.14844, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2021-10-28",
    "authors": [
      {
        "authorId": "2110362600",
        "name": "Yao Zhou"
      },
      {
        "authorId": null,
        "name": "Haonan Wang"
      },
      {
        "authorId": "31108652",
        "name": "Jingrui He"
      },
      {
        "authorId": "2109590665",
        "name": "Haixun Wang"
      }
    ],
    "abstract": "With the prevalence of deep learning based embedding approaches, recommender systems have become a proven and indispensable tool in various information filtering applications. However, many of them remain difficult to diagnose what aspects of the deep models' input drive the final ranking decision, thus, they cannot often be understood by human stakeholders. In this paper, we investigate the dilemma between recommendation and explainability, and show that by utilizing the contextual features (e.g., item reviews from users), we can design a series of explainable recommender systems without sacrificing their performance. In particular, we propose three types of explainable recommendation strategies with gradual change of model transparency: whitebox, graybox, and blackbox. Each strategy explains its ranking decisions via different mechanisms: attention weights, adversarial perturbations, and counterfactual perturbations. We apply these explainable models on five real-world data sets under the contextualized setting where users and items have explicit interactions. The empirical results show that our model achieves highly competitive ranking performance, and generates accurate and effective explanations in terms of numerous quantitative metrics and qualitative visualizations.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "baf0bc1609a866c226e8ec8cbefbf399ca489910",
    "externalIds": {
      "DBLP": "conf/cui/Hernandez-Bocanegra21",
      "DOI": "10.1145/3469595.3469596",
      "CorpusId": 236203046
    },
    "url": "https://www.semanticscholar.org/paper/baf0bc1609a866c226e8ec8cbefbf399ca489910",
    "title": "Conversational review-based explanations for recommender systems: Exploring users’ query behavior",
    "venue": "International Conference on Conversational User Interfaces",
    "year": 2021,
    "referenceCount": 69,
    "citationCount": 19,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3469595.3469596?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3469595.3469596, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2021-07-23",
    "authors": [
      {
        "authorId": "1573207580",
        "name": "Diana C. Hernandez-Bocanegra"
      },
      {
        "authorId": "145758499",
        "name": "J. Ziegler"
      }
    ],
    "abstract": "Providing explanations based on user reviews in recommender systems (RS) can increase users’ perception of system transparency. While static explanations are dominant, interactive explanatory approaches have emerged in explainable artificial intelligence (XAI), so that users are more likely to examine system decisions and get more arguments supporting system assertions. However, little attention has been paid to conversational approaches for explanations targeting end users. In this paper we explore how to design a conversational interface to provide explanations in a review-based RS, and present the results of a Wizard of Oz (WoOz) study that provided insights into the type of questions users might ask in such a context, as well as their perception of a system simulating such a dialog. Consequently, we propose a dialog management policy and user intents for explainable review-based RS, taking as an example the hotels domain.",
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "c0c789bfcac6c5d723f49feb47e939a9afe8a219",
    "externalIds": {
      "PubMedCentral": "7885753",
      "DBLP": "journals/bise/BauerHAW21",
      "DOI": "10.1007/s12599-021-00683-2",
      "CorpusId": 231934401
    },
    "url": "https://www.semanticscholar.org/paper/c0c789bfcac6c5d723f49feb47e939a9afe8a219",
    "title": "Expl(AI)n It to Me – Explainable AI and Information Systems Research",
    "venue": "Business & Information Systems Engineering",
    "year": 2021,
    "referenceCount": 23,
    "citationCount": 45,
    "openAccessPdf": {
      "url": "https://link.springer.com/content/pdf/10.1007/s12599-021-00683-2.pdf",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7885753, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-02-16",
    "authors": [
      {
        "authorId": "2004925877",
        "name": "Kevin Bauer"
      },
      {
        "authorId": "2040528",
        "name": "O. Hinz"
      },
      {
        "authorId": "145898058",
        "name": "Wil M.P. van der Aalst"
      },
      {
        "authorId": "1733795",
        "name": "Christof Weinhardt"
      }
    ],
    "abstract": "The field of Artificial Intelligence has seen dramatic progress over the last 15 years. Using machine learning methods, software systems that automatically learn and improve relationships using digitized experience, researchers and practitioners alike have developed practical applications that are indispensable and strongly facilitate people's everyday life [Jordan and Mitchell 2015]. Pervasive examples include object recognition (e.g., Facebook's Moments and Intel Security's True Key), natural language processing (e.g., DeepL and Google Translate), recommender systems (e.g., recommendations by Netflix or iTunes), and digital assistants (e.g., Alexa and Siri). At its core, these applications have in common that highly complex and increasingly opaque networks of mathematical constructs are trained using historical data to make predictions about an uncertain state of the world. Based on large sets of labeled images, Deep Convolutional Neural Networks, for instance, can learn to make highly accurate individual-level predictions about the presence of diseases. This includes predicting positive COVID-19 patients [Shi et al. 2020]. While highly accurate predictions in and of themselves are vital to informing fact-based decision-making (regarding disease detection even in a literal sense), the high predictive performance of state-of-the-art machine learning models generally comes at the expense of transparency and interpretability of their outputs [Voosen 2017, Du et al. 2020]. Put differently: the majority of high-performance machine learning models are characterized by an incapability to convey human-interpretable information about how and why they produce specific predictions. Hence, such machine learning applications are often complete black boxes to their human users and even expert designers, who frequently lack an understanding of the reason behind decision-critical outputs. From a methodological point of view, the inability to provide an explanation that accompanies specific predictions creates three types of high-level problems. systems'",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "1b5e31cdaad616f42227578254cf74cbf3bdf245",
    "externalIds": {
      "DBLP": "journals/ci/SrinivasuIW24",
      "DOI": "10.1111/coin.12629",
      "CorpusId": 267112710
    },
    "url": "https://www.semanticscholar.org/paper/1b5e31cdaad616f42227578254cf74cbf3bdf245",
    "title": "XAI‐driven model for crop recommender system for use in precision agriculture",
    "venue": "International Conference on Climate Informatics",
    "year": 2024,
    "referenceCount": 50,
    "citationCount": 28,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1111/coin.12629?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1111/coin.12629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-14",
    "authors": [
      {
        "authorId": "34929616",
        "name": "P. Srinivasu"
      },
      {
        "authorId": "2265974169",
        "name": "Muhammad Fazal Ijaz"
      },
      {
        "authorId": "2248168293",
        "name": "Marcin Woźniak"
      }
    ],
    "abstract": "Agriculture serves as the predominant driver of a country's economy, constituting the largest share of the nation's manpower. Most farmers are facing a problem in choosing the most appropriate crop that can yield better based on the environmental conditions and make profits for them. As a consequence of this, there will be a notable decline in their overall productivity. Precision agriculture has effectively resolved the issues encountered by farmers. Today's farmers may benefit from what's known as precision agriculture. This method takes into account local climate, soil type, and past crop yields to determine which varieties will provide the best results. The explainable artificial intelligence (XAI) technique is used with radial basis functions neural network and spider monkey optimization to classify suitable crops based on the underlying soil and environmental conditions. The XAI technology would provide assets in better transparency of the prediction model on deciding the most suitable crops for their farms, taking into account a variety of geographical and operational criteria. The proposed model is assessed using standard metrics like precision, recall, accuracy, and F1‐score. In contrast to other cutting‐edge approaches discussed in this study, the model has shown fair performance with approximately 12% better accuracy than the other models considered in the current study. Similarly, precision has improvised by 10%, recall by 11%, and F1‐score by 10%.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "eb007f59728e16e36d30a11da52e3e0910481f71",
    "externalIds": {
      "DOI": "10.1109/ICCIT64611.2024.11022562",
      "CorpusId": 279268193
    },
    "url": "https://www.semanticscholar.org/paper/eb007f59728e16e36d30a11da52e3e0910481f71",
    "title": "The Role of XAI in User-Centric Recommender Systems Using Collaborative and Content-Based Approaches",
    "venue": "2024 27th International Conference on Computer and Information Technology (ICCIT)",
    "year": 2024,
    "referenceCount": 12,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCIT64611.2024.11022562?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCIT64611.2024.11022562, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2024-12-20",
    "authors": [
      {
        "authorId": "2287980771",
        "name": "H. M. S. Quadir"
      },
      {
        "authorId": "2364531161",
        "name": "Md. Atikur Rahman"
      },
      {
        "authorId": "2364195350",
        "name": "Moontasir Mahmood"
      },
      {
        "authorId": "2364202857",
        "name": "Nishat Tasnim"
      },
      {
        "authorId": "2115566499",
        "name": "Md. Tohidul Islam"
      },
      {
        "authorId": "2126004501",
        "name": "Md. Golam Rashed"
      },
      {
        "authorId": "2288118940",
        "name": "Dipankar Das"
      }
    ],
    "abstract": "Recommendation systems are integral to new-age digital platforms, influencing user behavior by offering personalized suggestions. However, their lack of opacity often sparks concerns about user autonomy and fairness, as users are left unenlightened of how recommendations are generated. This paper addresses the importance of explainable AI (XAI) in recommendation systems to build user trust. We implemented collaborative and content-based movie recommender systems using the MovieLens 100k dataset, and extended the content-based approach to the TMDB 5000 dataset demonstrating how explainability can be integrated into recommendation models to provide users with insights into the reasoning behind specific movie suggestions. By generating simple yet effective explanations based on movie genre, user ratings, commonality among users, similarity score, keywords, and other relevant features, our approach intends to help users understand the reasoning behind their recommendations. Our comparative analysis highlights the advantages of explainable recommendation systems over non-explainable ones, showing that the inclusion of explanations significantly improves user satisfaction by reducing confusion and increasing transparency.",
    "affiliations": [],
    "countries": [],
    "num_authors": 7
  },
  {
    "paperId": "78cbd19d11d5c7fb4007c182706607e7dff3dd75",
    "externalIds": {
      "DBLP": "conf/amcis/YoonLHK24",
      "CorpusId": 271325516
    },
    "url": "https://www.semanticscholar.org/paper/78cbd19d11d5c7fb4007c182706607e7dff3dd75",
    "title": "How Can Users Maintain Self-Determination in AI Recommender Systems? The Role of Explainable AI (XAI)",
    "venue": "Americas Conference on Information Systems",
    "year": 2024,
    "referenceCount": 0,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2312309413",
        "name": "Youngho Yoon"
      },
      {
        "authorId": "2312308620",
        "name": "One-Ki Daniel Lee"
      },
      {
        "authorId": "2312308246",
        "name": "Wu Haoxi"
      },
      {
        "authorId": "2312308198",
        "name": "Joon Koh"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "830c2f7d3c5e41aac1ba148837cafedf5218d058",
    "externalIds": {
      "ArXiv": "2311.02078",
      "DBLP": "journals/corr/abs-2311-02078",
      "DOI": "10.5121/csit.2023.131612",
      "CorpusId": 262092972
    },
    "url": "https://www.semanticscholar.org/paper/830c2f7d3c5e41aac1ba148837cafedf5218d058",
    "title": "Interpretability is not Explainability: New Quantitative XAI Approach with a focus on Recommender Systems in Education",
    "venue": "Machine Learning Techniques and NLP",
    "year": 2023,
    "referenceCount": 17,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "https://doi.org/10.5121/csit.2023.131612",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.02078, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-18",
    "authors": [
      {
        "authorId": "2243417849",
        "name": "Riccardo Porcedda"
      }
    ],
    "abstract": "The field of eXplainable Artificial Intelligence faces challenges due to the absence of a widely accepted taxonomy that facilitates the quantitative evaluation of explainability in Machine Learning algorithms. In this paper, we propose a novel taxonomy that addresses the current gap in the literature by providing a clear and unambiguous understanding of the key concepts and relationships in XAI. Our approach is rooted in a systematic analysis of existing definitions and frameworks, with a focus on transparency, interpretability, completeness, complexity and understandability as essential dimensions of explainability. This comprehensive taxonomy aims to establish a shared vocabulary for future research. To demonstrate the utility of our proposed taxonomy, we examine a case study of a Recommender System designed to curate and recommend the most suitable online resources from MERLOT. By employing the SHAP package, we quantify and enhance the explainability of the RS within the context of our newly developed taxonomy.",
    "affiliations": [],
    "countries": [],
    "num_authors": 1
  },
  {
    "paperId": "79d9c72ff3892d081bf310e34e406e60b8ff7416",
    "externalIds": {
      "DOI": "10.1109/ICCMC65190.2025.11140655",
      "CorpusId": 281152207
    },
    "url": "https://www.semanticscholar.org/paper/79d9c72ff3892d081bf310e34e406e60b8ff7416",
    "title": "PawPals: A Personalized, Machine Learning-based Pet Breed Recommender with XAI Principles",
    "venue": "International Conference Computing Methodologies and Communication",
    "year": 2025,
    "referenceCount": 18,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCMC65190.2025.11140655?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCMC65190.2025.11140655, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2025-07-23",
    "authors": [
      {
        "authorId": "2379267035",
        "name": "Stuti Patel"
      },
      {
        "authorId": "2379240165",
        "name": "Tishika Gupta"
      },
      {
        "authorId": "2118928293",
        "name": "Shruti Gupta"
      }
    ],
    "abstract": "Choosing the right pet is a crucial decision, yet many owners later realize that their pet’s energy levels, grooming needs, or temperament do not align with their lifestyle. This often results in owner dissatisfaction, rehoming, or abandonment, negatively impacting the pet's welfare. The primary cause of this issue is a lack of informed decision-making when selecting a pet breed. To address this, we propose PawPals, a personalized breed recommendation system for dogs and cats. The system utilizes a content-based filtering approach to recommend suitable dog and cat breeds based on user lifestyle preferences. User inputs are transformed into feature vectors and matched with breed profiles using cosine similarity. Key attributes are normalized via Min-Max Scaling, and missing data is imputed to maintain dataset integrity. To enhance transparency and user trust, the system incorporates principles of Explainable Artificial Intelligence (XAI), offering interpretable justifications for recommendations. The system is implemented using Python and Flask, featuring a lightweight web interface. Recommendation performance is evaluated using standard metrics including Precision, Recall, F1-score, MRR, NDCG, MAP, and Coverage.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "fb0df2489d3c3c80319494e2b7e1730d67926a30",
    "externalIds": {
      "DOI": "10.1007/s41870-025-02491-3",
      "CorpusId": 277619833
    },
    "url": "https://www.semanticscholar.org/paper/fb0df2489d3c3c80319494e2b7e1730d67926a30",
    "title": "An in-depth analysis of Recommender systems for integration of knowledge graphs Utilising federated XAI Model",
    "venue": "International journal of information technology",
    "year": 2025,
    "referenceCount": 16,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s41870-025-02491-3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s41870-025-02491-3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-04-05",
    "authors": [
      {
        "authorId": "2354128245",
        "name": "Chaman Kumar"
      },
      {
        "authorId": "2354195485",
        "name": "Mansaf Alam"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "aaa4ac27f7279fd39f012a28961ee96343338e36",
    "externalIds": {
      "DOI": "10.1080/02564602.2025.2512086",
      "CorpusId": 280554736
    },
    "url": "https://www.semanticscholar.org/paper/aaa4ac27f7279fd39f012a28961ee96343338e36",
    "title": "Evolution of AI-Driven Decision Making with Decision Support Systems, Expert Systems, Recommender Systems, and XAI",
    "venue": "IETE Technical Review",
    "year": 2025,
    "referenceCount": 64,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/02564602.2025.2512086?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/02564602.2025.2512086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-08-04",
    "authors": [
      {
        "authorId": "152866889",
        "name": "M. Ravi"
      },
      {
        "authorId": "1728262",
        "name": "A. Negi"
      },
      {
        "authorId": "2122595015",
        "name": "Nitin Sai Bommi"
      },
      {
        "authorId": "2375228578",
        "name": "Nusrat Rouf"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "803fb686e175587d4ee9c10611e2a1b5cc446373",
    "externalIds": {
      "DBLP": "conf/ozchi/ColleyKHV23",
      "DOI": "10.1145/3638380.3638426",
      "CorpusId": 269748001
    },
    "url": "https://www.semanticscholar.org/paper/803fb686e175587d4ee9c10611e2a1b5cc446373",
    "title": "Exploring Tangible Explainable AI (TangXAI): A User Study of Two XAI Approaches",
    "venue": "Australasian Computer-Human Interaction Conference",
    "year": 2023,
    "referenceCount": 19,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3638380.3638426",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3638380.3638426?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3638380.3638426, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2023-12-02",
    "authors": [
      {
        "authorId": "145222186",
        "name": "Ashley Colley"
      },
      {
        "authorId": "134414269",
        "name": "Matilda Kalving"
      },
      {
        "authorId": "2265111029",
        "name": "Jonna Häkkilä"
      },
      {
        "authorId": "2266517102",
        "name": "Kaisa Väänänen"
      }
    ],
    "abstract": "Explainable AI (XAI) has garnered significant attention as a theoretical subject in the research community. However, the practical application of XAI, particularly in the realm of user interfaces, remains limited. Moreover, evaluations of these interfaces from the perspective of end-users are scarce. In this paper, we introduce and evaluate two innovative tangible XAI interface concepts. The tangible interfaces capitalize on the widely recognized advantages of data physicalization, offering users a more intuitive and hands-on experience. We implemented two distinct XAI approaches within this tangible framework: feature relevance and local explanations. These approaches were applied to real-world use cases: recommending recipes and selecting jogging routes, respectively. The findings of our Wizard of Oz study indicate that participants had some challenges in distinguishing between the primary objectives of the XAI interface and the typical interactions associated with an AI recommender system. However, tangibility seems to support users’ understanding of AI’s explanations and enables users to reflect on their trust in the AI model.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "db563ec5efcbefcb777e6f0d59a7a4bf5cf3bcf8",
    "externalIds": {
      "DBLP": "journals/corr/abs-2309-05507",
      "ArXiv": "2309.05507",
      "DOI": "10.48550/arXiv.2309.05507",
      "CorpusId": 261697007
    },
    "url": "https://www.semanticscholar.org/paper/db563ec5efcbefcb777e6f0d59a7a4bf5cf3bcf8",
    "title": "A Co-design Study for Multi-Stakeholder Job Recommender System Explanations",
    "venue": "xAI",
    "year": 2023,
    "referenceCount": 39,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.05507",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.05507, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-09-11",
    "authors": [
      {
        "authorId": "2187199259",
        "name": "Roan Schellingerhout"
      },
      {
        "authorId": "2239104329",
        "name": "Francesco Barile"
      },
      {
        "authorId": "1803171",
        "name": "N. Tintarev"
      }
    ],
    "abstract": "Recent legislation proposals have significantly increased the demand for eXplainable Artificial Intelligence (XAI) in many businesses, especially in so-called `high-risk' domains, such as recruitment. Within recruitment, AI has become commonplace, mainly in the form of job recommender systems (JRSs), which try to match candidates to vacancies, and vice versa. However, common XAI techniques often fall short in this domain due to the different levels and types of expertise of the individuals involved, making explanations difficult to generalize. To determine the explanation preferences of the different stakeholder types - candidates, recruiters, and companies - we created and validated a semi-structured interview guide. Using grounded theory, we structurally analyzed the results of these interviews and found that different stakeholder types indeed have strongly differing explanation preferences. Candidates indicated a preference for brief, textual explanations that allow them to quickly judge potential matches. On the other hand, hiring managers preferred visual graph-based explanations that provide a more technical and comprehensive overview at a glance. Recruiters found more exhaustive textual explanations preferable, as those provided them with more talking points to convince both parties of the match. Based on these findings, we describe guidelines on how to design an explanation interface that fulfills the requirements of all three stakeholder types. Furthermore, we provide the validated interview guide, which can assist future research in determining the explanation preferences of different stakeholder types.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "64dc60230467414f974804b88525265fb6ecbffa",
    "externalIds": {
      "DBLP": "conf/xai/ZanonRM24",
      "DOI": "10.1007/978-3-031-63797-1_1",
      "CorpusId": 273496237
    },
    "url": "https://www.semanticscholar.org/paper/64dc60230467414f974804b88525265fb6ecbffa",
    "title": "Model-Agnostic Knowledge Graph Embedding Explanations for Recommender Systems",
    "venue": "xAI",
    "year": 2024,
    "referenceCount": 0,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-031-63797-1_1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-031-63797-1_1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2028804534",
        "name": "A. L. Zanon"
      },
      {
        "authorId": "2326950743",
        "name": "L. Rocha"
      },
      {
        "authorId": "9125814",
        "name": "M. Manzato"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "077a00706df79175a250fd9edbb973f73de78a2f",
    "externalIds": {
      "DBLP": "conf/cikm/CugnyACRT22",
      "ArXiv": "2210.02795",
      "DOI": "10.1145/3511808.3557247",
      "CorpusId": 252904786
    },
    "url": "https://www.semanticscholar.org/paper/077a00706df79175a250fd9edbb973f73de78a2f",
    "title": "AutoXAI: A Framework to Automatically Select the Most Adapted XAI Solution",
    "venue": "International Conference on Information and Knowledge Management",
    "year": 2022,
    "referenceCount": 47,
    "citationCount": 18,
    "openAccessPdf": {
      "url": "https://hal.science/hal-03854778/file/AutoXAI_CIKM-2.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.02795, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2022-10-06",
    "authors": [
      {
        "authorId": "2002008670",
        "name": "Robin Cugny"
      },
      {
        "authorId": "2384104411",
        "name": "Julien Aligon"
      },
      {
        "authorId": "2125848513",
        "name": "Max Chevalier"
      },
      {
        "authorId": "1397757929",
        "name": "G. Roman-Jimenez"
      },
      {
        "authorId": "2862341",
        "name": "O. Teste"
      }
    ],
    "abstract": "A large number of XAI (eXplainable Artificial Intelligence) solutions have been proposed in recent years. Recently, thanks to new XAI evaluation metrics, it has become possible to compare these XAI solutions. However, selecting the most relevant XAI solution among all this diversity is still a tedious task, especially if a user has specific needs and constraints. In this paper, we propose AutoXAI, a framework that recommends the best XAI solution and its hyperparameters according to specified XAI evaluation metrics while considering the user's context (dataset, machine learning model, XAI needs and constraints). It adapts approaches from context-aware recommender systems on one side and strategies of optimization and evaluation from AutoML (Automated Machine Learning) on the other. Through two use cases, we show that AutoXAI recommends XAI solutions adapted to the user's needs with the best hyperparameters matching the user's constraints.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "36f7f12caec947253341aac5dc9acf4a72730a3d",
    "externalIds": {
      "DOI": "10.1109/ICEDEG58167.2023.10122066",
      "CorpusId": 258728871
    },
    "url": "https://www.semanticscholar.org/paper/36f7f12caec947253341aac5dc9acf4a72730a3d",
    "title": "Designing a Framework for Explainable Health Recommender System Based on the Ecuadorian Data Protection Regulations",
    "venue": "International Conference on eDemocracy & eGovernment",
    "year": 2023,
    "referenceCount": 49,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICEDEG58167.2023.10122066?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICEDEG58167.2023.10122066, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference",
      "Review"
    ],
    "publicationDate": "2023-04-03",
    "authors": [
      {
        "authorId": "2217495891",
        "name": "Byron Jaramillo"
      },
      {
        "authorId": "1410416827",
        "name": "Edison Loza-Aguirre"
      },
      {
        "authorId": "48928268",
        "name": "Luis Terán"
      }
    ],
    "abstract": "Technological advances and development in explainable artificial intelligence (XAI) in the health sector are growing worldwide. However, according to different studies in Latin America, there is a lack of knowledge, know-how, and technical skills to master these new technologies. Most medical software professionals can be considered “black boxes.” This paper focuses on a case study on the lack of technological uses of XAI methods in the health Ecuadorian medical system to support health professionals in treating and managing patients' diseases based on Ecuadorian data protection regulations. A survey was conducted with 71 Ecuadorian medical professionals to know their technological problems in medical appointments. Related works were reviewed to understand techniques or other existing solutions in the XAI field that can complement the designing of so-called health recommender systems. This paper shows the main results of the survey. These results provide guidelines for further designing a framework for managing sensitive data and developing the XAI health recommender system to optimize medical professionals' decision-making, avoiding third-party use of sensitive patient data for other uses.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "568f2660b6369e6e4885f51106344debdc8cfcd7",
    "externalIds": {
      "DBLP": "conf/xai/Caro-MartinezJD24",
      "DOI": "10.1007/978-3-031-63797-1_2",
      "CorpusId": 273496250
    },
    "url": "https://www.semanticscholar.org/paper/568f2660b6369e6e4885f51106344debdc8cfcd7",
    "title": "Graph-Based Interface for Explanations by Examples in Recommender Systems: A User Study",
    "venue": "xAI",
    "year": 2024,
    "referenceCount": 0,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-031-63797-1_2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-031-63797-1_2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1410523548",
        "name": "Marta Caro-Martínez"
      },
      {
        "authorId": "1409231466",
        "name": "J. L. Jorro-Aragoneses"
      },
      {
        "authorId": "2303166233",
        "name": "Belén Díaz-Agudo"
      },
      {
        "authorId": "1389962946",
        "name": "Juan A. Recio-García"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "7ab9326d0d732024e6db53859d32e7ca2527e453",
    "externalIds": {
      "DBLP": "conf/hai/TchappiHPBN23",
      "DOI": "10.1145/3623809.3623945",
      "CorpusId": 265552618
    },
    "url": "https://www.semanticscholar.org/paper/7ab9326d0d732024e6db53859d32e7ca2527e453",
    "title": "Towards Explainable Recommender Systems for Illiterate Users",
    "venue": "International Conference on Human-Agent Interaction",
    "year": 2023,
    "referenceCount": 5,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3623809.3623945?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3623809.3623945, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2023-12-04",
    "authors": [
      {
        "authorId": "51221370",
        "name": "I. Tchappi"
      },
      {
        "authorId": "145539910",
        "name": "J. Hulstijn"
      },
      {
        "authorId": "2269152703",
        "name": "Ephraim Sinyabe Pagou"
      },
      {
        "authorId": "2269463095",
        "name": "Sukriti Bhattacharya"
      },
      {
        "authorId": "2297347",
        "name": "A. Najjar"
      }
    ],
    "abstract": "Explainable AI (XAI) has emerged in recent years as a set of techniques to build systems that enable humans to understand the outcomes produced by artificial intelligent entities. Although these initiatives have advanced over the past few years, most approaches focus on explanations that are meant for literate or even skilled end users such as engineers, researchers etc. Few works available in the literature address the needs of illiterate end-users in XAI (illiterate centered design). This paper proposes a generic model to extract the contents of explanations from a given explainable AI system, and translate them into a representation format that illiterate end users may understand. The usefulness of the model is shown by reference to an application of a food recommender system.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "3417bcc2951241bdf888aa4ce3e0b20305bce818",
    "externalIds": {
      "DBLP": "journals/sigir/BrusilovskyGFLPSW23",
      "DOI": "10.1145/3642979.3642999",
      "CorpusId": 266595574
    },
    "url": "https://www.semanticscholar.org/paper/3417bcc2951241bdf888aa4ce3e0b20305bce818",
    "title": "Report on the 10th Joint Workshop on Interfaces and Human Decision Making for Recommender Systems (IntRS 2023) at ACM RecSys 2023",
    "venue": "SIGIR Forum",
    "year": 2023,
    "referenceCount": 7,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "https://research.tue.nl/files/323926595/3642979.3642999.pdf",
      "status": "GREEN",
      "license": "other-oa",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3642979.3642999?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3642979.3642999, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-01",
    "authors": [
      {
        "authorId": "2276738068",
        "name": "Peter Brusilovsky"
      },
      {
        "authorId": "6042025",
        "name": "M. D. Gemmis"
      },
      {
        "authorId": "1688968",
        "name": "A. Felfernig"
      },
      {
        "authorId": "2270202662",
        "name": "Pasquale Lops"
      },
      {
        "authorId": "2683903",
        "name": "Marco Polignano"
      },
      {
        "authorId": "2270205261",
        "name": "Giovanni Semeraro"
      },
      {
        "authorId": "1918235",
        "name": "M. Willemsen"
      }
    ],
    "abstract": "The 10th edition of the Joint Workshop on Interfaces and Human Decision Making for Recommender Systems was held as part of the 17th ACM Conference on Recommender Systems (RecSys), the premier international forum for the presentation of new research results, systems and techniques in the broad field of recommender systems. The workshop was organized as a hybrid event: the physical session took place on September 18th at the venue of the main conference, Singapore, with the possibility for authors to present remotely. The IntRS workshop brings together an interdisciplinary community of researchers and practitioners who share research on new recommender systems (informed by psychology), including new design technologies and evaluation methodologies, and aim to identify critical challenges and emerging topics in the field. This year we focused particularly on topics related to Human-Centered AI, Explainability of decision-making models, User-adaptive XAI systems, which are becoming more and more popular in the last years, especially in domains where recommended options might have ethical and legal impacts on users. The integration of XAI with recommender systems is crucial for enhancing their transparency, interpretability, and accountability. This topic attracted a lot of interest from the community, as demonstrated by the fact that several workshop papers proposed methods for recommendation explanations. Date: 18 September 2023. Website: https://intrs2023.wordpress.com.",
    "affiliations": [],
    "countries": [],
    "num_authors": 7
  },
  {
    "paperId": "23977d9bb79aaa08afc6ff0a3965c81b1d30b999",
    "externalIds": {
      "DOI": "10.1177/21695067231192247",
      "CorpusId": 264370385
    },
    "url": "https://www.semanticscholar.org/paper/23977d9bb79aaa08afc6ff0a3965c81b1d30b999",
    "title": "Model Blindness II: Investigating a Model-Based Recommender System’s Impact on Decision Making",
    "venue": "Proceedings of the Human Factors and Ergonomics Society Annual Meeting",
    "year": 2023,
    "referenceCount": 9,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/21695067231192247?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/21695067231192247, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-01",
    "authors": [
      {
        "authorId": "65951406",
        "name": "Sweta Parmar"
      },
      {
        "authorId": "47988909",
        "name": "David A. Illingworth"
      },
      {
        "authorId": "2110843039",
        "name": "Rick P. Thomas"
      }
    ],
    "abstract": "Adoption and use of misspecified models can lead to impoverished decision-making—a phenomenon we term model blindness . A series of two experiments investigated the consequences of model blindness on human decision-making and performance and how those consequences can be mitigated via an explainable AI (XAI) intervention. The experiments implemented a simulated route recommender system as a Decision Support System (DSS) with a true data-generating model. In Experiment 1, the true model generating the recommended routes was misspecified at two different levels to impose model blindness on users. In Experiment 2, the same route-recommender system was augmented with a mitigation technique to overcome the impact of model-misspecifications on decision-making. Overall, the results of both experiments provided little support for performance degradation. The participants' decision strategies revealed that they could understand model limitations from feedback and explanations and could adapt their strategy to account for those misspecifications.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "5b1297768c1b1482b43b272b6510b4781042e52b",
    "externalIds": {
      "DBLP": "conf/xai/AragonZM23",
      "CorpusId": 265467318
    },
    "url": "https://www.semanticscholar.org/paper/5b1297768c1b1482b43b272b6510b4781042e52b",
    "title": "When Attention Turn To Be Explanation. A Case Study in Recommender Systems",
    "venue": "xAI",
    "year": 2023,
    "referenceCount": 31,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1740656487",
        "name": "Ricardo Anibal Matamoros Aragon"
      },
      {
        "authorId": "1947730",
        "name": "I. Zoppis"
      },
      {
        "authorId": "2193901169",
        "name": "Sara L. Manzoni"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "b54d607265ceb0b8d38b12d2c6ecfa9aa0e56bec",
    "externalIds": {
      "DBLP": "journals/corr/abs-2502-10230",
      "ArXiv": "2502.10230",
      "DOI": "10.1007/978-3-031-61000-4_11",
      "CorpusId": 270555065
    },
    "url": "https://www.semanticscholar.org/paper/b54d607265ceb0b8d38b12d2c6ecfa9aa0e56bec",
    "title": "ProReco: A Process Discovery Recommender System",
    "venue": "CAiSE Forum",
    "year": 2025,
    "referenceCount": 0,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.10230, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-02-14",
    "authors": [
      {
        "authorId": "2155296705",
        "name": "Tsung-Hao Huang"
      },
      {
        "authorId": "2306949685",
        "name": "Tarek Junied"
      },
      {
        "authorId": "2065528105",
        "name": "Marco Pegoraro"
      },
      {
        "authorId": "2278265731",
        "name": "W. M. Aalst"
      }
    ],
    "abstract": "Process discovery aims to automatically derive process models from historical execution data (event logs). While various process discovery algorithms have been proposed in the last 25 years, there is no consensus on a dominating discovery algorithm. Selecting the most suitable discovery algorithm remains a challenge due to competing quality measures and diverse user requirements. Manually selecting the most suitable process discovery algorithm from a range of options for a given event log is a time-consuming and error-prone task. This paper introduces ProReco, a Process discovery Recommender system designed to recommend the most appropriate algorithm based on user preferences and event log characteristics. ProReco incorporates state-of-the-art discovery algorithms, extends the feature pools from previous work, and utilizes eXplainable AI (XAI) techniques to provide explanations for its recommendations.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "764498c12194f84ee56b295dfcea4058d7c0af2f",
    "externalIds": {
      "DOI": "10.1109/ETHICS65148.2025.11098238",
      "CorpusId": 280537691
    },
    "url": "https://www.semanticscholar.org/paper/764498c12194f84ee56b295dfcea4058d7c0af2f",
    "title": "Understanding Explainability in Recommender Systems-User Insights and Perspectives",
    "venue": "Ethics: An International Journal of Social, Political, and Legal Philosophy",
    "year": 2025,
    "referenceCount": 27,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ETHICS65148.2025.11098238?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ETHICS65148.2025.11098238, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "2025-06-06",
    "authors": [
      {
        "authorId": "2375057999",
        "name": "Abtin Karimi Malek Abadi"
      },
      {
        "authorId": "2283431031",
        "name": "Faisal Kalota"
      },
      {
        "authorId": "2375060208",
        "name": "Atefeh Jamil Pour"
      }
    ],
    "abstract": "This paper presents our preliminary findings from a systematic literature review on explainability in recommender systems from a user-centered perspective. Despite extensive literature on explainability in Artificial Intelligence (XAI), this study focuses specifically on how explain ability in recommender systems affects user trust, taking into account insights drawn from design and Human-Computer Interaction (HCI). To this end, we extracted 387 journal and conference papers from ACM, IEEE, Taylor and Francis, Science Direct, and Springer. After applying inclusion and exclusion criteria, 10 relevant articles published between 2018 and 2024 were selected for this analysis. According to the results, users value justifications for recommendations to better understand why certain products or services are suggested. Moreover, scrutability is crucial for enabling users to provide feedback when recommendations do not align with their preferences. Explanations should be informative and easily understandable to enhance transparency and decision-making efficiency. The final component of fostering user trust, satisfaction, and transparency is to provide adaptive explanations, allowing users to control the level of detail based on their mental models and personal characteristics.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "72d90b8ffba9da569eef63bc41b11edcdd7b082f",
    "externalIds": {
      "ArXiv": "2509.06475",
      "DBLP": "journals/corr/abs-2509-06475",
      "DOI": "10.48550/arXiv.2509.06475",
      "CorpusId": 281203161
    },
    "url": "https://www.semanticscholar.org/paper/72d90b8ffba9da569eef63bc41b11edcdd7b082f",
    "title": "Explained, yet misunderstood: How AI Literacy shapes HR Managers' interpretation of User Interfaces in Recruiting Recommender Systems",
    "venue": "arXiv.org",
    "year": 2025,
    "referenceCount": 33,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.06475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-09-08",
    "authors": [
      {
        "authorId": "2300230687",
        "name": "Yannick Kalff"
      },
      {
        "authorId": "78197292",
        "name": "Katharina Simbeck"
      }
    ],
    "abstract": "AI-based recommender systems increasingly influence recruitment decisions. Thus, transparency and responsible adoption in Human Resource Management (HRM) are critical. This study examines how HR managers'AI literacy influences their subjective perception and objective understanding of explainable AI (XAI) elements in recruiting recommender dashboards. In an online experiment, 410 German-based HR managers compared baseline dashboards to versions enriched with three XAI styles: important features, counterfactuals, and model criteria. Our results show that the dashboards used in practice do not explain AI results and even keep AI elements opaque. However, while adding XAI features improves subjective perceptions of helpfulness and trust among users with moderate or high AI literacy, it does not increase their objective understanding. It may even reduce accurate understanding, especially with complex explanations. Only overlays of important features significantly aided the interpretations of high-literacy users. Our findings highlight that the benefits of XAI in recruitment depend on users'AI literacy, emphasizing the need for tailored explanation strategies and targeted literacy training in HRM to ensure fair, transparent, and effective adoption of AI.",
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "b19f87290ace50cdc346825a19b1cfd78841afdb",
    "externalIds": {
      "DBLP": "conf/recsys/Parra22",
      "CorpusId": 252781810
    },
    "url": "https://www.semanticscholar.org/paper/b19f87290ace50cdc346825a19b1cfd78841afdb",
    "title": "From User Control and Explainability in Recommendation Interfaces to Visual XAI",
    "venue": "IntRS@RecSys",
    "year": 2022,
    "referenceCount": 5,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "145831267",
        "name": "Denis Parra"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 1
  },
  {
    "paperId": "3e7959e8d5dafea2b15be63ca197425e34545b72",
    "externalIds": {
      "DBLP": "conf/aiia/LietoPSZD22",
      "CorpusId": 255227243
    },
    "url": "https://www.semanticscholar.org/paper/3e7959e8d5dafea2b15be63ca197425e34545b72",
    "title": "Formal Methods Meet XAI: the Tool DEGARI 2.0 for Social Inclusion",
    "venue": "OVERLAY@AI*IA",
    "year": 2022,
    "referenceCount": 15,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1792687",
        "name": "Antonio Lieto"
      },
      {
        "authorId": "34993275",
        "name": "G. Pozzato"
      },
      {
        "authorId": "2261268858",
        "name": "Manuel Striani"
      },
      {
        "authorId": "12975653",
        "name": "S. Zoia"
      },
      {
        "authorId": "144411873",
        "name": "R. Damiano"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "6616971bae360a3bfdb40b99c1d80c87bac7890a",
    "externalIds": {
      "DBLP": "journals/cogcom/CesariniMPSX24",
      "DOI": "10.1007/s12559-024-10325-w",
      "CorpusId": 271749158
    },
    "url": "https://www.semanticscholar.org/paper/6616971bae360a3bfdb40b99c1d80c87bac7890a",
    "title": "Explainable AI for Text Classification: Lessons from a Comprehensive Evaluation of Post Hoc Methods",
    "venue": "Cognitive Computation",
    "year": 2024,
    "referenceCount": 34,
    "citationCount": 16,
    "openAccessPdf": {
      "url": "https://doi.org/10.1007/s12559-024-10325-w",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s12559-024-10325-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s12559-024-10325-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-08-06",
    "authors": [
      {
        "authorId": "2315171615",
        "name": "Mirko Cesarini"
      },
      {
        "authorId": "36039638",
        "name": "Lorenzo Malandri"
      },
      {
        "authorId": "79386137",
        "name": "Filippo Pallucchini"
      },
      {
        "authorId": "101904272",
        "name": "Andrea Seveso"
      },
      {
        "authorId": "79386137",
        "name": "Filippo Pallucchini"
      }
    ],
    "abstract": "This paper addresses the notable gap in evaluating eXplainable Artificial Intelligence (XAI) methods for text classification. While existing frameworks focus on assessing XAI in areas such as recommender systems and visual analytics, a comprehensive evaluation is missing. Our study surveys and categorises recent post hoc XAI methods according to their scope of explanation and output format. We then conduct a systematic evaluation, assessing the effectiveness of these methods across varying scopes and levels of output granularity using a combination of objective metrics and user studies. Key findings reveal that feature-based explanations exhibit higher fidelity than rule-based ones. While global explanations are perceived as more satisfying and trustworthy, they are less practical than local explanations. These insights enhance understanding of XAI in text classification and offer valuable guidance for developing effective XAI systems, enabling users to evaluate each explainer’s pros and cons and select the most suitable one for their needs.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "5b60cfa5ada16c22566e1eea48b166a861391afd",
    "externalIds": {
      "DBLP": "journals/pami/RongLNFQUSKK24",
      "ArXiv": "2210.11584",
      "DOI": "10.1109/TPAMI.2023.3331846",
      "CorpusId": 261558562,
      "PubMed": "37956008"
    },
    "url": "https://www.semanticscholar.org/paper/5b60cfa5ada16c22566e1eea48b166a861391afd",
    "title": "Towards Human-Centered Explainable AI: A Survey of User Studies for Model Explanations",
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "year": 2022,
    "referenceCount": 289,
    "citationCount": 127,
    "openAccessPdf": {
      "url": "https://ieeexplore.ieee.org/ielx7/34/4359286/10316181.pdf",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.11584, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2022-10-20",
    "authors": [
      {
        "authorId": "2057815422",
        "name": "Yao Rong"
      },
      {
        "authorId": "2130899453",
        "name": "Tobias Leemann"
      },
      {
        "authorId": "2237989866",
        "name": "Thai-trang Nguyen"
      },
      {
        "authorId": "2188651075",
        "name": "Lisa Fiedler"
      },
      {
        "authorId": "2237993118",
        "name": "Peizhu Qian"
      },
      {
        "authorId": "3311584",
        "name": "Vaibhav Unhelkar"
      },
      {
        "authorId": "2291244896",
        "name": "Tina Seidel"
      },
      {
        "authorId": "1686448",
        "name": "Gjergji Kasneci"
      },
      {
        "authorId": "1884159",
        "name": "Enkelejda Kasneci"
      }
    ],
    "abstract": "Explainable AI (XAI) is widely viewed as a sine qua non for ever-expanding AI research. A better understanding of the needs of XAI users, as well as human-centered evaluations of explainable models are both a necessity and a challenge. In this paper, we explore how human-computer interaction (HCI) and AI researchers conduct user studies in XAI applications based on a systematic literature review. After identifying and thoroughly analyzing 97 core papers with human-based XAI evaluations over the past five years, we categorize them along the measured characteristics of explanatory methods, namely trust, understanding, usability, and human-AI collaboration performance. Our research shows that XAI is spreading more rapidly in certain application domains, such as recommender systems than in others, but that user evaluations are still rather sparse and incorporate hardly any insights from cognitive or social sciences. Based on a comprehensive discussion of best practices, i.e., common models, design choices, and measures in user studies, we propose practical guidelines on designing and conducting user studies for XAI researchers and practitioners. Lastly, this survey also highlights several open research directions, particularly linking psychological science and human-centered XAI.",
    "affiliations": [],
    "countries": [],
    "num_authors": 9
  },
  {
    "paperId": "c4ded7b14fbc704d9513d0174f4313c1a65da2f1",
    "externalIds": {
      "CorpusId": 274785024
    },
    "url": "https://www.semanticscholar.org/paper/c4ded7b14fbc704d9513d0174f4313c1a65da2f1",
    "title": "An Automated Online Recommender System for Stroke Risk Assessment",
    "venue": "",
    "year": null,
    "referenceCount": 10,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null
    },
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2334746063",
        "name": "Shams Khan"
      },
      {
        "authorId": "2316632846",
        "name": "Nour Dekhil"
      },
      {
        "authorId": "2325058168",
        "name": "Ehsan Mamatjan"
      },
      {
        "authorId": "2336062843",
        "name": "Safwat Hassan"
      },
      {
        "authorId": "2335035043",
        "name": "Yasin Mamatjan"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "bd9bebd07e6d649b8ddd1019b78f56536d5773ce",
    "externalIds": {
      "DBLP": "journals/tiis/Hernandez-Bocanegra23",
      "DOI": "10.1145/3579541",
      "CorpusId": 256033256
    },
    "url": "https://www.semanticscholar.org/paper/bd9bebd07e6d649b8ddd1019b78f56536d5773ce",
    "title": "Explaining Recommendations through Conversations: Dialog Model and the Effects of Interface Type and Degree of Interactivity",
    "venue": "ACM Trans. Interact. Intell. Syst.",
    "year": 2023,
    "referenceCount": 133,
    "citationCount": 23,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3579541?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3579541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-01-21",
    "authors": [
      {
        "authorId": "1573207580",
        "name": "Diana C. Hernandez-Bocanegra"
      },
      {
        "authorId": "145758499",
        "name": "J. Ziegler"
      }
    ],
    "abstract": "Explaining system-generated recommendations based on user reviews can foster users’ understanding and assessment of the recommended items and the recommender system (RS) as a whole. While up to now explanations have mostly been static, shown in a single presentation unit, some interactive explanatory approaches have emerged in explainable artificial intelligence (XAI), making it easier for users to examine system decisions and to explore arguments according to their information needs. However, little is known about how interactive interfaces should be conceptualized and designed to meet the explanatory aims of transparency, effectiveness, and trust in RS. Thus, we investigate the potential of interactive, conversational explanations in review-based RS and propose an explanation approach inspired by dialog models and formal argument structures. In particular, we investigate users’ perception of two different interface types for presenting explanations, a graphical user interface (GUI)-based dialog consisting of a sequence of explanatory steps, and a chatbot-like natural-language interface. Since providing explanations by means of natural language conversation is a novel approach, there is a lack of understanding how users would formulate their questions with a corresponding lack of datasets. We thus propose an intent model for explanatory queries and describe the development of ConvEx-DS, a dataset containing intent annotations of 1,806 user questions in the domain of hotels, that can be used to to train intent detection methods as part of the development of conversational agents for explainable RS. We validate the model by measuring user-perceived helpfulness of answers given based on the implemented intent detection. Finally, we report on a user study investigating users’ evaluation of the two types of interactive explanations proposed (GUI and chatbot), and to test the effect of varying degrees of interactivity that result in greater or lesser access to explanatory information. By using Structural Equation Modeling, we reveal details on the relationships between the perceived quality of an explanation and the explanatory objectives of transparency, trust, and effectiveness. Our results show that providing interactive options for scrutinizing explanatory arguments has a significant positive influence on the evaluation by users (compared to low interactive alternatives). Results also suggest that user characteristics such as decision-making style may have a significant influence on the evaluation of different types of interactive explanation interfaces.",
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "01e41be5da8ab3e3a02c985641399834baed4b64",
    "externalIds": {
      "DBLP": "journals/corr/abs-2503-04261",
      "ArXiv": "2503.04261",
      "DOI": "10.1109/DCOSS-IoT65416.2025.00156",
      "CorpusId": 276812879
    },
    "url": "https://www.semanticscholar.org/paper/01e41be5da8ab3e3a02c985641399834baed4b64",
    "title": "VirtualXAI: A User-Centric Framework for Explainability Assessment Leveraging GPT-Generated Personas",
    "venue": "2025 21st International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT)",
    "year": 2025,
    "referenceCount": 44,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.04261, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2025-03-06",
    "authors": [
      {
        "authorId": "2042705161",
        "name": "Georgios Makridis"
      },
      {
        "authorId": "2204584082",
        "name": "Vasileios Koukos"
      },
      {
        "authorId": "90773902",
        "name": "G. Fatouros"
      },
      {
        "authorId": "1681006",
        "name": "D. Kyriazis"
      }
    ],
    "abstract": "In today's data-driven era, computational systems generate vast amounts of data that drive the digital transformation of industries, where Artificial Intelligence (AI) plays a key role. Currently, the demand for eXplainable AI (XAI) has increased to enhance the interpretability, transparency, and trustworthiness of AI models. However, evaluating XAI methods remains challenging: existing evaluation frameworks typically focus on quantitative properties such as fidelity, consistency, and stability without taking into account qualitative characteristics such as satisfaction and interpretability. In addition, practitioners face a lack of guidance in selecting appropriate datasets, AI models, and XAI methods - a major hurdle in human-AI collaboration. To address these gaps, we propose a framework that integrates quantitative benchmarking with qualitative user assessments through virtual personas based on the “Anthology” of backstories of the Large Language Model (LLM). Our framework also incorporates a content-based recommender system that leverages dataset-specific characteristics to match new input data with a repository of benchmarked datasets. This yields an estimated XAI score and provides tailored recommendations for both the optimal AI model and the XAI method for a given scenario.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "a13afb9a2f069074953fad112e6864d9bc055334",
    "externalIds": {
      "DBLP": "journals/pacmhci/VanbrabantERV25",
      "DOI": "10.1145/3734191",
      "CorpusId": 279603579
    },
    "url": "https://www.semanticscholar.org/paper/a13afb9a2f069074953fad112e6864d9bc055334",
    "title": "ECHO: Enhancing Conversational Explainable AI through Tool-Augmented Language Models",
    "venue": "Proc. ACM Hum. Comput. Interact.",
    "year": 2025,
    "referenceCount": 85,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3734191?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3734191, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-06-27",
    "authors": [
      {
        "authorId": "2331325931",
        "name": "Sebe Vanbrabant"
      },
      {
        "authorId": "2331327399",
        "name": "Gilles Eerlings"
      },
      {
        "authorId": "1400932629",
        "name": "Gustavo Alberto Rovelo Ruiz"
      },
      {
        "authorId": "3202396",
        "name": "D. Vanacken"
      }
    ],
    "abstract": "This paper introduces ECHO, an LLM-powered system framework to explore and interrogate the internals of AI models through tool-augmented language models. While traditional XAI methods typically offer a small and technical set of explanation types, ECHO advances the accessibility and usability of AI explanations through a conversational approach, combining LLMs with a collection of tools and a human-in-the-loop process. We identify various explanation types from the literature, for which we create a set of predefined tools for tabular data. Using a modular architecture, ECHO integrates these predefined tools with dynamically generated tools to interact with AI models, facilitating tailored explanations for a large variety of user queries. This paper details ECHO’s design, implementation, and use cases, demonstrating its capabilities in the context of a movie recommender, healthcare decision tree and neural network for educational classification.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "9a7a9794af00bfc8017496a31d650e7c4e38e6e9",
    "externalIds": {
      "DOI": "10.1145/3732292",
      "CorpusId": 278082217
    },
    "url": "https://www.semanticscholar.org/paper/9a7a9794af00bfc8017496a31d650e7c4e38e6e9",
    "title": "LXR: Learning to eXplain Recommendations",
    "venue": "ACM Transactions on Recommender Systems",
    "year": 2025,
    "referenceCount": 43,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3732292?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3732292, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-04-24",
    "authors": [
      {
        "authorId": "2300576841",
        "name": "Liya Gurevitch"
      },
      {
        "authorId": "3368624",
        "name": "Veronika Bogina"
      },
      {
        "authorId": "48797862",
        "name": "Oren Barkan"
      },
      {
        "authorId": "2357449423",
        "name": "Yahlly Schein"
      },
      {
        "authorId": "2260651766",
        "name": "Yehonatan Elisha"
      },
      {
        "authorId": "1683070",
        "name": "Noam Koenigstein"
      }
    ],
    "abstract": "Recommender systems have become integral to many online services, leveraging user data to provide personalized recommendations. However, as these systems grow in complexity, understanding the rationale behind their recommendations becomes increasingly difficult. Explainable Artificial Intelligence (XAI) has emerged as a crucial field addressing this challenge, particularly in ensuring transparency and trustworthiness in automated decision-making processes. In this paper, we introduce Learning to eXplain Recommendations (LXR), a scalable, model-agnostic framework designed to generate counterfactually correct explanations for recommender systems. LXR generates explanations for recommendations produced by any differentiable recommender system. By leveraging both factual and counterfactual loss terms, LXR offers robust, accurate, and computationally efficient explanations that reflect the model’s internal decision-making process. A key feature of LXR is its focus on the factual correctness of explanations through counterfactual reasoning, bridging the gap between plausible and accurate explanations. Unlike traditional approaches that rely on exhaustive perturbations of user data, LXR uses a self-supervised learning method to generate explanations efficiently, without sacrificing accuracy. LXR operates in two stages: a pre-training step and a novel Inference-Time Fine-tuning (ITF) step that refines explanations at the individual recommendation level, significantly improving accuracy with minimal computational overhead. Additionally, LXR is applied to hybrid recommender models incorporating demographic data, demonstrating its versatility across real-world scenarios. Finally, we also showcase LXR’s ability to explain recommendations at various ranks within a user’s recommendation list. As a secondary contribution, we introduce several novel evaluation metrics, inspired by saliency maps from computer vision, to rigorously assess the counterfactual correctness of explanations in recommender systems. Our results demonstrate that LXR sets a new benchmark for explainability, providing accurate, transparent, and interpretable explanations. The code is available on our GitHub repository: https://github.com/DeltaLabTLV/LXR.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "4a5e91ae113e0f71e1313cca43d8445246dd4d54",
    "externalIds": {
      "DBLP": "conf/gecco/LipinskiB24",
      "DOI": "10.1145/3638530.3664156",
      "CorpusId": 271645596
    },
    "url": "https://www.semanticscholar.org/paper/4a5e91ae113e0f71e1313cca43d8445246dd4d54",
    "title": "Explaining Session-based Recommendations using Grammatical Evolution",
    "venue": "GECCO Companion",
    "year": 2024,
    "referenceCount": 26,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3638530.3664156?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3638530.3664156, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2024-07-14",
    "authors": [
      {
        "authorId": "2267039591",
        "name": "Piotr Lipinski"
      },
      {
        "authorId": "2267065463",
        "name": "Klaudia Balcer"
      }
    ],
    "abstract": "This paper concerns explaining session-based recommendations using Grammatical Evolution. A session-based recommender system processes a given sequence of products browsed by a user and suggests the most relevant next product to display to the user. State-of-the-art session-based recommender systems are often a type of deep learning black box, so explaining their results is a challenge. In this paper, we propose an approach with a grammatical expression that provides explanations of recommendations generated by session-based recommender systems as well as an evolutionary algorithm, GE-XAI-SBRS, based on Grammatical Evolution, with its own initialization and crossover operators, to construct such a grammatical expression. Our approach uses latent product representations, so-called vector embeddings, generated by the recommender systems and providing some additional knowledge on dependencies between products. Computational experiments on the YooChoose dataset being one of the most popular session-based benchmarks, and the recommendations generated by the Target Attentive Graph Neural Network (TAGNN) model confirm the usefulness of the proposed approach, the efficiency of the proposed algorithm and outperforming the regular GE algorithm in the task under consideration.",
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "1f71252d4127d39f8eeb2d628319f653bc61e791",
    "externalIds": {
      "DBLP": "conf/iui/SimicSPVS24",
      "DOI": "10.1145/3640544.3645217",
      "CorpusId": 268962158
    },
    "url": "https://www.semanticscholar.org/paper/1f71252d4127d39f8eeb2d628319f653bc61e791",
    "title": "XAIVIER: Time Series Classifier Verification with Faithful Explainable AI",
    "venue": "IUI Companion",
    "year": 2024,
    "referenceCount": 18,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3640544.3645217",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3640544.3645217?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3640544.3645217, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2024-03-18",
    "authors": [
      {
        "authorId": "51520289",
        "name": "Ilija Simic"
      },
      {
        "authorId": "2275026650",
        "name": "Santokh Singh"
      },
      {
        "authorId": "2274477292",
        "name": "Christian Partl"
      },
      {
        "authorId": "89180102",
        "name": "Eduardo E. Veas"
      },
      {
        "authorId": "1788794",
        "name": "Vedran Sabol"
      }
    ],
    "abstract": "Ensuring that a machine learning model performs as intended is a critical step before it can be used in practice. This is commonly done by measuring a model’s predictive performance (e.g., accuracy). However, in high-stakes settings it is often necessary to verify on which data aspects the model actually relies on. This demo presents XAIVIER, the eXplainable AI VIsual Explorer and Recommender, a web application for interactive XAI on time series data. XAIVIER supports dataset exploration and model inspection, allowing users to explain model predictions using various explainer methods. An explainer recommender is provided to advise users which explainer delivers most faithful explanations for their dataset and model. Finally, explanation-based grouping is provided to reveal the model’s underlying decision-making strategies. The proposed set of features aims to cover the full model verification use case for time series classifiers. A demo of XAIVIER is available at https://xai-explorer-demo.know-center.at",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "57829b4f7ddf3a7ca3b7a3c40d482b038a082e6e",
    "externalIds": {
      "DBLP": "journals/corr/abs-2507-05636",
      "ArXiv": "2507.05636",
      "DOI": "10.48550/arXiv.2507.05636",
      "CorpusId": 280113349
    },
    "url": "https://www.semanticscholar.org/paper/57829b4f7ddf3a7ca3b7a3c40d482b038a082e6e",
    "title": "Graph Learning",
    "venue": "arXiv.org",
    "year": 2025,
    "referenceCount": 0,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.05636, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2025-07-07",
    "authors": [
      {
        "authorId": "2283308587",
        "name": "Feng Xia"
      },
      {
        "authorId": "1726114642",
        "name": "Ciyuan Peng"
      },
      {
        "authorId": "2319211734",
        "name": "Jing Ren"
      },
      {
        "authorId": "2150707265",
        "name": "F. Febrinanto"
      },
      {
        "authorId": "2298756911",
        "name": "Renqiang Luo"
      },
      {
        "authorId": "2102095",
        "name": "Vidya Saikrishna"
      },
      {
        "authorId": "2116619083",
        "name": "Shuo Yu"
      },
      {
        "authorId": "2345144690",
        "name": "Xiangjie Kong"
      }
    ],
    "abstract": "Graph learning has rapidly evolved into a critical subfield of machine learning and artificial intelligence (AI). Its development began with early graph-theoretic methods, gaining significant momentum with the advent of graph neural networks (GNNs). Over the past decade, progress in scalable architectures, dynamic graph modeling, multimodal learning, generative AI, explainable AI (XAI), and responsible AI has broadened the applicability of graph learning to various challenging environments. Graph learning is significant due to its ability to model complex, non-Euclidean relationships that traditional machine learning struggles to capture, thus better supporting real-world applications ranging from drug discovery and fraud detection to recommender systems and scientific reasoning. However, challenges like scalability, generalization, heterogeneity, interpretability, and trustworthiness must be addressed to unlock its full potential. This survey provides a comprehensive introduction to graph learning, focusing on key dimensions including scalable, temporal, multimodal, generative, explainable, and responsible graph learning. We review state-of-the-art techniques for efficiently handling large-scale graphs, capturing dynamic temporal dependencies, integrating heterogeneous data modalities, generating novel graph samples, and enhancing interpretability to foster trust and transparency. We also explore ethical considerations, such as privacy and fairness, to ensure responsible deployment of graph learning models. Additionally, we identify and discuss emerging topics, highlighting recent integration of graph learning and other AI paradigms and offering insights into future directions. This survey serves as a valuable resource for researchers and practitioners seeking to navigate the rapidly evolving landscape of graph learning.",
    "affiliations": [],
    "countries": [],
    "num_authors": 8
  },
  {
    "paperId": "bb0530490a159f41d6b60a13a4130e23c9b95943",
    "externalIds": {
      "DBLP": "conf/biostec/BallettiGLOSZ25",
      "DOI": "10.5220/0013179400003911",
      "CorpusId": 276641606
    },
    "url": "https://www.semanticscholar.org/paper/bb0530490a159f41d6b60a13a4130e23c9b95943",
    "title": "Integrating Gait and Clinical Data with Explainable Artificial Intelligence for Parkinson's Prediction: The EDAM System",
    "venue": "BIOSTEC : HEALTHINF",
    "year": 2025,
    "referenceCount": 38,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5220/0013179400003911?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5220/0013179400003911, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "11676277",
        "name": "Nicoletta Balletti"
      },
      {
        "authorId": "2161966049",
        "name": "Emanuela Guglielmi"
      },
      {
        "authorId": "1394880067",
        "name": "G. Laudato"
      },
      {
        "authorId": "2287253439",
        "name": "Rocco Oliveto"
      },
      {
        "authorId": "1622692796",
        "name": "Jonathan Simeone"
      },
      {
        "authorId": "2287227303",
        "name": "Roberto Zinni"
      }
    ],
    "abstract": ": Several machine learning (ML) approaches have been introduced for gait and posture analysis, recognized as crucial for early diagnosing neurological disorders, particularly Parkinson’s disease. However, these existing methods are often limited by their lack of integration with other clinical biomarkers and their inability to provide transparent, explainable predictions. To overcome these limitations, we introduce EDAM (Explain-able Diagnosis Recommender), a system that leverages Explainable Artificial Intelligence (XAI) techniques to deliver both accurate predictions and clear, interpretable explanations of its diagnostic decisions. We evaluate the capabilities of EDAM in two main areas: distinguishing between healthy individuals and those with Parkinson’s disease, and classifying abnormal gait patterns that may indicate early-stage Parkinson’s disease. To ensure a comprehensive evaluation, we constructed one of the largest known dataset by merging and stan-dardizing several existing datasets. This dataset includes 557 features and 7,303 labelled instances, covering a wide range of gait patterns and clinical features. Results show that EDAM achieves high accuracy in both tasks, demonstrating its potential for early detection of neurological disorders.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "876e295f01340ec72923da896905dd857fb406f8",
    "externalIds": {
      "DOI": "10.32014/2025.2518-1726.366",
      "CorpusId": 281697739
    },
    "url": "https://www.semanticscholar.org/paper/876e295f01340ec72923da896905dd857fb406f8",
    "title": "RECOMMENDATION ALGORITHMS FOR EDUCATIONAL PREFERENCES: A REVIEW",
    "venue": "NEWS of National Academy of Sciences of the Republic of Kazakhstan",
    "year": 2025,
    "referenceCount": 0,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.32014/2025.2518-1726.366?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.32014/2025.2518-1726.366, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2025-09-27",
    "authors": [
      {
        "authorId": "2383089473",
        "name": "A. Bekarystankyzy"
      },
      {
        "authorId": "2383092951",
        "name": "М. Baizakova"
      },
      {
        "authorId": "2383086249",
        "name": "A. Kassenkhan"
      },
      {
        "authorId": "2383090474",
        "name": "M. Iglikova"
      }
    ],
    "abstract": "Educational recommender systems (ERS) are transforming the learning experience by providing personalized content and course recommendations tailored to the individual needs of students.  These types of systems employ filtering approaches like collaborative and content-based, machine learning and deep learning, big data processing approaches, hybrid models to improve the educational process. The implementation of such systems persistently meets challenges like data sparsity, cold-start problems and issues related with privacy. But the recent achievements in the development of deep learning, knowledge graphs and hybrid learning give promising results of the accuracy of recommendations in e-learning. The paper studies the listed techniques above and the feasibility of engaging large datasets and explainable artificial intelligence(XAI) to boost the process of electronic education.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "de38c35bbd17336a5f108287f95398d7ee985cf8",
    "externalIds": {
      "DOI": "10.22624/aims/maths/v13n2p5",
      "CorpusId": 279364215
    },
    "url": "https://www.semanticscholar.org/paper/de38c35bbd17336a5f108287f95398d7ee985cf8",
    "title": "Enhancing Perceived Transparency and Purchase Decisions through Explanations in Personalized Product Recommendations",
    "venue": "advances in multidisciplinary &amp; scientific research journal publication",
    "year": 2025,
    "referenceCount": 0,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.22624/aims/maths/v13n2p5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.22624/aims/maths/v13n2p5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-06-30",
    "authors": [
      {
        "authorId": "2366901064",
        "name": "U.M. Ene"
      },
      {
        "authorId": "2366895318",
        "name": "F.S. Bakpo"
      },
      {
        "authorId": "2475091",
        "name": "D. Ebem"
      },
      {
        "authorId": "2258278005",
        "name": "C.N. Udanor"
      },
      {
        "authorId": "150011207",
        "name": "A. Eneh"
      }
    ],
    "abstract": "The increasing adoption of artificial intelligence(AI) in financial services has raised concerns about the transparency of product recommender systems. This study investigates how explanations in personalized product recommendations influence users’ perceived transparency and purchase decisions to help customers make informed decisions and improve product adoption. This research builds on concepts from Explainable Artificial Intelligence (XAI) and Dual-Process Theory. It draws from these theories to understand how users respond to financial product suggestions and how explanations support decision-making. The methodology adopted a comparative experimental design with a within-subject approach. Participants from different backgrounds interacted with the system versions. Structured questionnaires were administered to gather responses on perceived transparency and purchase decisions using a 5-point Likert scale. Statistical analysis included descriptive statistics and the Mann-Whitney U test. Results revealed that users perceived the Personalized Product Recommender (PPR) system with explanations as significantly more transparent (U1 = 2279.5, U2 = 2837, U3 = 3192, p < 0.05) and were more likely to act on the recommendations (U = 2889, p < 0.05). Explanations improved transparency and trust, supporting informed purchase decisions. This research provides insight for designing transparent, user-aligned financial recommender models, which implies improved customer experience strategies and AI decision support. This study presents a working explanation framework with real user evaluation, revealing its value for advancing transparency in financial services.\n\nKeywords: Personalized Product Recommender(PPR) System, Transparent explanations, \nDecision-making, Artificial Intelligence(AI) in Financial Services, Explainable Artificial Intelligence(XAI).\n\nEne U.M., Bakpo F.S., Ebem D.U., Udanor C.N. & Eneh A.H. (2025): Enhancing Perceived Transparency and Purchase Decisions through Explanations in Personalized Product Recommendations. Journal of Advances in Mathematical & Computational Science. Vol. 13, No. 2. Pp 53-66. Available online at www.isteams.net/mathematics-computationaljournal. \ndx.doi.org/10.22624/AIMS/MATHS/V13N2P5",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "f919fcb8fbf27804138006578b57f9f0050d4f71",
    "externalIds": {
      "DBLP": "journals/computers/SamirSA23",
      "DOI": "10.3390/computers12070128",
      "CorpusId": 259696056
    },
    "url": "https://www.semanticscholar.org/paper/f919fcb8fbf27804138006578b57f9f0050d4f71",
    "title": "Improving Bug Assignment and Developer Allocation in Software Engineering through Interpretable Machine Learning Models",
    "venue": "De Computis",
    "year": 2023,
    "referenceCount": 34,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "https://www.mdpi.com/2073-431X/12/7/128/pdf?version=1687663894",
      "status": "GOLD",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/computers12070128?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/computers12070128, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-23",
    "authors": [
      {
        "authorId": "2222436846",
        "name": "Mina Samir"
      },
      {
        "authorId": "143731151",
        "name": "N. Sherief"
      },
      {
        "authorId": "2303862",
        "name": "W. Abdelmoez"
      }
    ],
    "abstract": "Software engineering is a comprehensive process that requires developers and team members to collaborate across multiple tasks. In software testing, bug triaging is a tedious and time-consuming process. Assigning bugs to the appropriate developers can save time and maintain their motivation. However, without knowledge about a bug’s class, triaging is difficult. Motivated by this challenge, this paper focuses on the problem of assigning a suitable developer to a new bug by analyzing the history of developers’ profiles and analyzing the history of bugs for all developers using machine learning-based recommender systems. Explainable AI (XAI) is AI that humans can understand. It contrasts with “black box” AI, which even its designers cannot explain. By providing appropriate explanations for results, users can better comprehend the underlying insight behind the outcomes, boosting the recommender system’s effectiveness, transparency, and confidence. The trained model is utilized in the recommendation stage to calculate relevance scores for developers based on expertise and past bug handling performance, ultimately presenting the developers with the highest scores as recommendations for new bugs. This approach aims to strike a balance between computational efficiency and accurate predictions, enabling efficient bug assignment while considering developer expertise and historical performance. In this paper, we propose two explainable models for recommendation. The first is an explainable recommender model for personalized developers generated from bug history to know what the preferred type of bug is for each developer. The second model is an explainable recommender model based on bugs to identify the most suitable developer for each bug from bug history.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "1b82af66272e978d160cc95ef37363de2f2fb0b0",
    "externalIds": {
      "DBLP": "journals/corr/abs-2210-11584",
      "DOI": "10.48550/arXiv.2210.11584",
      "CorpusId": 253080405
    },
    "url": "https://www.semanticscholar.org/paper/1b82af66272e978d160cc95ef37363de2f2fb0b0",
    "title": "Towards Human-centered Explainable AI: User Studies for Model Explanations",
    "venue": "arXiv.org",
    "year": 2022,
    "referenceCount": 271,
    "citationCount": 28,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2210.11584",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2210.11584?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2210.11584, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2057815422",
        "name": "Yao Rong"
      },
      {
        "authorId": "2130899453",
        "name": "Tobias Leemann"
      },
      {
        "authorId": "2216398486",
        "name": "T. Nguyen"
      },
      {
        "authorId": "2188651075",
        "name": "Lisa Fiedler"
      },
      {
        "authorId": "2516513",
        "name": "T. Seidel"
      },
      {
        "authorId": "1686448",
        "name": "Gjergji Kasneci"
      },
      {
        "authorId": "1884159",
        "name": "Enkelejda Kasneci"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 7
  },
  {
    "paperId": "294669896da18d98cc3a78bc5dfd057e403bd119",
    "externalIds": {
      "DBLP": "conf/xai/Donoso-GuzmanOP23",
      "ArXiv": "2308.06274",
      "DOI": "10.1007/978-3-031-44070-0_10",
      "CorpusId": 260886864
    },
    "url": "https://www.semanticscholar.org/paper/294669896da18d98cc3a78bc5dfd057e403bd119",
    "title": "Towards a Comprehensive Human-Centred Evaluation Framework for Explainable AI",
    "venue": "xAI",
    "year": 2023,
    "referenceCount": 53,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.06274",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.06274, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-31",
    "authors": [
      {
        "authorId": "2231541547",
        "name": "Ivania Donoso-Guzm'an"
      },
      {
        "authorId": "2007839030",
        "name": "Jeroen Ooge"
      },
      {
        "authorId": "145831267",
        "name": "Denis Parra"
      },
      {
        "authorId": "2629441",
        "name": "K. Verbert"
      }
    ],
    "abstract": "While research on explainable AI (XAI) is booming and explanation techniques have proven promising in many application domains, standardised human-centred evaluation procedures are still missing. In addition, current evaluation procedures do not assess XAI methods holistically in the sense that they do not treat explanations' effects on humans as a complex user experience. To tackle this challenge, we propose to adapt the User-Centric Evaluation Framework used in recommender systems: we integrate explanation aspects, summarise explanation properties, indicate relations between them, and categorise metrics that measure these properties. With this comprehensive evaluation framework, we hope to contribute to the human-centred standardisation of XAI evaluation.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "1e3273a6e2aeab1dbb8f71ddf7731215561d6085",
    "externalIds": {
      "ArXiv": "2306.04791",
      "DBLP": "conf/xai/LairdMKC23",
      "DOI": "10.48550/arXiv.2306.04791",
      "CorpusId": 259108514
    },
    "url": "https://www.semanticscholar.org/paper/1e3273a6e2aeab1dbb8f71ddf7731215561d6085",
    "title": "XInsight: Revealing Model Insights for GNNs with Flow-based Explanations",
    "venue": "xAI",
    "year": 2023,
    "referenceCount": 50,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.04791",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.04791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-07",
    "authors": [
      {
        "authorId": "2158367804",
        "name": "Eli J. Laird"
      },
      {
        "authorId": "2092454493",
        "name": "Ayesh Madushanka"
      },
      {
        "authorId": "3162215",
        "name": "E. Kraka"
      },
      {
        "authorId": "9261516",
        "name": "Corey Clark"
      }
    ],
    "abstract": "Progress in graph neural networks has grown rapidly in recent years, with many new developments in drug discovery, medical diagnosis, and recommender systems. While this progress is significant, many networks are `black boxes' with little understanding of the `what' exactly the network is learning. Many high-stakes applications, such as drug discovery, require human-intelligible explanations from the models so that users can recognize errors and discover new knowledge. Therefore, the development of explainable AI algorithms is essential for us to reap the benefits of AI. We propose an explainability algorithm for GNNs called eXplainable Insight (XInsight) that generates a distribution of model explanations using GFlowNets. Since GFlowNets generate objects with probabilities proportional to a reward, XInsight can generate a diverse set of explanations, compared to previous methods that only learn the maximum reward sample. We demonstrate XInsight by generating explanations for GNNs trained on two graph classification tasks: classifying mutagenic compounds with the MUTAG dataset and classifying acyclic graphs with a synthetic dataset that we have open-sourced. We show the utility of XInsight's explanations by analyzing the generated compounds using QSAR modeling, and we find that XInsight generates compounds that cluster by lipophilicity, a known correlate of mutagenicity. Our results show that XInsight generates a distribution of explanations that uncovers the underlying relationships demonstrated by the model. They also highlight the importance of generating a diverse set of explanations, as it enables us to discover hidden relationships in the model and provides valuable guidance for further analysis.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "ea56ef7540345caa9d2c1b952d4ba1752b1f8afc",
    "externalIds": {
      "DBLP": "conf/recsys/SzymanskiVA22",
      "DOI": "10.1145/3523227.3547427",
      "CorpusId": 252216553
    },
    "url": "https://www.semanticscholar.org/paper/ea56ef7540345caa9d2c1b952d4ba1752b1f8afc",
    "title": "Designing and evaluating explainable AI for non-AI experts: challenges and opportunities",
    "venue": "ACM Conference on Recommender Systems",
    "year": 2022,
    "referenceCount": 14,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3523227.3547427?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3523227.3547427, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2022-09-18",
    "authors": [
      {
        "authorId": "2075160673",
        "name": "Maxwell Szymanski"
      },
      {
        "authorId": "2629441",
        "name": "K. Verbert"
      },
      {
        "authorId": "8474018",
        "name": "V. Abeele"
      }
    ],
    "abstract": "Artificial intelligence (AI) has seen a steady increase in use in the health and medical field, where it is used by lay users and health experts alike. However, these AI systems often lack transparency regarding the inputs and decision making process (often called black boxes), which in turn can be detrimental to the user’s satisfaction and trust towards these systems. Explainable AI (XAI) aims to overcome this problem by opening up certain aspects of the black box, and has proven to be a successful means of increasing trust, transparency and even system effectiveness. However, for certain groups (i.e. lay users in health), explanation methods and evaluation metrics still remain underexplored. In this paper, we will outline our research regarding designing and evaluating explanations for health recommendations for lay users and domain experts, as well as list a few takeaways we were already able to find in our initial studies.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "391f16dd945171901e6381ffb1b1e2c4d7185f75",
    "externalIds": {
      "DBLP": "journals/datamine/IferroudjeneLRPA23",
      "DOI": "10.1007/s10618-022-00897-2",
      "CorpusId": 256785690
    },
    "url": "https://www.semanticscholar.org/paper/391f16dd945171901e6381ffb1b1e2c4d7185f75",
    "title": "Methods for explaining Top-N recommendations through subgroup discovery",
    "venue": "Data mining and knowledge discovery",
    "year": 2022,
    "referenceCount": 50,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10618-022-00897-2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10618-022-00897-2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-11-28",
    "authors": [
      {
        "authorId": "2205454947",
        "name": "Mouloud Iferroudjene"
      },
      {
        "authorId": "66745976",
        "name": "Corentin Lonjarret"
      },
      {
        "authorId": "1763302",
        "name": "C. Robardet"
      },
      {
        "authorId": "2331463",
        "name": "Marc Plantevit"
      },
      {
        "authorId": "2191921580",
        "name": "Martin Atzmueller"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "c4a1acf425fa661d5e310309caa9add36c25534c",
    "externalIds": {
      "DBLP": "conf/hci/FranklinL22",
      "DOI": "10.1007/978-3-031-06417-3_54",
      "CorpusId": 249929423
    },
    "url": "https://www.semanticscholar.org/paper/c4a1acf425fa661d5e310309caa9add36c25534c",
    "title": "Human-AI Interaction Paradigm for Evaluating Explainable Artificial Intelligence",
    "venue": "Interacción",
    "year": 2022,
    "referenceCount": 37,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-031-06417-3_54?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-031-06417-3_54, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2057715716",
        "name": "Matija Franklin"
      },
      {
        "authorId": "1953585",
        "name": "D. Lagnado"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "48096f635976277e9d4364b2ff73b794bdad4512",
    "externalIds": {
      "DBLP": "conf/icstcc/Vultureanu-Albisi21",
      "DOI": "10.1109/ICSTCC52150.2021.9607106",
      "CorpusId": 244273176
    },
    "url": "https://www.semanticscholar.org/paper/48096f635976277e9d4364b2ff73b794bdad4512",
    "title": "Explainable Collaborative Filtering Recommendations Enriched with Contextual Information",
    "venue": "International Conference on System Theory, Control and Computing",
    "year": 2021,
    "referenceCount": 0,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICSTCC52150.2021.9607106?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICSTCC52150.2021.9607106, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Conference",
      "Review"
    ],
    "publicationDate": "2021-10-20",
    "authors": [
      {
        "authorId": "2091486093",
        "name": "Alexandra Vultureanu-Albişi"
      },
      {
        "authorId": "1740341",
        "name": "C. Bǎdicǎ"
      }
    ],
    "abstract": "Today, the most important requirement of intelligent systems is to be able to explain their decisions to the end-user. Fulfilling this requirement is the goal of explainable AI (XAI) that proposes to produce explainable models that enable end-users to understand and trust the models. This research addresses the explainability of the recommendations. This paper presents an explainable recommender system for point of interest recommendations taking into account the context of the user. In our experiments we have used the STS (South Tyrol Suggests) dataset. The following major steps are part of our methodology: i) presenting the dataset, ii) using Restricted Boltzmann Machine based collaborative filtering recommendations, iii) using contextual information, and iv) extracting and presenting explanations for recommendations based on contextual information. The novelty that we propose in explainable recommender systems is a new explainable recommendation technique, which is quantitative and qualitative, providing both the list of top-n recommendations and the explanations of the recommendations based on context. This paper also provides an overview of research on this topic.",
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "469b81bd22e7e84543c988f09914ada41fa1ff66",
    "externalIds": {
      "DBLP": "conf/iui/OduorQLP20",
      "MAG": "3011753684",
      "DOI": "10.1145/3379336.3381468",
      "CorpusId": 212675387
    },
    "url": "https://www.semanticscholar.org/paper/469b81bd22e7e84543c988f09914ada41fa1ff66",
    "title": "XAIT: An Interactive Website for Explainable AI for Text",
    "venue": "IUI Companion",
    "year": 2020,
    "referenceCount": 3,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3379336.3381468?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3379336.3381468, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Review"
    ],
    "publicationDate": "2020-03-13",
    "authors": [
      {
        "authorId": "2383655",
        "name": "Erick Oduor"
      },
      {
        "authorId": "2053225294",
        "name": "Kun Qian"
      },
      {
        "authorId": "1573872877",
        "name": "Yunyao Li"
      },
      {
        "authorId": "145378077",
        "name": "Lucian Popa"
      }
    ],
    "abstract": "Explainable AI (XAI) for text is an emerging field focused on developing novel techniques to render black-box models more interpretable for text-related tasks. To understand the recent advances in XAI for text, we have done an extensive literature review and user studies. Allowing users to easily explore the assets we created is a major challenge. In this demo we present an interactive website named XAIT. The core of XAIT is a tree-like taxonomy, with which the users can interactively explore and understand the field of XAI for text through different dimensions: (1) the type of text tasks in consideration; (2) the explanation techniques used for a particular task; (3) who are the target and appropriate users for a particular explanation technique. XAIT can be used as a recommender system for users to find out what are the appropriate and suitable explanation techniques for their text-related tasks, or for researchers who want to find out publications and tools relating to XAI for text.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "f3e81e58e4a1b573281ee06394e5305d3942c03b",
    "externalIds": {
      "DBLP": "conf/aiia/MarconiAZMME20",
      "CorpusId": 227258032
    },
    "url": "https://www.semanticscholar.org/paper/f3e81e58e4a1b573281ee06394e5305d3942c03b",
    "title": "Approaching Explainable Recommendations for Personalized Social Learning: the Current Stage in the Educational Platform \"WhoTeach\"",
    "venue": "XAI.it@AI*IA",
    "year": 2020,
    "referenceCount": 31,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "144471674",
        "name": "Luca Marconi"
      },
      {
        "authorId": "1740656487",
        "name": "Ricardo Anibal Matamoros Aragon"
      },
      {
        "authorId": "1947730",
        "name": "I. Zoppis"
      },
      {
        "authorId": "1718628",
        "name": "S. Manzoni"
      },
      {
        "authorId": "1747770",
        "name": "G. Mauri"
      },
      {
        "authorId": "2449175",
        "name": "F. Epifania"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "ca19d0873bed9e8ead5d00a0ae32da60bd4faf86",
    "externalIds": {
      "DBLP": "journals/tist/WuWTYL24",
      "DOI": "10.1145/3663483",
      "CorpusId": 269619920
    },
    "url": "https://www.semanticscholar.org/paper/ca19d0873bed9e8ead5d00a0ae32da60bd4faf86",
    "title": "DIRECT: Dual Interpretable Recommendation with Multi-aspect Word Attribution",
    "venue": "ACM Transactions on Intelligent Systems and Technology",
    "year": 2024,
    "referenceCount": 70,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3663483",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3663483?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3663483, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-05-06",
    "authors": [
      {
        "authorId": "2145346360",
        "name": "Xuansheng Wu"
      },
      {
        "authorId": "2300234049",
        "name": "Hanqin Wan"
      },
      {
        "authorId": "2293725776",
        "name": "Qiaoyu Tan"
      },
      {
        "authorId": "2291141500",
        "name": "Wenlin Yao"
      },
      {
        "authorId": "2256183798",
        "name": "Ninghao Liu"
      }
    ],
    "abstract": "Recommending products to users with intuitive explanations helps improve the system in transparency, persuasiveness, and satisfaction. Existing interpretation techniques include post hoc methods and interpretable modeling. The former category could quantitatively analyze input contribution to model prediction but has limited interpretation faithfulness, while the latter could explain model internal mechanisms but may not directly attribute model predictions to input features. In this study, we propose a novel Dual Interpretable Recommendation model called DIRECT, which integrates ideas of the two interpretation categories to inherit their advantages and avoid limitations. Specifically, DIRECT makes use of item descriptions as explainable evidence for recommendation. First, similar to the post hoc interpretation, DIRECT could attribute the prediction of a user preference score to textual words of the item descriptions. The attribution of each word is related to its sentiment polarity and word importance, where a word is important if it corresponds to an item aspect that the user is interested in. Second, to improve the interpretability of embedding space, we propose to extract high-level concepts from embeddings, where each concept corresponds to an item aspect. To learn discriminative concepts, we employ a concept bottleneck layer and maximize the coding rate reduction on word-aspect embeddings by leveraging a word–word affinity graph extracted from a pre-trained language model. In this way, DIRECT simultaneously achieves faithful attribution and usable interpretation of embedding space. We also show that DIRECT achieves linear inference time complexity regarding the length of item reviews. We conduct experiments including ablation studies on five real-world datasets. Quantitative analysis, visualizations, and case studies verify the interpretability of DIRECT. Our code is available at: https://github.com/JacksonWuxs/DIRECT.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "e54ad446b2e684be12b71e3f0445ec574aca7b91",
    "externalIds": {
      "DBLP": "conf/mlnlp/LongJ23",
      "DOI": "10.1145/3639479.3639501",
      "CorpusId": 268178472
    },
    "url": "https://www.semanticscholar.org/paper/e54ad446b2e684be12b71e3f0445ec574aca7b91",
    "title": "Interpretable Recommendation Based on Review Aspect-Level Preferences",
    "venue": "MLNLP",
    "year": 2023,
    "referenceCount": 16,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3639479.3639501?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3639479.3639501, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Review"
    ],
    "publicationDate": "2023-12-27",
    "authors": [
      {
        "authorId": "2279036193",
        "name": "Xiuhua Long"
      },
      {
        "authorId": "2279036078",
        "name": "Ting Jin"
      }
    ],
    "abstract": "Review-based recommendation systems usually employ explicit independent methods to learn vector representations of users and items to enhance recommendation performance, while often disregarding the interpretability of the recommendations. This paper proposes an explainable recommendation framework which called Interpretable Recommendation Based on Review Aspect-Level Preferences (IRAP). The model employs a two-stage task design to predict user preference and provide meaningful explanations. First, we utilize convolutional neural networks and aspect-level word embedding modules to extract user preferences and item features. Subsequently, we aggregate user and item vector representations along with rating label information and construct soft prompt texts, integrating them into pre-trained models. Finally, we undertake joint training, encompassing rating prediction tasks and review generation tasks, to improve recommendation performance and explanation quality. We conducted comparative experiments on two public datasets, and the results demonstrate that our model has shown significant improvement in BLEU and ROUGE metrics for review generation tasks.",
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "7f2ab0802ffcb025bcdad82bddb57dcac65ec9be",
    "externalIds": {
      "DBLP": "journals/anor/CoussementBG22",
      "MAG": "3137946891",
      "DOI": "10.1007/s10479-021-03979-4",
      "CorpusId": 233709010
    },
    "url": "https://www.semanticscholar.org/paper/7f2ab0802ffcb025bcdad82bddb57dcac65ec9be",
    "title": "A decision-analytic framework for interpretable recommendation systems with multiple input data sources: a case study for a European e-tailer",
    "venue": "Annals of Operations Research",
    "year": 2021,
    "referenceCount": 67,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10479-021-03979-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10479-021-03979-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-03-12",
    "authors": [
      {
        "authorId": "2060729",
        "name": "Kristof Coussement"
      },
      {
        "authorId": "21426759",
        "name": "K. D. Bock"
      },
      {
        "authorId": "24302223",
        "name": "Stijn Geuens"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "5af571df904cc767be84dfd680c630cfb8051a82",
    "externalIds": {
      "DBLP": "conf/ksem/WuL022",
      "DOI": "10.1007/978-3-031-10983-6_3",
      "CorpusId": 250926959
    },
    "url": "https://www.semanticscholar.org/paper/5af571df904cc767be84dfd680c630cfb8051a82",
    "title": "KIR: A Knowledge-Enhanced Interpretable Recommendation Method",
    "venue": "Knowledge Science, Engineering and Management",
    "year": 2022,
    "referenceCount": 0,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-031-10983-6_3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-031-10983-6_3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "30382724",
        "name": "Yuejia Wu"
      },
      {
        "authorId": "2108961719",
        "name": "Jia-Le Li"
      },
      {
        "authorId": "2143464767",
        "name": "Jiantao Zhou"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "e9d13ea652c38303ed0d3e0f06cf8f904df28309",
    "externalIds": {
      "MAG": "3161275189",
      "DBLP": "journals/el/ZengSPD21",
      "DOI": "10.1108/EL-06-2020-0154",
      "CorpusId": 236522186
    },
    "url": "https://www.semanticscholar.org/paper/e9d13ea652c38303ed0d3e0f06cf8f904df28309",
    "title": "Using latent features for building an interpretable recommendation system",
    "venue": "Electronic library",
    "year": 2021,
    "referenceCount": 22,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1108/EL-06-2020-0154?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1108/EL-06-2020-0154, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-05-17",
    "authors": [
      {
        "authorId": "31156287",
        "name": "Ziming Zeng"
      },
      {
        "authorId": "2152861954",
        "name": "Yu Shi"
      },
      {
        "authorId": "2121508084",
        "name": "L. F. Pieptea"
      },
      {
        "authorId": "1560317211",
        "name": "Junhua Ding"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "4e05b1a6f9e03661941b7663f3abb1646535a619",
    "externalIds": {
      "MAG": "2906814785",
      "ArXiv": "1902.07494",
      "DBLP": "journals/corr/abs-1902-07494",
      "DOI": "10.1145/3289600.3290609",
      "CorpusId": 59528440
    },
    "url": "https://www.semanticscholar.org/paper/4e05b1a6f9e03661941b7663f3abb1646535a619",
    "title": "NAIRS: A Neural Attentive Interpretable Recommendation System",
    "venue": "Web Search and Data Mining",
    "year": 2019,
    "referenceCount": 30,
    "citationCount": 19,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.07494, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2019-01-30",
    "authors": [
      {
        "authorId": "47421395",
        "name": "Shuai Yu"
      },
      {
        "authorId": "2109302622",
        "name": "Yongbo Wang"
      },
      {
        "authorId": "2110950699",
        "name": "Min Yang"
      },
      {
        "authorId": "2118424213",
        "name": "Baocheng Li"
      },
      {
        "authorId": "143980270",
        "name": "Qiang Qu"
      },
      {
        "authorId": "38203359",
        "name": "Jialie Shen"
      }
    ],
    "abstract": "In this paper, we develop a neural attentive interpretable recommendation system, named NAIRS. A self-attention network, as a key component of the system, is designed to assign attention weights to interacted items of a user. This attention mechanism can distinguish the importance of the various interacted items in contributing to a user profile. %, and it also provides interpretable recommendations. Based on the user profiles obtained by the self-attention network, NAIRS offers personalized high-quality recommendation. Moreover, it develops visual cues to interpret recommendations. This demo application with the implementation of NAIRS enables users to interact with a recommendation system, and it persistently collects training data to improve the system. The demonstration and experimental results show the effectiveness of NAIRS.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "be6b0b4d6f7ba22d200c5ac9f82ae0053126cfea",
    "externalIds": {
      "DBLP": "conf/recsys/LimaDMPOC21",
      "DOI": "10.1145/3460231.3478850",
      "CorpusId": 237494538
    },
    "url": "https://www.semanticscholar.org/paper/be6b0b4d6f7ba22d200c5ac9f82ae0053126cfea",
    "title": "An Interpretable Recommendation Model for Gerontological Care",
    "venue": "ACM Conference on Recommender Systems",
    "year": 2021,
    "referenceCount": 36,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3460231.3478850?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3460231.3478850, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2021-09-13",
    "authors": [
      {
        "authorId": "51114110",
        "name": "Andre Paulino de Lima"
      },
      {
        "authorId": "2126515235",
        "name": "Laurentino Augusto Dantas"
      },
      {
        "authorId": "9125814",
        "name": "M. Manzato"
      },
      {
        "authorId": "144475599",
        "name": "M. Pimentel"
      },
      {
        "authorId": "2076153114",
        "name": "B. D. M. Orlandi"
      },
      {
        "authorId": "1571629403",
        "name": "P. Castro"
      }
    ],
    "abstract": "Recommender systems have been successfully applied to diverse areas, but their use in the healthcare domain is still rare. One challenge of applying recommender systems to this domain is related to legal concerns about the consequences of provided recommendations. In this work, we advance an expert-in-the-loop, explanation-first approach to tackle this challenge in a specific healthcare niche: gerontological care. A key aspect of the proposed approach is that both recommendations and explanations reflect the structured questionnaire employed by the practitioner to identify patient needs. Another key aspect is that a clinical dataset of patient assessments and respective assigned interventions is used to estimate effects of alternative interventions during the recommendation process. To evaluate the feasibility of this modelling approach, an explanation style was designed with help of practitioners, and a recommendation model was devised and evaluated against a clinical dataset, which was collected by a partner research group working on gerontological primary care. When compared to other traditional recommendation models, the attained precision was competitive across several evaluation conditions. The results suggest that the proposed approach is feasible and may point new ways of adapting recommender systems to play an assistive role in health care.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "1f165e1620dbe3463589cf3d223ec004c153c01f",
    "externalIds": {
      "DBLP": "journals/eswa/YuYQS19",
      "MAG": "2955610593",
      "DOI": "10.1016/J.ESWA.2019.06.051",
      "CorpusId": 198361195
    },
    "url": "https://www.semanticscholar.org/paper/1f165e1620dbe3463589cf3d223ec004c153c01f",
    "title": "Contextual-boosted deep neural collaborative filtering model for interpretable recommendation",
    "venue": "Expert systems with applications",
    "year": 2019,
    "referenceCount": 51,
    "citationCount": 26,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/J.ESWA.2019.06.051?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/J.ESWA.2019.06.051, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2019-12-01",
    "authors": [
      {
        "authorId": "47421395",
        "name": "Shuai Yu"
      },
      {
        "authorId": "2110950699",
        "name": "Min Yang"
      },
      {
        "authorId": "143980270",
        "name": "Qiang Qu"
      },
      {
        "authorId": "143822679",
        "name": "Ying Shen"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "3926ec2c463b5b19b58121e766374a9112859e03",
    "externalIds": {
      "ArXiv": "2508.05225",
      "DBLP": "journals/corr/abs-2508-05225",
      "DOI": "10.48550/arXiv.2508.05225",
      "CorpusId": 280546117
    },
    "url": "https://www.semanticscholar.org/paper/3926ec2c463b5b19b58121e766374a9112859e03",
    "title": "FIRE: Faithful Interpretable Recommendation Explanations",
    "venue": "arXiv.org",
    "year": 2025,
    "referenceCount": 41,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.05225, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2025-08-07",
    "authors": [
      {
        "authorId": "2257004165",
        "name": "S.M.F. Sani"
      },
      {
        "authorId": "2375142690",
        "name": "Asal Meskin"
      },
      {
        "authorId": "2053568286",
        "name": "Mohammad Amanlou"
      },
      {
        "authorId": "2258572519",
        "name": "Hamid R. Rabiee"
      }
    ],
    "abstract": "Natural language explanations in recommender systems are often framed as a review generation task, leveraging user reviews as ground-truth supervision. While convenient, this approach conflates a user's opinion with the system's reasoning, leading to explanations that may be fluent but fail to reflect the true logic behind recommendations. In this work, we revisit the core objective of explainable recommendation: to transparently communicate why an item is recommended by linking user needs to relevant item features. Through a comprehensive analysis of existing methods across multiple benchmark datasets, we identify common limitations-explanations that are weakly aligned with model predictions, vague or inaccurate in identifying user intents, and overly repetitive or generic. To overcome these challenges, we propose FIRE, a lightweight and interpretable framework that combines SHAP-based feature attribution with structured, prompt-driven language generation. FIRE produces faithful, diverse, and user-aligned explanations, grounded in the actual decision-making process of the model. Our results demonstrate that FIRE not only achieves competitive recommendation accuracy but also significantly improves explanation quality along critical dimensions such as alignment, structure, and faithfulness. This work highlights the need to move beyond the review-as-explanation paradigm and toward explanation methods that are both accountable and interpretable.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "907e53b6a63ab37ad6e97a5425c400c7c53b71fa",
    "externalIds": {
      "MAG": "2987225481",
      "DBLP": "conf/cikm/LiuWJYZZ19",
      "DOI": "10.1145/3357384.3358017",
      "CorpusId": 207758091
    },
    "url": "https://www.semanticscholar.org/paper/907e53b6a63ab37ad6e97a5425c400c7c53b71fa",
    "title": "In2Rec: Influence-based Interpretable Recommendation",
    "venue": "International Conference on Information and Knowledge Management",
    "year": 2019,
    "referenceCount": 33,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "https://repository.kaust.edu.sa/bitstream/10754/660624/1/lp1303-liuA%5b2%5d.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3357384.3358017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3357384.3358017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference",
      "Review"
    ],
    "publicationDate": "2019-11-03",
    "authors": [
      {
        "authorId": "2145489099",
        "name": "Huafeng Liu"
      },
      {
        "authorId": "2113341875",
        "name": "Jingxuan Wen"
      },
      {
        "authorId": "144889532",
        "name": "L. Jing"
      },
      {
        "authorId": "1740321",
        "name": "Jian Yu"
      },
      {
        "authorId": "2928371",
        "name": "Xiangliang Zhang"
      },
      {
        "authorId": "38898636",
        "name": "Min Zhang"
      }
    ],
    "abstract": "Interpretability of recommender systems has caused increasing attention due to its promotion of the effectiveness and persuasiveness of recommendation decision, and thus user satisfaction. Most existing methods, such as Matrix Factorization (MF), tend to be black-box machine learning models that lack interpretability and do not provide a straightforward explanation for their outputs. In this paper, we focus on probabilistic factorization model and further assume the absence of any auxiliary information, such as item content or user review. We propose an influence mechanism to evaluate the importance of the users' historical data, so that the most related users and items can be selected to explain each predicted rating. The proposed method is thus called Influencebased Interpretable Recommendation model (In2Rec). To further enhance the recommendation accuracy, we address the important issue of missing not at random, i.e., missing ratings are not independent from the observed and other unobserved ratings, because users tend to only interact what they like. In2Rec models the generative process for both observed and missing data, and integrates the influence mechanism in a Bayesian graphical model. A learning algorithm capitalizing on iterated condition modes is proposed to tackle the non-convex optimization problem pertaining to maximum a posteriori estimation for In2Rec. A series of experiments on four real-world datasets (Movielens 10M, Netflix, Epinions, and Yelp) have been conducted. By comparing with the state-of-the-art recommendation methods, the experimental results have shown that In2Rec can consistently benefit the recommendation system in both rating prediction and ranking estimation tasks, and friendly interpret the recommendation results with the aid of the proposed influence mechanism.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "1a2b28dd509c2e3b6c07ac9c9dc3592bc40b0795",
    "externalIds": {
      "DBLP": "conf/ijcai/HuJCC18",
      "MAG": "2808613198",
      "DOI": "10.24963/ijcai.2018/472",
      "CorpusId": 51606322
    },
    "url": "https://www.semanticscholar.org/paper/1a2b28dd509c2e3b6c07ac9c9dc3592bc40b0795",
    "title": "Interpretable Recommendation via Attraction Modeling: Learning Multilevel Attractiveness over Multimodal Movie Contents",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2018,
    "referenceCount": 30,
    "citationCount": 32,
    "openAccessPdf": {
      "url": "https://www.ijcai.org/proceedings/2018/0472.pdf",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2018/472?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2018/472, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2018-07-01",
    "authors": [
      {
        "authorId": "144159009",
        "name": "Liang Hu"
      },
      {
        "authorId": "8479039",
        "name": "Songlei Jian"
      },
      {
        "authorId": "2148761004",
        "name": "Longbing Cao"
      },
      {
        "authorId": "2907183",
        "name": "Qingkui Chen"
      }
    ],
    "abstract": "New contents like blogs and online videos are produced in every second in the new media age. We argue that attraction is one of the decisive factors for user selection of new contents. However, collaborative filtering cannot work without user feedback; and the existing content-based recommender systems are ineligible to capture and interpret the attractive points on new contents. Accordingly, we propose attraction modeling to learn and interpret user attractiveness. Specially, we build a multilevel attraction model (MLAM) over the content features -- the story (textual data) and cast members (categorical data) of movies. In particular, we design multilevel personal filters to calculate users' attractiveness on words, sentences and cast members at different levels. The experimental results show the superiority of MLAM over the state-of-the-art methods. In addition, a case study is provided to demonstrate the interpretability of MLAM by visualizing user attractiveness on a movie.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "f8bb0a8d974e12ff731292e2b2a8a6b5f64043cc",
    "externalIds": {
      "MAG": "3017094854",
      "DOI": "10.1088/1757-899X/782/4/042013",
      "CorpusId": 250870681
    },
    "url": "https://www.semanticscholar.org/paper/f8bb0a8d974e12ff731292e2b2a8a6b5f64043cc",
    "title": "Interpretable Recommendation System Based on Knowledge Map Features",
    "venue": "IOP Conference Series: Materials Science and Engineering",
    "year": 2020,
    "referenceCount": 13,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "https://doi.org/10.1088/1757-899x/782/4/042013",
      "status": "GOLD",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1088/1757-899X/782/4/042013?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1088/1757-899X/782/4/042013, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2020-04-15",
    "authors": [
      {
        "authorId": "2144211696",
        "name": "Hongyu Chen"
      },
      {
        "authorId": "2128690853",
        "name": "Jingyang Liu"
      },
      {
        "authorId": "2110611142",
        "name": "Mao Yang"
      }
    ],
    "abstract": "At present, it can be explained that the recommendation system is mostly designed for a specific recommendation model, and its scalability is weak. It is not enough for the emerging recommendation models, such as complex and mixed models with deep neural networks.Knowledge maps as a highly readable external knowledge carrier provide great possibilities for improving the ability of algorithms to interpret.In essence, the knowledge map is intended to describe the various entities or concepts and their relationships that exist in the real world, which constitute a huge semantic network diagram, nodes represent entities or concepts, and edges are composed of attributes or relationships.Based on the current research status, this paper analyzes and studies the concept of knowledge map, the combination of knowledge map and recommendation system, the object of interpretability and the application of knowledge map in interpretability model, combined with relevant recommendation models. The knowledge map, open up the relationship between the media, flexibly choose the most suitable medium according to the specific situation to recommend and explain the user.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "57931fe0eb921e001e6ef5034bdb7725acb52c61",
    "externalIds": {
      "DBLP": "journals/corr/abs-2410-07671",
      "ArXiv": "2410.07671",
      "DOI": "10.1109/ICDM59182.2024.00066",
      "CorpusId": 273233580
    },
    "url": "https://www.semanticscholar.org/paper/57931fe0eb921e001e6ef5034bdb7725acb52c61",
    "title": "DISCO: A Hierarchical Disentangled Cognitive Diagnosis Framework for Interpretable Job Recommendation",
    "venue": "Industrial Conference on Data Mining",
    "year": 2024,
    "referenceCount": 55,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.07671, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-10-10",
    "authors": [
      {
        "authorId": "2255108725",
        "name": "Xiaoshan Yu"
      },
      {
        "authorId": "2260648967",
        "name": "Chuan Qin"
      },
      {
        "authorId": "2145906911",
        "name": "Qi Zhang"
      },
      {
        "authorId": "144469723",
        "name": "Chen Zhu"
      },
      {
        "authorId": "2152191294",
        "name": "Haiping Ma"
      },
      {
        "authorId": "2177334713",
        "name": "Xingyi Zhang"
      },
      {
        "authorId": "2283086825",
        "name": "Hengshu Zhu"
      }
    ],
    "abstract": "The rapid development of online recruitment platforms has created unprecedented opportunities for job seekers while concurrently posing the significant challenge of quickly and accurately pinpointing positions that align with their skills and preferences. Job recommendation systems have significantly alleviated the extensive search burden for job seekers by optimizing user engagement metrics, such as clicks and applications, thus achieving notable success. In recent years, a substantial amount of research has been devoted to developing effective job recommendation models, primarily focusing on text-matching based and behavior modeling based methods. While these approaches have realized impressive outcomes, it is imperative to note that research on the explainability of recruitment recommendations remains profoundly unexplored. To this end, in this paper, we propose DISCO, a hierarchical Disentanglement based Cognitive diagnosis framework, aimed at flexibly accommodating the underlying representation learning model for effective and interpretable job recommendations. Specifically, we first design a hierarchical representation disentangling module to explicitly mine the hierarchical skill-related factors implied in hidden representations of job seekers and jobs. Subsequently, we propose level-aware association modeling to enhance information communication and robust representation learning both inter- and intra-level, which consists of the inter-level knowledge influence module and the level-wise contrastive learning. Finally, we devise an interaction diagnosis module incorporating a neural diagnosis function for effectively modeling the multi-level recruitment interaction process between job seekers and jobs, which introduces the cognitive measurement theory. Extensive experiments on two real-world recruitment recommendation datasets and an educational recommendation dataset clearly demonstrate the effectiveness and interpretability of our proposed DISCO framework. Our codes are available at https://github.com/LabyrinthineLeo/DISCO.",
    "affiliations": [],
    "countries": [],
    "num_authors": 7
  },
  {
    "paperId": "85c763a7d3d24b184d5a2de133006ddf55917d4f",
    "externalIds": {
      "PubMedCentral": "9929065",
      "DOI": "10.1038/s41598-023-28766-y",
      "CorpusId": 256832678,
      "PubMed": "36788243"
    },
    "url": "https://www.semanticscholar.org/paper/85c763a7d3d24b184d5a2de133006ddf55917d4f",
    "title": "Interpretable patent recommendation with knowledge graph and deep learning",
    "venue": "Scientific Reports",
    "year": 2023,
    "referenceCount": 48,
    "citationCount": 15,
    "openAccessPdf": {
      "url": "https://www.nature.com/articles/s41598-023-28766-y.pdf",
      "status": "GOLD",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9929065, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-02-14",
    "authors": [
      {
        "authorId": "70637888",
        "name": "Han Chen"
      },
      {
        "authorId": "46778857",
        "name": "Weiwei Deng"
      }
    ],
    "abstract": "Patent transfer is a common practice for companies to obtain competitive advantages. However, they encounter the difficulty of selecting suitable patents because the number of patents is increasingly large. Many patent recommendation methods have been proposed to ease the difficulty, but they ignore patent quality and cannot explain why certain patents are recommended. Patent quality and recommendation explanations affect companies’ decision-making in the patent transfer context. Failing to consider them in the recommendation process leads to less effective recommendation results. To fill these gaps, this paper proposes an interpretable patent recommendation method based on knowledge graph and deep learning. The proposed method organizes heterogeneous patent information as a knowledge graph. Then it extracts connectivity and quality features from the knowledge graph for pairs of patents and companies. The former features indicate the relevance of the pairs while the latter features reflect the quality of the patents. Based on the features, we design an interpretable recommendation model by combining a deep neural network with a relevance propagation technique. We conduct experiments with real-world data to evaluate the proposed method. Recommendation lists with varying lengths show that the average precision, recall, and mean average precision of the proposed method are 0.596, 0.636, and 0.584, which improve corresponding performance of best baselines by 7.28%, 18.35%, and 8.60%, respectively. Besides, our method interprets recommendation results by identifying important features leading to the results.",
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "bdf8c51662e11671e046b5fd33a97c2dd67ac354",
    "externalIds": {
      "DBLP": "journals/mlc/YinFFXJ23",
      "DOI": "10.1007/s13042-023-01850-5",
      "CorpusId": 258562020
    },
    "url": "https://www.semanticscholar.org/paper/bdf8c51662e11671e046b5fd33a97c2dd67ac354",
    "title": "An interpretable neural network TV program recommendation based on SHAP",
    "venue": "International Journal of Machine Learning and Cybernetics",
    "year": 2023,
    "referenceCount": 51,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s13042-023-01850-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s13042-023-01850-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-07",
    "authors": [
      {
        "authorId": "39540381",
        "name": "Fulian Yin"
      },
      {
        "authorId": "2149584431",
        "name": "Ruiling Fu"
      },
      {
        "authorId": "2149116741",
        "name": "Xiaoli Feng"
      },
      {
        "authorId": "1751530780",
        "name": "Tongtong Xing"
      },
      {
        "authorId": "2055622149",
        "name": "Meiqi Ji"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "6e86a5b24527e2e15ae919d963ada0806e1e6d7c",
    "externalIds": {
      "DBLP": "conf/recsys/DingXHYGZZL23",
      "DOI": "10.1145/3604915.3608818",
      "CorpusId": 261823433
    },
    "url": "https://www.semanticscholar.org/paper/6e86a5b24527e2e15ae919d963ada0806e1e6d7c",
    "title": "Interpretable User Retention Modeling in Recommendation",
    "venue": "ACM Conference on Recommender Systems",
    "year": 2023,
    "referenceCount": 43,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3604915.3608818?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3604915.3608818, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-09-14",
    "authors": [
      {
        "authorId": "2112006541",
        "name": "Rui Ding"
      },
      {
        "authorId": "3360722",
        "name": "Ruobing Xie"
      },
      {
        "authorId": "2106412609",
        "name": "Xiaobo Hao"
      },
      {
        "authorId": "2240815849",
        "name": "Xiaochun Yang"
      },
      {
        "authorId": "2065008252",
        "name": "Kaikai Ge"
      },
      {
        "authorId": "2115462783",
        "name": "Xu Zhang"
      },
      {
        "authorId": "48128428",
        "name": "Jie Zhou"
      },
      {
        "authorId": "4950224",
        "name": "Leyu Lin"
      }
    ],
    "abstract": "Recommendation usually focuses on immediate accuracy metrics like CTR as training objectives. User retention rate, which reflects the percentage of today’s users that will return to the recommender system in the next few days, should be paid more attention to in real-world systems. User retention is the most intuitive and accurate reflection of user long-term satisfaction. However, most existing recommender systems are not focused on user retention-related objectives, since their complexity and uncertainty make it extremely hard to discover why a user will or will not return to a system and which behaviors affect user retention. In this work, we conduct a series of preliminary explorations on discovering and making full use of the reasons for user retention in recommendation. Specifically, we make a first attempt to design a rationale contrastive multi-instance learning framework to explore the rationale and improve the interpretability of user retention. Extensive offline and online evaluations with detailed analyses of a real-world recommender system verify the effectiveness of our user retention modeling. We further reveal the real-world interpretable factors of user retention from both user surveys and explicit negative feedback quantitative analyses to facilitate future model designs. The source codes are released at https://github.com/dinry/IURO.",
    "affiliations": [],
    "countries": [],
    "num_authors": 8
  },
  {
    "paperId": "6159ea65a6dab3499c82bc90c0b70cff4a80941c",
    "externalIds": {
      "DBLP": "conf/www/ZhangYWW22",
      "DOI": "10.1145/3485447.3512042",
      "CorpusId": 248367536
    },
    "url": "https://www.semanticscholar.org/paper/6159ea65a6dab3499c82bc90c0b70cff4a80941c",
    "title": "Neuro-Symbolic Interpretable Collaborative Filtering for Attribute-based Recommendation",
    "venue": "The Web Conference",
    "year": 2022,
    "referenceCount": 49,
    "citationCount": 31,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3485447.3512042?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3485447.3512042, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-04-25",
    "authors": [
      {
        "authorId": "40538912",
        "name": "Wei Zhang"
      },
      {
        "authorId": "2112593769",
        "name": "Junbing Yan"
      },
      {
        "authorId": "101279813",
        "name": "Zhuo Wang"
      },
      {
        "authorId": "2447408",
        "name": "Jianyong Wang"
      }
    ],
    "abstract": "Recommender System (RS) is ubiquitous on today’s Internet to provide multifaceted personalized information services. While an enormous success has been made in pushing forward high-accuracy recommendations, the other side of the coin — the recommendation explainability — needs to be better handled for pursuing persuasiveness, especially for the era of deep learning based recommendation. A few research efforts investigate interpretable recommendation from the feature and result levels. Compared with them, model-level explanation, which unfolds the reasoning process of recommendation through transparent models, still remains underexplored and deserves more attention. In this paper, we propose a model-based explainable recommendation approach, i.e., NS-ICF, which stands for Neuro-Symbolic Interpretable Collaborative Filtering. Thanks to the recent advance on neuro-symbolic computation for automatic rule learning, NS-ICF learns interpretable recommendation rules (consisting of user and item attributes) based on neural networks with two innovations: (1) a three-tower architecture tailored for the user and item sides in the RS domain; (2) fusing the powerful personalized representations of users and items to achieve adaptive rule weights and without sacrificing interpretability. Comprehensive experiments on public datasets demonstrate NS-ICF is comparable to state-of-the-art deep recommendation models and is transparent for its unique neuro-symbolic architecture.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "608e3f4501911cfef1c056a6e92643520ef2555c",
    "externalIds": {
      "ArXiv": "2005.09344",
      "DBLP": "conf/sigir/ChenYYH0020",
      "MAG": "3028091557",
      "DOI": "10.1145/3397271.3401042",
      "CorpusId": 218684894
    },
    "url": "https://www.semanticscholar.org/paper/608e3f4501911cfef1c056a6e92643520ef2555c",
    "title": "Try This Instead: Personalized and Interpretable Substitute Recommendation",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2020,
    "referenceCount": 57,
    "citationCount": 111,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.09344, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference",
      "Review"
    ],
    "publicationDate": "2020-05-19",
    "authors": [
      {
        "authorId": "1490931831",
        "name": "Tong Chen"
      },
      {
        "authorId": "2416851",
        "name": "Hongzhi Yin"
      },
      {
        "authorId": "1849611",
        "name": "Guanhua Ye"
      },
      {
        "authorId": "145622169",
        "name": "Zi Huang"
      },
      {
        "authorId": "2155651371",
        "name": "Yang Wang"
      },
      {
        "authorId": "2146059429",
        "name": "Meng Wang"
      }
    ],
    "abstract": "As a fundamental yet significant process in personalized recommendation, candidate generation and suggestion effectively help users spot the most suitable items for them. Consequently, identifying substitutable items that are interchangeable opens up new opportunities to refine the quality of generated candidates. When a user is browsing a specific type of product (e.g., a laptop) to buy, the accurate recommendation of substitutes (e.g., better equipped laptops) can offer the user more suitable options to choose from, thus substantially increasing the chance of a successful purchase. However, existing methods merely treat this problem as mining pairwise item relationships without the consideration of users' personal preferences. Moreover, the substitutable relationships are implicitly identified through the learned latent representations of items, leading to uninterpretable recommendation results. In this paper, we propose attribute-aware collaborative filtering (A2CF) to perform substitute recommendation by addressing issues from both personalization and interpretability perspectives. In A2CF, instead of directly modelling user-item interactions, we extract explicit and polarized item attributes from user reviews with sentiment analysis, whereafter the representations of attributes, users, and items are simultaneously learned. Then, by treating attributes as the bridge between users and items, we can thoroughly model the user-item preferences (i.e., personalization) and item-item relationships (i.e., substitution) for recommendation. In addition, A2CF is capable of generating intuitive interpretations by analyzing which attributes a user currently cares the most and comparing the recommended substitutes with her/his currently browsed items at an attribute level. The recommendation effectiveness and interpretation quality of A2CF are further demonstrated via extensive experiments on three real-life datasets.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "e9ef0b133d8720591a9dc90a47e5cfaf56f604cd",
    "externalIds": {
      "DBLP": "conf/www/SunZZHX21",
      "DOI": "10.1145/3442381.3449985",
      "CorpusId": 235324834
    },
    "url": "https://www.semanticscholar.org/paper/e9ef0b133d8720591a9dc90a47e5cfaf56f604cd",
    "title": "Cost-Effective and Interpretable Job Skill Recommendation with Deep Reinforcement Learning",
    "venue": "The Web Conference",
    "year": 2021,
    "referenceCount": 38,
    "citationCount": 29,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3442381.3449985",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3442381.3449985?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3442381.3449985, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2021-04-19",
    "authors": [
      {
        "authorId": "2108939663",
        "name": "Ying Sun"
      },
      {
        "authorId": "1799525",
        "name": "Fuzhen Zhuang"
      },
      {
        "authorId": "1968806",
        "name": "Hengshu Zhu"
      },
      {
        "authorId": "144131273",
        "name": "Qing He"
      },
      {
        "authorId": "2093122576",
        "name": "Hui Xiong"
      }
    ],
    "abstract": "Nowadays, as organizations operate in very fast-paced and competitive environments, workforce has to be agile and adaptable to regularly learning new job skills. However, it is nontrivial for talents to know which skills to develop at each working stage. To this end, in this paper, we aim to develop a cost-effective recommendation system based on deep reinforcement learning, which can provide personalized and interpretable job skill recommendation for each talent. Specifically, we first design an environment to estimate the utilities of skill learning by mining the massive job advertisement data, which includes a skill-matching-based salary estimator and a frequent itemset-based learning difficulty estimator. Based on the environment, we design a Skill Recommendation Deep Q-Network (SRDQN) with multi-task structure to estimate the long-term skill learning utilities. In particular, SRDQN recommends job skills in a personalized and cost-effective manner; that is, the talents will only learn the recommended necessary skills for achieving their career goals. Finally, extensive experiments on a real-world dataset clearly validate the effectiveness and interpretability of our approach.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "8480141c2fffad657ba88f79b6e5c87bcfb26cd1",
    "externalIds": {
      "MAG": "3185913548",
      "DBLP": "journals/intr/YangL22",
      "DOI": "10.1108/intr-08-2020-0471",
      "CorpusId": 237698776
    },
    "url": "https://www.semanticscholar.org/paper/8480141c2fffad657ba88f79b6e5c87bcfb26cd1",
    "title": "Interpretable video tag recommendation with multimedia deep learning framework",
    "venue": "Internet Research",
    "year": 2021,
    "referenceCount": 45,
    "citationCount": 19,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1108/intr-08-2020-0471?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1108/intr-08-2020-0471, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-07-26",
    "authors": [
      {
        "authorId": "1582890834",
        "name": "Zekun Yang"
      },
      {
        "authorId": "2112412685",
        "name": "Zhijie Lin"
      }
    ],
    "abstract": "PurposeTags help promote customer engagement on video-sharing platforms. Video tag recommender systems are artificial intelligence-enabled frameworks that strive for recommending precise tags for videos. Extant video tag recommender systems are uninterpretable, which leads to distrust of the recommendation outcome, hesitation in tag adoption and difficulty in the system debugging process. This study aims at constructing an interpretable and novel video tag recommender system to assist video-sharing platform users in tagging their newly uploaded videos.Design/methodology/approachThe proposed interpretable video tag recommender system is a multimedia deep learning framework composed of convolutional neural networks (CNNs), which receives texts and images as inputs. The interpretability of the proposed system is realized through layer-wise relevance propagation.FindingsThe case study and user study demonstrate that the proposed interpretable multimedia CNN model could effectively explain its recommended tag to users by highlighting keywords and key patches that contribute the most to the recommended tag. Moreover, the proposed model achieves an improved recommendation performance by outperforming state-of-the-art models.Practical implicationsThe interpretability of the proposed recommender system makes its decision process more transparent, builds users’ trust in the recommender systems and prompts users to adopt the recommended tags. Through labeling videos with human-understandable and accurate tags, the exposure of videos to their target audiences would increase, which enhances information technology (IT) adoption, customer engagement, value co-creation and precision marketing on the video-sharing platform.Originality/valueThe proposed model is not only the first explainable video tag recommender system but also the first explainable multimedia tag recommender system to the best of our knowledge.",
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "83c518af8dde9f096c776fd86040a7cb992d7345",
    "externalIds": {
      "DBLP": "journals/intr/FanJLZ22",
      "MAG": "3173342190",
      "DOI": "10.1108/intr-08-2020-0477",
      "CorpusId": 237877261
    },
    "url": "https://www.semanticscholar.org/paper/83c518af8dde9f096c776fd86040a7cb992d7345",
    "title": "Interpretable MOOC recommendation: a multi-attention network for personalized learning behavior analysis",
    "venue": "Internet Research",
    "year": 2021,
    "referenceCount": 50,
    "citationCount": 31,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1108/intr-08-2020-0477?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1108/intr-08-2020-0477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2021-06-24",
    "authors": [
      {
        "authorId": "2148329494",
        "name": "Ju Fan"
      },
      {
        "authorId": "3422635",
        "name": "Yuanchun Jiang"
      },
      {
        "authorId": "2114770",
        "name": "Yezheng Liu"
      },
      {
        "authorId": "2145493959",
        "name": "Yonghang Zhou"
      }
    ],
    "abstract": "PurposeCourse recommendations are important for improving learner satisfaction and reducing dropout rates on massive open online course (MOOC) platforms. This study aims to propose an interpretable method of analyzing students' learning behaviors and recommending MOOCs by integrating multiple data sources.Design/methodology/approachThe study proposes a deep learning method of recommending MOOCs to students based on a multi-attention mechanism comprising learning records attention, word-level review attention, sentence-level review attention and course description attention. The proposed model is validated using real-world data consisting of the learning records of 6,628 students for 1,789 courses and 65,155 reviews.FindingsThe main contribution of this study is its exploration of multiple unstructured information using the proposed multi-attention network model. It provides an interpretable strategy for analyzing students' learning behaviors and conducting personalized MOOC recommendations.Practical implicationsThe findings suggest that MOOC platforms must fully utilize the information implied in course reviews to extract personalized learning preferences.Originality/valueThis study is the first attempt to recommend MOOCs by exploring students' preferences in course reviews. The proposed multi-attention mechanism improves the interpretability of MOOC recommendations.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "03053c61c32e88d41b2e7dedcba4ed208715d2c7",
    "externalIds": {
      "DBLP": "journals/eswa/YeL21",
      "MAG": "3113225827",
      "DOI": "10.1016/j.eswa.2020.114454",
      "CorpusId": 230585381
    },
    "url": "https://www.semanticscholar.org/paper/03053c61c32e88d41b2e7dedcba4ed208715d2c7",
    "title": "An interpretable sequential three-way recommendation based on collaborative topic regression",
    "venue": "Expert systems with applications",
    "year": 2021,
    "referenceCount": 33,
    "citationCount": 45,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.eswa.2020.114454?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.eswa.2020.114454, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-04-15",
    "authors": [
      {
        "authorId": "48673687",
        "name": "Xiaoqing Ye"
      },
      {
        "authorId": "1802040",
        "name": "Dun Liu"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "5141d0822cd2e5c5e1b8b761fe31049d91ad65b1",
    "externalIds": {
      "DBLP": "journals/jmlr/LiuJWXWYN21",
      "CorpusId": 238999227
    },
    "url": "https://www.semanticscholar.org/paper/5141d0822cd2e5c5e1b8b761fe31049d91ad65b1",
    "title": "Interpretable Deep Generative Recommendation Models",
    "venue": "Journal of machine learning research",
    "year": 2021,
    "referenceCount": 73,
    "citationCount": 14,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2145489099",
        "name": "Huafeng Liu"
      },
      {
        "authorId": "144889532",
        "name": "L. Jing"
      },
      {
        "authorId": "2113341875",
        "name": "Jingxuan Wen"
      },
      {
        "authorId": "2153916677",
        "name": "Pengyu Xu"
      },
      {
        "authorId": "2276490385",
        "name": "Jiaqi Wang"
      },
      {
        "authorId": "1740321",
        "name": "Jian Yu"
      },
      {
        "authorId": "2054543584",
        "name": "Michael K. Ng"
      },
      {
        "authorId": "1751569",
        "name": "Samy Bengio"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 8
  },
  {
    "paperId": "457ac20839d737e3e5c2bb93daf18ba17c15c905",
    "externalIds": {
      "DBLP": "journals/jifs/YEYZ21",
      "MAG": "3129729133",
      "DOI": "10.3233/JIFS-202308",
      "CorpusId": 234141772
    },
    "url": "https://www.semanticscholar.org/paper/457ac20839d737e3e5c2bb93daf18ba17c15c905",
    "title": "An interpretable mechanism for personalized recommendation based on cross feature",
    "venue": "Journal of Intelligent & Fuzzy Systems",
    "year": 2021,
    "referenceCount": 26,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3233/JIFS-202308?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3233/JIFS-202308, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2111920328",
        "name": "Lv Ye"
      },
      {
        "authorId": "2118064936",
        "name": "Yue Yang"
      },
      {
        "authorId": "2090374470",
        "name": "Jianxu Zeng"
      }
    ],
    "abstract": "The existing recommender system provides personalized recommendation service for users in online shopping, entertainment, and other activities. In order to improve the probability of users accepting the system’s recommendation service, compared with the traditional recommender system, the interpretable recommender system will give the recommendation reasons and results at the same time. In this paper, an interpretable recommendation model based on XGBoost tree is proposed to obtain comprehensible and effective cross features from side information. The results are input into the embedded model based on attention mechanism to capture the invisible interaction among user IDs, item IDs and cross features. The captured interactions are used to predict the match score between the user and the recommended item. Cross-feature attention score is used to generate different recommendation reasons for different user-items.Experimental results show that the proposed algorithm can guarantee the quality of recommendation. The transparency and readability of the recommendation process has been improved by providing reference reasons. This method can help users better understand the recommendation behavior of the system and has certain enlightenment to help the recommender system become more personalized and intelligent.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "d383f02f62bb31275265710d952cc8feda869875",
    "externalIds": {
      "MAG": "3171878434",
      "DBLP": "journals/intr/DuYZM22",
      "DOI": "10.1108/INTR-08-2020-0473",
      "CorpusId": 236299736
    },
    "url": "https://www.semanticscholar.org/paper/d383f02f62bb31275265710d952cc8feda869875",
    "title": "Leveraging online behaviors for interpretable knowledge-aware patent recommendation",
    "venue": "Internet Research",
    "year": 2021,
    "referenceCount": 62,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1108/INTR-08-2020-0473?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1108/INTR-08-2020-0473, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-06-11",
    "authors": [
      {
        "authorId": "2072591721",
        "name": "Wei Du"
      },
      {
        "authorId": "2151898602",
        "name": "Qiang Yan"
      },
      {
        "authorId": "50549847",
        "name": "Wenping Zhang"
      },
      {
        "authorId": "2146391772",
        "name": "Jian Ma"
      }
    ],
    "abstract": "PurposePatent trade recommendations necessitate recommendation interpretability in addition to recommendation accuracy because of patent transaction risks and the technological complexity of patents. This study designs an interpretable knowledge-aware patent recommendation model (IKPRM) for patent trading. IKPRM first creates a patent knowledge graph (PKG) for patent trade recommendations and then leverages paths in the PKG to achieve recommendation interpretability.Design/methodology/approachFirst, we construct a PKG to integrate online company behaviors and patent information using natural language processing techniques. Second, a bidirectional long short-term memory network (BiLSTM) is utilized with an attention mechanism to establish the connecting paths of a company — patent pair in PKG. Finally, the prediction score of a company — patent pair is calculated by assigning different weights to their connecting paths. The semantic relationships in connecting paths help explain why a candidate patent is recommended.FindingsExperiments on a real dataset from a patent trading platform verify that IKPRM significantly outperforms baseline methods in terms of hit ratio and normalized discounted cumulative gain (nDCG). The analysis of an online user study verified the interpretability of our recommendations.Originality/valueA meta-path-based recommendation can achieve certain explainability but suffers from low flexibility when reasoning on heterogeneous information. To bridge this gap, we propose the IKPRM to explain the full paths in the knowledge graph. IKPRM demonstrates good performance and transparency and is a solid foundation for integrating interpretable artificial intelligence into complex tasks such as intelligent recommendations.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "543de44dea9831b8e7576b0970120173ac14ffe2",
    "externalIds": {
      "MAG": "3105911749",
      "ArXiv": "2007.06133",
      "DBLP": "conf/ijcai/PanLLZ20",
      "DOI": "10.24963/ijcai.2020/373",
      "CorpusId": 277466145
    },
    "url": "https://www.semanticscholar.org/paper/543de44dea9831b8e7576b0970120173ac14ffe2",
    "title": "Explainable Recommendation via Interpretable Feature Mapping and Evaluation of Explainability",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2020,
    "referenceCount": 34,
    "citationCount": 21,
    "openAccessPdf": {
      "url": "https://www.ijcai.org/proceedings/2020/0373.pdf",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.06133, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-07-01",
    "authors": [
      {
        "authorId": "1727055",
        "name": "Deng Pan"
      },
      {
        "authorId": "2145429537",
        "name": "Xiangrui Li"
      },
      {
        "authorId": "50080172",
        "name": "X. Li"
      },
      {
        "authorId": "2331463316",
        "name": "Dongxiao Zhu"
      }
    ],
    "abstract": "Latent factor collaborative filtering (CF) has been a widely used technique for recommender system by learning the semantic representations of users and items. Recently, explainable recommendation has attracted much attention from research community. However, trade-off exists between explainability and performance of the recommendation where metadata is often needed to alleviate the dilemma. We present a novel feature mapping approach that maps the uninterpretable general features onto the interpretable aspect features, achieving both satisfactory\n\naccuracy and explainability in the recommendations by simultaneous minimization of rating prediction loss and interpretation loss. To evaluate\n\nthe explainability, we propose two new evaluation metrics specifically designed for aspect-level explanation using surrogate ground truth. Experimental results demonstrate a strong performance in both recommendation and explaining explanation, eliminating the need for metadata. Code is available from https://github.com/pd90506/AMCF.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "1a1fb1da841f1ba05cc61b9636458ae0827700a7",
    "externalIds": {
      "MAG": "3099775457",
      "DBLP": "journals/corr/abs-2007-15236",
      "ArXiv": "2007.15236",
      "DOI": "10.1145/3383313.3412211",
      "CorpusId": 220870928
    },
    "url": "https://www.semanticscholar.org/paper/1a1fb1da841f1ba05cc61b9636458ae0827700a7",
    "title": "Interpretable Contextual Team-aware Item Recommendation: Application in Multiplayer Online Battle Arena Games",
    "venue": "ACM Conference on Recommender Systems",
    "year": 2020,
    "referenceCount": 27,
    "citationCount": 18,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2007.15236",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.15236, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2020-07-30",
    "authors": [
      {
        "authorId": "2064406147",
        "name": "Andrés Villa"
      },
      {
        "authorId": "88988180",
        "name": "Vladimir Araujo"
      },
      {
        "authorId": "1840918150",
        "name": "Francisca Cattan"
      },
      {
        "authorId": "145831267",
        "name": "Denis Parra"
      }
    ],
    "abstract": "The video game industry has adopted recommendation systems to boost users interest with a focus on game sales. Other exciting applications within video games are those that help the player make decisions that would maximize their playing experience, which is a desirable feature in real-time strategy video games such as Multiplayer Online Battle Arena (MOBA) like as DotA and LoL. Among these tasks, the recommendation of items is challenging, given both the contextual nature of the game and how it exposes the dependence on the formation of each team. Existing works on this topic do not take advantage of all the available contextual match data and dismiss potentially valuable information. To address this problem we develop TTIR, a contextual recommender model derived from the Transformer neural architecture that suggests a set of items to every team member, based on the contexts of teams and roles that describe the match. TTIR outperforms several approaches and provides interpretable recommendations through visualization of attention weights. Our evaluation indicates that both the Transformer architecture and the contextual information are essential to get the best results for this item recommendation task. Furthermore, a preliminary user survey indicates the usefulness of attention weights for explaining recommendations as well as ideas for future work. The code and dataset are available at https://github.com/ojedaf/IC-TIR-Lol .",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "d9ad09017b1a585502d8fd0c7e2a6d5c5b746a6e",
    "externalIds": {
      "MAG": "3013206027",
      "DBLP": "journals/access/WuZC20",
      "DOI": "10.1109/ACCESS.2020.2978272",
      "CorpusId": 216106136
    },
    "url": "https://www.semanticscholar.org/paper/d9ad09017b1a585502d8fd0c7e2a6d5c5b746a6e",
    "title": "Visual and Textual Jointly Enhanced Interpretable Fashion Recommendation",
    "venue": "IEEE Access",
    "year": 2020,
    "referenceCount": 39,
    "citationCount": 16,
    "openAccessPdf": {
      "url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09046774.pdf",
      "status": "GOLD",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.2978272?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.2978272, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2115911214",
        "name": "Qianqian Wu"
      },
      {
        "authorId": "2927967",
        "name": "Pengpeng Zhao"
      },
      {
        "authorId": "1765993",
        "name": "Zhiming Cui"
      }
    ],
    "abstract": "With the rapid development of online shopping, interpretable personalized fashion recommendation using image has attracted increasing attention in recent years. The current work has been able to capture the user’s preferences for visible features and provide visual explanations. However, they ignored the invisible features, such as the material and quality of the clothes, and failed to offer textual explanations. To this end, we propose a Visual and Textual Jointly Enhanced Interpretable (VTJEI) model for fashion recommendations based on the product image and historical review. The VTJEI can provide more accurate recommendations and visual and textual explanations through the joint enhancement of textual information and visual information. Specifically, we design a bidirectional two-layer adaptive attention review model to capture the user’s visible and invisible preferences to the target product and provide textual explanations by highlighting some words. Moreover, we propose a review-driven visual attention model to get a more personalized image representation driven by the user’s preference obtained from the historical review. In this way, we not only realize the joint enhancement of visual information and textual information but also provide a visual explanation by highlighting some regions. Finally, we performed extensive experiments on real datasets to confirm the superiority of our model on Top-N recommendations. We also built a labeled dataset for evaluating our provided visible and invisible explanations quantitatively. The result shows that we can not only provide more accurate recommendations but also can provide both visual and textual explanations.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "21f6904f85029c2b58f3925cfa99661bc8a4ffc3",
    "externalIds": {
      "DBLP": "conf/lats/ZhaoBTZG20",
      "MAG": "3046437295",
      "DOI": "10.1145/3386527.3406739",
      "CorpusId": 220885773
    },
    "url": "https://www.semanticscholar.org/paper/21f6904f85029c2b58f3925cfa99661bc8a4ffc3",
    "title": "Interpretable Personalized Knowledge Tracing and Next Learning Activity Recommendation",
    "venue": "ACM Conference on Learning @ Scale",
    "year": 2020,
    "referenceCount": 13,
    "citationCount": 17,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3386527.3406739?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3386527.3406739, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2020-08-12",
    "authors": [
      {
        "authorId": "1845781483",
        "name": "Jinjin Zhao"
      },
      {
        "authorId": "23127304",
        "name": "Shreyansh P. Bhatt"
      },
      {
        "authorId": "4124476",
        "name": "Candace Thille"
      },
      {
        "authorId": "49314374",
        "name": "D. Zimmaro"
      },
      {
        "authorId": "1845780509",
        "name": "Neelesh Gattani"
      }
    ],
    "abstract": "Online learning systems that provide actionable and personalized guidance can help learners make better decisions during learning. Bayesian Knowledge Tracing (BKT) extensions and deep learning based approaches have demonstrated improved mastery prediction accuracy compared to the basic BKT model; however, neither set of models provides actionable guidance on learning activities beyond mastery prediction. We propose a novel framework for personalized knowledge tracing with attention mechanism. Our proposed framework incorporates auxiliary learner attributes into knowledge tracing and interprets mastery prediction with the learning attributes. The proposed approach can also provide personalized next best learning activity recommendations. We demonstrate that the accuracy of the proposed approach in mastery prediction is slightly higher compared to deep learning based approaches and that the proposed approach can provide personalized next best learning activity recommendation.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "04dcd39f03f1bd7aaafa256d486e83c1ba45ccaf",
    "externalIds": {
      "MAG": "3113206165",
      "DBLP": "conf/qrs/KeWFGS20",
      "DOI": "10.1109/QRS51102.2020.00068",
      "CorpusId": 228097715
    },
    "url": "https://www.semanticscholar.org/paper/04dcd39f03f1bd7aaafa256d486e83c1ba45ccaf",
    "title": "Interpretable Test Case Recommendation based on Knowledge Graph",
    "venue": "International Conference on Software Quality, Reliability and Security",
    "year": 2020,
    "referenceCount": 33,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/QRS51102.2020.00068?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/QRS51102.2020.00068, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-12-01",
    "authors": [
      {
        "authorId": "2034348705",
        "name": "Wenjun Ke"
      },
      {
        "authorId": "2115424644",
        "name": "Chao Wu"
      },
      {
        "authorId": "2175530057",
        "name": "Xiufeng Fu"
      },
      {
        "authorId": "2114086587",
        "name": "Chen Gao"
      },
      {
        "authorId": "2034347474",
        "name": "Yinyi Song"
      }
    ],
    "abstract": "Reproducing bugs and identifying causes is essential for the debugging of complex software systems. However, existing test case selection and recommendation technique diagnose bugs but failed to provide information to understand the cause. In this paper, we present an interpretable test case recommendation technique by building up knowledge graphs based on massive test cases, bug reports, code changes, and documents stored in software repositories. Specifically, it identifies correlations between new issue reports and historical information based on the knowledge graph and thus present test cases and corresponding documents to support the bug diagnosis. We conduct an empirical study on autonomous driving systems to show our technique is capable of identifying the proper test case. Further, we validate the effectiveness of recommended interpretation. The study shows that the recommended interpretation can help testers to comprehend bug reports and diagnose bugs efficiently.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "ec06b9ba949c0cb5f1ae6fb27373d4c6234eceea",
    "externalIds": {
      "MAG": "3043945839",
      "DOI": "10.1360/ssi-2019-0268",
      "CorpusId": 225517568
    },
    "url": "https://www.semanticscholar.org/paper/ec06b9ba949c0cb5f1ae6fb27373d4c6234eceea",
    "title": "An interpretable attraction recommendation method based on knowledge graph",
    "venue": "Scientia Sinica Informationis",
    "year": 2020,
    "referenceCount": 26,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://www.sciengine.com/doi/pdf/4A1EB01F7476464687E12673FD436BFC",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1360/ssi-2019-0268?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1360/ssi-2019-0268, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": null,
    "publicationDate": "2020-07-01",
    "authors": [
      {
        "authorId": "2149483374",
        "name": "Li Yu"
      },
      {
        "authorId": "2157656075",
        "name": "Zongcai Huang"
      },
      {
        "authorId": "2065810964",
        "name": "Peiyuan Qiu"
      },
      {
        "authorId": "2149258162",
        "name": "Jialiang Gao"
      },
      {
        "authorId": "2114295961",
        "name": "Feng Lu"
      }
    ],
    "abstract": "The attraction recommendation systems not only (cid:12)lter out overwhelming irrelevant information for visitors but also identify potential customers for service providers. However, the current attraction recommendation methods such as content-based methods, collaborative (cid:12)ltering, or deep learning-based methods are either inaccurate due to data sparsity, or lack of interpretability, which results in the users’ suspicion on the recommendation results. To address the limitations of the current methods, we introduce a novel framework for preference propagation on knowledge graphs (KGs), which utilizes lots of parameters to capture the abundant semantics of existing KGs more comprehensively, and meanwhile explains the results through reasoning the link paths from user’s history to candidates on KGs. With a multi-view spatiotemporal analysis on real-world travel data, we investigate the geographical characteristics of human tour activities and build a tourism-oriented KG based on open web resources. Then, we propose a KG-aware attraction recommendation method named Geo-RippleNet and implement it with extensive experiments on large-scale datasets. It is argued that the framework for preference propagation on KGs not only absorb rich semantic information to achieve substantial performance gains in the attraction recommendation scenario but also enhance the interpretability of recommendation results with the support of abundant relational knowledge. Moreover, incorporating the spatiotemporal characteristics of human tour activities into the framework for preference propagation further makes the recommendation performance more aligned with the potential interests of visitors.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "8c3fbf49464fa1878e5177c217fd13b200250600",
    "externalIds": {
      "MAG": "2997892038",
      "DBLP": "journals/corr/abs-2001-04346",
      "ArXiv": "2001.04346",
      "DOI": "10.1609/AAAI.V34I05.6268",
      "CorpusId": 210164537
    },
    "url": "https://www.semanticscholar.org/paper/8c3fbf49464fa1878e5177c217fd13b200250600",
    "title": "Asymmetrical Hierarchical Networks with Attentive Interactions for Interpretable Review-Based Recommendation",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2019,
    "referenceCount": 31,
    "citationCount": 57,
    "openAccessPdf": {
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/6268/6124",
      "status": "GOLD",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2001.04346, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Conference",
      "Review"
    ],
    "publicationDate": "2019-12-18",
    "authors": [
      {
        "authorId": "38639538",
        "name": "Xin Dong"
      },
      {
        "authorId": "2090567",
        "name": "Jingchao Ni"
      },
      {
        "authorId": "92186360",
        "name": "Wei Cheng"
      },
      {
        "authorId": "1766853",
        "name": "Zhengzhang Chen"
      },
      {
        "authorId": "2991105",
        "name": "Bo Zong"
      },
      {
        "authorId": "2451800",
        "name": "Dongjin Song"
      },
      {
        "authorId": "3215702",
        "name": "Yanchi Liu"
      },
      {
        "authorId": "2145225543",
        "name": "Haifeng Chen"
      },
      {
        "authorId": "144608002",
        "name": "Gerard de Melo"
      }
    ],
    "abstract": "Recently, recommender systems have been able to emit substantially improved recommendations by leveraging user-provided reviews. Existing methods typically merge all reviews of a given user (item) into a long document, and then process user and item documents in the same manner. In practice, however, these two sets of reviews are notably different: users' reviews reflect a variety of items that they have bought and are hence very heterogeneous in their topics, while an item's reviews pertain only to that single item and are thus topically homogeneous. In this work, we develop a novel neural network model that properly accounts for this important difference by means of asymmetric attentive modules. The user module learns to attend to only those signals that are relevant with respect to the target item, whereas the item module learns to extract the most salient contents with regard to properties of the item. Our multi-hierarchical paradigm accounts for the fact that neither are all reviews equally useful, nor are all sentences within each review equally pertinent. Extensive experimental results on a variety of real datasets demonstrate the effectiveness of our method.",
    "affiliations": [],
    "countries": [],
    "num_authors": 9
  },
  {
    "paperId": "0f8bd2127d671b3bd0cdf39563ab714adc2fa552",
    "externalIds": {
      "MAG": "2908521458",
      "DBLP": "journals/access/ZhangZLZZ19",
      "DOI": "10.1109/ACCESS.2019.2891513",
      "CorpusId": 59553927
    },
    "url": "https://www.semanticscholar.org/paper/0f8bd2127d671b3bd0cdf39563ab714adc2fa552",
    "title": "An Interpretable and Scalable Recommendation Method Based on Network Embedding",
    "venue": "IEEE Access",
    "year": 2019,
    "referenceCount": 31,
    "citationCount": 11,
    "openAccessPdf": {
      "url": "https://ieeexplore.ieee.org/ielx7/6287639/8600701/08613775.pdf",
      "status": "GOLD",
      "license": "CCBYNCND",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2019.2891513?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2019.2891513, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2019-01-16",
    "authors": [
      {
        "authorId": "2108132766",
        "name": "Xuejian Zhang"
      },
      {
        "authorId": "145793197",
        "name": "Zhongying Zhao"
      },
      {
        "authorId": "46651287",
        "name": "C. Li"
      },
      {
        "authorId": "2144289435",
        "name": "Yong Zhang"
      },
      {
        "authorId": "2130175278",
        "name": "Jianli Zhao"
      }
    ],
    "abstract": "Matrix factorization is a widely used technique in recommender systems. However, its performance is often affected by the sparsity and the scalability. To address the above-mentioned problem, we propose an interpretable and scalable recommendation method based on network embedding (ISRM_NE) in this paper. First, a novel user-item co-occurrence network is presented, which reflects both the user’s preferences and the co-occurrence relationship among items. Second, the conceptions of tightness and equivalence are given to describe the structural similarity in the network, which can explore four relationships in recommender system: user’s preference, item co-occurrence relationship, user’s potential preference, and similarity between users. Finally, two sampling strategies are combined to traverse the network so as to get the latent vector of items and users through network representation learning. Thus, the top-N recommendation can be achieved by vector computing. The proposed method called ISRM_NE improves the performance of the recommender system in terms of interpretability and scalability. Moreover, extensive experiments on two real-world datasets demonstrate that the ISRM_NE outperforms three popular methods.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "7e2389cab2334aaa0d7fe8f5185a2ba63372dc30",
    "externalIds": {
      "MAG": "2954678526",
      "DBLP": "journals/ijautcomp/BanerjeeCG19",
      "DOI": "10.1007/s11633-019-1185-8",
      "CorpusId": 198456362
    },
    "url": "https://www.semanticscholar.org/paper/7e2389cab2334aaa0d7fe8f5185a2ba63372dc30",
    "title": "A Wide Learning Approach for Interpretable Feature Recommendation for 1-D Sensor Data in IoT Analytics",
    "venue": "International Journal of Automation and Computing",
    "year": 2019,
    "referenceCount": 50,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11633-019-1185-8?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11633-019-1185-8, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2019-07-02",
    "authors": [
      {
        "authorId": "153564586",
        "name": "Snehasish Banerjee"
      },
      {
        "authorId": "34923882",
        "name": "T. Chattopadhyay"
      },
      {
        "authorId": "1804457",
        "name": "Utpal Garain"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "336067ade694c79b5838b2e8158acf18546bc5a5",
    "externalIds": {
      "ArXiv": "1806.09820",
      "DBLP": "journals/corr/abs-1806-09820",
      "MAG": "2887251607",
      "CorpusId": 49429829
    },
    "url": "https://www.semanticscholar.org/paper/336067ade694c79b5838b2e8158acf18546bc5a5",
    "title": "Visually-Aware Personalized Recommendation using Interpretable Image Representations",
    "venue": "arXiv.org",
    "year": 2018,
    "referenceCount": 9,
    "citationCount": 19,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.09820, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2018-06-26",
    "authors": [
      {
        "authorId": "9955459",
        "name": "Charles Packer"
      },
      {
        "authorId": "35660011",
        "name": "Julian McAuley"
      },
      {
        "authorId": "1780343",
        "name": "Arnau Ramisa"
      }
    ],
    "abstract": "Visually-aware recommender systems use visual signals present in the underlying data to model the visual characteristics of items and users' preferences towards them. In the domain of clothing recommendation, incorporating items' visual information (e.g., product images) is particularly important since clothing item appearance is often a critical factor in influencing the user's purchasing decisions. Current state-of-the-art visually-aware recommender systems utilize image features extracted from pre-trained deep convolutional neural networks, however these extremely high-dimensional representations are difficult to interpret, especially in relation to the relatively low number of visual properties that may guide users' decisions. \nIn this paper we propose a novel approach to personalized clothing recommendation that models the dynamics of individual users' visual preferences. By using interpretable image representations generated with a unique feature learning process, our model learns to explain users' prior feedback in terms of their affinity towards specific visual attributes and styles. Our approach achieves state-of-the-art performance on personalized ranking tasks, and the incorporation of interpretable visual features allows for powerful model introspection, which we demonstrate by using an interactive recommendation algorithm and visualizing the rise and fall of fashion trends over time.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "7cc3e02ef88084aeb289652b579ba579ee9cdc2b",
    "externalIds": {
      "ArXiv": "2412.08847",
      "DBLP": "conf/kdd/ZhangWMTNLMJCZ025",
      "DOI": "10.48550/arXiv.2412.08847",
      "CorpusId": 274655977
    },
    "url": "https://www.semanticscholar.org/paper/7cc3e02ef88084aeb289652b579ba579ee9cdc2b",
    "title": "MOPI-HFRS: A Multi-objective Personalized Health-aware Food Recommendation System with LLM-enhanced Interpretation",
    "venue": "Knowledge Discovery and Data Mining",
    "year": 2024,
    "referenceCount": 56,
    "citationCount": 16,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.08847, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-12-12",
    "authors": [
      {
        "authorId": "2284184904",
        "name": "Zheyuan Zhang"
      },
      {
        "authorId": "2284578810",
        "name": "Zehong Wang"
      },
      {
        "authorId": "2283516218",
        "name": "Tianyi Ma"
      },
      {
        "authorId": "2334864668",
        "name": "Varun Sameer Taneja"
      },
      {
        "authorId": "2334866360",
        "name": "Sofia Nelson"
      },
      {
        "authorId": "2334864862",
        "name": "Nhi Ha Lan Le"
      },
      {
        "authorId": "3377711",
        "name": "K. Murugesan"
      },
      {
        "authorId": "67171225",
        "name": "Mingxuan Ju"
      },
      {
        "authorId": "2292582566",
        "name": "Nitesh V. Chawla"
      },
      {
        "authorId": "2117879943",
        "name": "Chuxu Zhang"
      },
      {
        "authorId": "2256127487",
        "name": "Yanfang Ye"
      }
    ],
    "abstract": "The prevalence of unhealthy eating habits has become a growing concern in the United States. However, popular food recommendation platforms, such as Yelp, tend to prioritize users' dietary preferences over the healthiness of their choices. While some efforts have focused on developing health-aware food recommendation systems, personalization based on specific health conditions remains underexplored. Additionally, the lack of interpretability in these systems prevents users from evaluating the reliability of recommendations, limiting their practical adoption. To address these issues, we introduce two large-scale personalized health-aware food recommendation benchmarks at the first attempt. Building on this, we propose a novel framework called the Multi-Objective Personalized Interpretable Health-aware Food Recommendation System (MOPI-HFRS). This system generates food recommendations by jointly optimizing three objectives: user preference, personalized healthiness, and nutritional diversity. It also incorporates a reasoning module enhanced by large language models (LLMs) to provide interpretable recommendations that promote healthy dietary knowledge. The framework integrates descriptive features and health data using two structure learning and pooling modules within a graph learning framework. Pareto optimization is applied to balance the multi-faceted objectives. To further enhance healthy dietary knowledge, the system leverages LLMs by infusing knowledge from the recommendation model, generating meaningful interpretations for the recommendations. Extensive experiments on the proposed benchmarks demonstrate that MOPI-HFRS outperforms state-of-the-art methods by delivering diverse, healthy food recommendations alongside reliable explanations.",
    "affiliations": [],
    "countries": [],
    "num_authors": 11
  },
  {
    "paperId": "288ba03fd161df4d6d1aab03638e2b175890e083",
    "externalIds": {
      "DBLP": "conf/naacl/BismayDC25",
      "ArXiv": "2410.23180",
      "DOI": "10.48550/arXiv.2410.23180",
      "CorpusId": 273695612
    },
    "url": "https://www.semanticscholar.org/paper/288ba03fd161df4d6d1aab03638e2b175890e083",
    "title": "ReasoningRec: Bridging Personalized Recommendations and Human-Interpretable Explanations through LLM Reasoning",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "referenceCount": 37,
    "citationCount": 13,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.23180, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-10-30",
    "authors": [
      {
        "authorId": "2328308788",
        "name": "Millennium Bismay"
      },
      {
        "authorId": "1716200686",
        "name": "Xiangjue Dong"
      },
      {
        "authorId": "2301581640",
        "name": "James Caverlee"
      }
    ],
    "abstract": "This paper presents ReasoningRec, a reasoning-based recommendation framework that leverages Large Language Models (LLMs) to bridge the gap between recommendations and human-interpretable explanations. In contrast to conventional recommendation systems that rely on implicit user-item interactions, ReasoningRec employs LLMs to model users and items, focusing on preferences, aversions, and explanatory reasoning. The framework utilizes a larger LLM to generate synthetic explanations for user preferences, subsequently used to fine-tune a smaller LLM for enhanced recommendation accuracy and human-interpretable explanation. Our experimental study investigates the impact of reasoning and contextual information on personalized recommendations, revealing that the quality of contextual and personalized data significantly influences the LLM's capacity to generate plausible explanations. Empirical evaluations demonstrate that ReasoningRec surpasses state-of-the-art methods by up to 12.5\\% in recommendation prediction while concurrently providing human-intelligible explanations. The code is available here: https://github.com/millenniumbismay/reasoningrec.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "c03cc9196512f6587f5241f23b2e1cc546328584",
    "externalIds": {
      "PubMedCentral": "10453635",
      "DOI": "10.3390/diagnostics13162681",
      "CorpusId": 260941132,
      "PubMed": "37627940"
    },
    "url": "https://www.semanticscholar.org/paper/c03cc9196512f6587f5241f23b2e1cc546328584",
    "title": "Interpretable Machine Learning for Personalized Medical Recommendations: A LIME-Based Approach",
    "venue": "Diagnostics",
    "year": 2023,
    "referenceCount": 46,
    "citationCount": 37,
    "openAccessPdf": {
      "url": "https://www.mdpi.com/2075-4418/13/16/2681/pdf?version=1692092666",
      "status": "GOLD",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10453635, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-01",
    "authors": [
      {
        "authorId": "2145066102",
        "name": "Yuanyuan Wu"
      },
      {
        "authorId": "2231861812",
        "name": "Linfei Zhang"
      },
      {
        "authorId": "11016472",
        "name": "U. Bhatti"
      },
      {
        "authorId": "2118492864",
        "name": "Mengxing Huang"
      }
    ],
    "abstract": "Chronic diseases are increasingly major threats to older persons, seriously affecting their physical health and well-being. Hospitals have accumulated a wealth of health-related data, including patients’ test reports, treatment histories, and diagnostic records, to better understand patients’ health, safety, and disease progression. Extracting relevant information from this data enables physicians to provide personalized patient-treatment recommendations. While collaborative filtering techniques and classical algorithms such as naive Bayes, logistic regression, and decision trees have had notable success in health-recommendation systems, most current systems primarily inform users of their likely preferences without providing explanations. This paper proposes an approach of deep learning with a local interpretable model–agnostic explanations (LIME)-based interpretable recommendation system to solve this problem. Specifically, we apply the proposed approach to two chronic diseases common in older adults: heart disease and diabetes. After data preprocessing, we use six deep-learning algorithms to form interpretations. In the heart-disease data set, the actual model recommendation of multi-layer perceptron and gradient-boosting algorithm differs from the local model’s recommendation of LIME, which can be used as its approximate prediction. From the feature importance of these two algorithms, it can be seen that the CholCheck, GenHith, and HighBP features are the most important for predicting heart disease. In the diabetes data set, the actual model predictions of the multi-layer perceptron and logistic-regression algorithm were little different from the local model’s prediction of LIME, which can be used as its approximate recommendation. Moreover, from the feature importance of the two algorithms, it can be seen that the three features of glucose, BMI, and age were the most important for predicting heart disease. Next, LIME is used to determine the importance of each feature that affected the results of the calculated model. Subsequently, we present the contribution coefficients of these features to the final recommendation. By analyzing the impact of different patient characteristics on the recommendations, our proposed system elucidates the underlying reasons behind these recommendations and enhances patient trust. This approach has important implications for medical recommendation systems and encourages informed decision-making in healthcare.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "1bfcbda4250a285c97b85f9c56ea4be5a00211bb",
    "externalIds": {
      "DBLP": "journals/jcst/XinLWH15",
      "MAG": "2239578085",
      "DOI": "10.1007/s11390-015-1570-x",
      "CorpusId": 18879203
    },
    "url": "https://www.semanticscholar.org/paper/1bfcbda4250a285c97b85f9c56ea4be5a00211bb",
    "title": "When Factorization Meets Heterogeneous Latent Topics: An Interpretable Cross-Site Recommendation Framework",
    "venue": "Journal of Computational Science and Technology",
    "year": 2015,
    "referenceCount": 45,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11390-015-1570-x?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11390-015-1570-x, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2015-07-08",
    "authors": [
      {
        "authorId": "36528303",
        "name": "Xin Xin"
      },
      {
        "authorId": "1781574",
        "name": "Chin-Yew Lin"
      },
      {
        "authorId": "7621447",
        "name": "Xiaochi Wei"
      },
      {
        "authorId": "4590286",
        "name": "Heyan Huang"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "b6948df388eb9eb1d24c70be319dc95e3291458c",
    "externalIds": {
      "ArXiv": "2403.03714",
      "DBLP": "journals/corr/abs-2403-03714",
      "DOI": "10.24963/ijcai.2023/260",
      "CorpusId": 260858817
    },
    "url": "https://www.semanticscholar.org/paper/b6948df388eb9eb1d24c70be319dc95e3291458c",
    "title": "Intent-aware Recommendation via Disentangled Graph Contrastive Learning",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2023,
    "referenceCount": 42,
    "citationCount": 31,
    "openAccessPdf": {
      "url": "https://www.ijcai.org/proceedings/2023/0260.pdf",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.03714, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-08-01",
    "authors": [
      {
        "authorId": "2115664393",
        "name": "Yuling Wang"
      },
      {
        "authorId": "144129720",
        "name": "Xiao Wang"
      },
      {
        "authorId": "2760554",
        "name": "Xiangzhou Huang"
      },
      {
        "authorId": "2152845491",
        "name": "Yanhua Yu"
      },
      {
        "authorId": "144911687",
        "name": "Haoyang Li"
      },
      {
        "authorId": "48985404",
        "name": "Mengdi Zhang"
      },
      {
        "authorId": "114361215",
        "name": "Zirui Guo"
      },
      {
        "authorId": "2118256000",
        "name": "Wei Wu"
      }
    ],
    "abstract": "Graph neural network (GNN) based recommender systems have become one of the mainstream trends due to the powerful learning ability from user behavior data. Understanding the user intents from behavior data is the key to recommender systems, which poses two basic requirements for GNN-based recommender systems. One is how to learn complex and diverse intents especially when the user behavior is usually inadequate in reality. The other is different behaviors have different intent distributions, so how to establish their relations for a more explainable recommender system. In this paper, we present the Intent-aware Recommendation via Disentangled Graph Contrastive Learning (IDCL), which simultaneously learns interpretable intents and behavior distributions over those intents. Specifically, we first model the user behavior data as a user-item-concept graph, and design a GNN based behavior disentangling module to learn the different intents. Then we propose the intent-wise contrastive learning to enhance the intent disentangling and meanwhile infer the behavior distributions. Finally, the coding rate reduction regularization is introduced to make the behaviors of different intents orthogonal. Extensive experiments demonstrate the effectiveness of IDCL in terms of substantial improvement and the interpretability.",
    "affiliations": [],
    "countries": [],
    "num_authors": 8
  },
  {
    "paperId": "303b260d5bef9b5c87c868110ec429fe5ea934ad",
    "externalIds": {
      "DBLP": "journals/corr/abs-1804-11192",
      "MAG": "3101366597",
      "ArXiv": "1804.11192",
      "DOI": "10.1561/1500000066",
      "CorpusId": 13752895
    },
    "url": "https://www.semanticscholar.org/paper/303b260d5bef9b5c87c868110ec429fe5ea934ad",
    "title": "Explainable Recommendation: A Survey and New Perspectives",
    "venue": "Foundations and Trends in Information Retrieval",
    "year": 2018,
    "referenceCount": 201,
    "citationCount": 939,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/1804.11192",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1804.11192, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2018-04-01",
    "authors": [
      {
        "authorId": "1739818",
        "name": "Yongfeng Zhang"
      },
      {
        "authorId": "49795005",
        "name": "Xu Chen"
      }
    ],
    "abstract": "Explainable recommendation attempts to develop models that generate not only high-quality recommendations but also intuitive explanations. The explanations may either be post-hoc or directly come from an explainable model (also called interpretable or transparent model in some contexts). Explainable recommendation tries to address the problem of why: by providing explanations to users or system designers, it helps humans to understand why certain items are recommended by the algorithm, where the human can either be users or system designers. Explainable recommendation helps to improve the transparency, persuasiveness, effectiveness, trustworthiness, and satisfaction of recommendation systems. It also facilitates system designers for better system debugging. In recent years, a large number of explainable recommendation approaches -- especially model-based methods -- have been proposed and applied in real-world systems. \nIn this survey, we provide a comprehensive review for the explainable recommendation research. We first highlight the position of explainable recommendation in recommender system research by categorizing recommendation problems into the 5W, i.e., what, when, who, where, and why. We then conduct a comprehensive survey of explainable recommendation on three perspectives: 1) We provide a chronological research timeline of explainable recommendation. 2) We provide a two-dimensional taxonomy to classify existing explainable recommendation research. 3) We summarize how explainable recommendation applies to different recommendation tasks. We also devote a chapter to discuss the explanation perspectives in broader IR and AI/ML research. We end the survey by discussing potential future directions to promote the explainable recommendation research area and beyond.",
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "9a14989424b16a4685c43ffc8057b40157631dd2",
    "externalIds": {
      "DBLP": "conf/sigir/XianFMMZ19",
      "MAG": "3099726771",
      "ArXiv": "1906.05237",
      "DOI": "10.1145/3331184.3331203",
      "CorpusId": 186206810
    },
    "url": "https://www.semanticscholar.org/paper/9a14989424b16a4685c43ffc8057b40157631dd2",
    "title": "Reinforcement Knowledge Graph Reasoning for Explainable Recommendation",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2019,
    "referenceCount": 39,
    "citationCount": 494,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/1906.05237",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.05237, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2019-06-12",
    "authors": [
      {
        "authorId": "2885287",
        "name": "Yikun Xian"
      },
      {
        "authorId": "2011378",
        "name": "Zuohui Fu"
      },
      {
        "authorId": "144963537",
        "name": "S. Muthukrishnan"
      },
      {
        "authorId": "144608002",
        "name": "Gerard de Melo"
      },
      {
        "authorId": "1739818",
        "name": "Yongfeng Zhang"
      }
    ],
    "abstract": "Recent advances in personalized recommendation have sparked great interest in the exploitation of rich structured information provided by knowledge graphs. Unlike most existing approaches that only focus on leveraging knowledge graphs for more accurate recommendation, we aim to conduct explicit reasoning with knowledge for decision making so that the recommendations are generated and supported by an interpretable causal inference procedure. To this end, we propose a method called Policy-Guided Path Reasoning (PGPR), which couples recommendation and interpretability by providing actual paths in a knowledge graph. Our contributions include four aspects. We first highlight the significance of incorporating knowledge graphs into recommendation to formally define and interpret the reasoning process. Second, we propose a reinforcement learning (RL) approach featured by an innovative soft reward strategy, user-conditional action pruning and a multi-hop scoring function. Third, we design a policy-guided graph search algorithm to efficiently and effectively sample reasoning paths for recommendation. Finally, we extensively evaluate our method on several large-scale real-world benchmark datasets, obtaining favorable results compared with state-of-the-art methods.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "ea76dce92072b9c84d22d94460e91e8d14b4b72c",
    "externalIds": {
      "DBLP": "journals/tois/XuZTFZA24",
      "DOI": "10.1145/3605357",
      "CorpusId": 259185006
    },
    "url": "https://www.semanticscholar.org/paper/ea76dce92072b9c84d22d94460e91e8d14b4b72c",
    "title": "A Reusable Model-agnostic Framework for Faithfully Explainable Recommendation and System Scrutability",
    "venue": "ACM Trans. Inf. Syst.",
    "year": 2023,
    "referenceCount": 98,
    "citationCount": 14,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3605357",
      "status": "HYBRID",
      "license": "other-oa",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3605357?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3605357, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-18",
    "authors": [
      {
        "authorId": "2284763197",
        "name": "Zhichao Xu"
      },
      {
        "authorId": "2029235362",
        "name": "Hansi Zeng"
      },
      {
        "authorId": "2110449137",
        "name": "Juntao Tan"
      },
      {
        "authorId": "2011378",
        "name": "Zuohui Fu"
      },
      {
        "authorId": "1591136873",
        "name": "Yongfeng Zhang"
      },
      {
        "authorId": "144922928",
        "name": "Qingyao Ai"
      }
    ],
    "abstract": "State-of-the-art industrial-level recommender system applications mostly adopt complicated model structures such as deep neural networks. While this helps with the model performance, the lack of system explainability caused by these nearly blackbox models also raises concerns and potentially weakens the users’ trust in the system. Existing work on explainable recommendation mostly focuses on designing interpretable model structures to generate model-intrinsic explanations. However, most of them have complex structures, and it is difficult to directly apply these designs onto existing recommendation applications due to the effectiveness and efficiency concerns. However, while there have been some studies on explaining recommendation models without knowing their internal structures (i.e., model-agnostic explanations), these methods have been criticized for not reflecting the actual reasoning process of the recommendation model or, in other words, faithfulness. How to develop model-agnostic explanation methods and evaluate them in terms of faithfulness is mostly unknown. In this work, we propose a reusable evaluation pipeline for model-agnostic explainable recommendation. Our pipeline evaluates the quality of model-agnostic explanation from the perspectives of faithfulness and scrutability. We further propose a model-agnostic explanation framework for recommendation and verify it with the proposed evaluation pipeline. Extensive experiments on public datasets demonstrate that our model-agnostic framework is able to generate explanations that are faithful to the recommendation model. We additionally provide quantitative and qualitative study to show that our explanation framework could enhance the scrutability of blackbox recommendation model. With proper modification, our evaluation pipeline and model-agnostic explanation framework could be easily migrated to existing applications. Through this work, we hope to encourage the community to focus more on faithfulness evaluation of explainable recommender systems.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "ab90bbf4de9411383e0c05b7e1066d928d65078c",
    "externalIds": {
      "DBLP": "conf/www/LiZLCRKL25",
      "ArXiv": "2502.12586",
      "DOI": "10.1145/3696410.3714727",
      "CorpusId": 276421643
    },
    "url": "https://www.semanticscholar.org/paper/ab90bbf4de9411383e0c05b7e1066d928d65078c",
    "title": "G-Refer: Graph Retrieval-Augmented Large Language Model for Explainable Recommendation",
    "venue": "The Web Conference",
    "year": 2025,
    "referenceCount": 91,
    "citationCount": 23,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2025-02-18",
    "authors": [
      {
        "authorId": "2267495002",
        "name": "Yuhan Li"
      },
      {
        "authorId": "2293442048",
        "name": "Xinni Zhang"
      },
      {
        "authorId": "2346640540",
        "name": "Linhao Luo"
      },
      {
        "authorId": "2303979712",
        "name": "Heng Chang"
      },
      {
        "authorId": "2345994084",
        "name": "Yuxiang Ren"
      },
      {
        "authorId": "2052565939",
        "name": "Irwin King"
      },
      {
        "authorId": "2273828439",
        "name": "Jia Li"
      }
    ],
    "abstract": "Explainable recommendation has demonstrated significant advantages in informing users about the logic behind recommendations, thereby increasing system transparency, effectiveness, and trustworthiness. To provide personalized and interpretable explanations, existing works often combine the generation capabilities of large language models (LLMs) with collaborative filtering (CF) information. CF information extracted from the user-item interaction graph captures the user behaviors and preferences, which is crucial for providing informative explanations. However, due to the complexity of graph structure, effectively extracting the CF information from graphs still remains a challenge. Moreover, existing methods often struggle with the integration of extracted CF information with LLMs due to its implicit representation and the modality gap between graph structures and natural language explanations. To address these challenges, we propose G-Refer, a framework using Graph Retrieval-augmented large language models (LLMs) for explainable recommendation. Specifically, we first employ a hybrid graph retrieval mechanism to retrieve explicit CF signals from both structural and semantic perspectives. The retrieved CF information is explicitly formulated as human-understandable text by the proposed graph translation and accounts for the explanations generated by LLMs. To bridge the modality gap, we introduce knowledge pruning and retrieval-augmented fine-tuning to enhance the ability of LLMs to process and utilize the retrieved CF information to generate explanations. Extensive experiments show that G-Refer achieves superior performance compared with existing methods in both explainability and stability. Codes and data are available at https://github.com/Yuhan1i/G-Refer.",
    "affiliations": [],
    "countries": [],
    "num_authors": 7
  },
  {
    "paperId": "18dfbd575bbedbf900b519d6620fdd983c3fb1bb",
    "externalIds": {
      "DBLP": "journals/corr/abs-2204-06818",
      "ArXiv": "2204.06818",
      "DOI": "10.48550/arXiv.2204.06818",
      "CorpusId": 248177788,
      "PubMed": "36279768"
    },
    "url": "https://www.semanticscholar.org/paper/18dfbd575bbedbf900b519d6620fdd983c3fb1bb",
    "title": "Interpretable Vertebral Fracture Quantification via Anchor-Free Landmarks Localization",
    "venue": "Medical Image Anal.",
    "year": 2022,
    "referenceCount": 46,
    "citationCount": 26,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2204.06818",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.06818, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-04-14",
    "authors": [
      {
        "authorId": "2055969909",
        "name": "A. Zakharov"
      },
      {
        "authorId": "35492383",
        "name": "M. Pisov"
      },
      {
        "authorId": "2162471099",
        "name": "Alim Bukharaev"
      },
      {
        "authorId": "2272819001",
        "name": "Alexey Petraikin"
      },
      {
        "authorId": "144890864",
        "name": "S. Morozov"
      },
      {
        "authorId": "40900270",
        "name": "V. Gombolevskiy"
      },
      {
        "authorId": "143677768",
        "name": "M. Belyaev"
      }
    ],
    "abstract": "Vertebral body compression fractures are early signs of osteoporosis. Though these fractures are visible on Computed Tomography (CT) images, they are frequently missed by radiologists in clinical settings. Prior research on automatic methods of vertebral fracture classification proves its reliable quality; however, existing methods provide hard-to-interpret outputs and sometimes fail to process cases with severe abnormalities such as highly pathological vertebrae or scoliosis. We propose a new two-step algorithm to localize the vertebral column in 3D CT images and then detect individual vertebrae and quantify fractures in 2D simultaneously. We train neural networks for both steps using a simple 6-keypoints based annotation scheme, which corresponds precisely to the current clinical recommendation. Our algorithm has no exclusion criteria, processes 3D CT in 2 seconds on a single GPU, and provides an interpretable and verifiable output. The method approaches expert-level performance and demonstrates state-of-the-art results in vertebrae 3D localization (the average error is 1mm), vertebrae 2D detection (precision and recall are 0.99), and fracture identification (ROC AUC at the patient level is up to 0.96). Our anchor-free vertebra detection network shows excellent generalizability on a new domain by achieving ROC AUC 0.95, sensitivity 0.85, specificity 0.9 on a challenging VerSe dataset with many unseen vertebra types.",
    "affiliations": [],
    "countries": [],
    "num_authors": 7
  },
  {
    "paperId": "5aed438fecf702ead9c0cddfa9a80bd00cff0138",
    "externalIds": {
      "DBLP": "conf/recsys/WuLHS20",
      "MAG": "3005071803",
      "DOI": "10.1145/3383313.3412258",
      "CorpusId": 211550824
    },
    "url": "https://www.semanticscholar.org/paper/5aed438fecf702ead9c0cddfa9a80bd00cff0138",
    "title": "SSE-PT: Sequential Recommendation Via Personalized Transformer",
    "venue": "ACM Conference on Recommender Systems",
    "year": 2020,
    "referenceCount": 40,
    "citationCount": 256,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3383313.3412258",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3383313.3412258?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3383313.3412258, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2020-09-22",
    "authors": [
      {
        "authorId": "2326354223",
        "name": "Liwei Wu"
      },
      {
        "authorId": "48831710",
        "name": "Shuqing Li"
      },
      {
        "authorId": "1793529",
        "name": "Cho-Jui Hsieh"
      },
      {
        "authorId": "1745601",
        "name": "J. Sharpnack"
      }
    ],
    "abstract": "Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users’ engagement history, we find our model not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Our novel application of the Stochastic Shared Embeddings (SSE) regularization is essential to the success of personalization. Code and data are open-sourced at https://github.com/wuliwei9278/SSE-PT.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "7cd1096132a481bbfed2eba783d2550a78c1fcc4",
    "externalIds": {
      "DBLP": "journals/tomccap/DivitiisBBB23",
      "DOI": "10.1145/3531017",
      "CorpusId": 248243754
    },
    "url": "https://www.semanticscholar.org/paper/7cd1096132a481bbfed2eba783d2550a78c1fcc4",
    "title": "Disentangling Features for Fashion Recommendation",
    "venue": "ACM Trans. Multim. Comput. Commun. Appl.",
    "year": 2022,
    "referenceCount": 40,
    "citationCount": 44,
    "openAccessPdf": {
      "url": "https://flore.unifi.it/bitstream/2158/1283285/1/3531017.pdf",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3531017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3531017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-04-20",
    "authors": [
      {
        "authorId": "2115010832",
        "name": "Lavinia De Divitiis"
      },
      {
        "authorId": "41172759",
        "name": "Federico Becattini"
      },
      {
        "authorId": "2407432",
        "name": "C. Baecchi"
      },
      {
        "authorId": "8196487",
        "name": "A. Bimbo"
      }
    ],
    "abstract": "Online stores have become fundamental for the fashion industry, revolving around recommendation systems to suggest appropriate items to customers. Such recommendations often suffer from a lack of diversity and propose items that are similar to previous purchases of a user. Recently, a novel kind of approach based on Memory Augmented Neural Networks (MANNs) has been proposed, aimed at recommending a variety of garments to create an outfit by complementing a given fashion item. In this article we address the task of compatible garment recommendation developing a MANN architecture by taking into account the co-occurrence of clothing attributes, such as shape and color, to compose an outfit. To this end we obtain disentangled representations of fashion items and store them in external memory modules, used to guide recommendations at inference time. We show that our disentangled representations are able to achieve significantly better performance compared to the state of the art and also provide interpretable latent spaces, giving a qualitative explanation of the recommendations.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "9bc518745f3e4609bf74d6af53c44814e973c2b0",
    "externalIds": {
      "DBLP": "journals/ijswis/LiLWZJT22",
      "DOI": "10.4018/ijswis.297146",
      "CorpusId": 246932583
    },
    "url": "https://www.semanticscholar.org/paper/9bc518745f3e4609bf74d6af53c44814e973c2b0",
    "title": "Scholar Recommendation Based on High-Order Propagation of Knowledge Graphs",
    "venue": "International Journal on Semantic Web and Information Systems (IJSWIS)",
    "year": 2022,
    "referenceCount": 35,
    "citationCount": 17,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.4018/ijswis.297146?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.4018/ijswis.297146, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-01-01",
    "authors": [
      {
        "authorId": "2116221833",
        "name": "Pu Li"
      },
      {
        "authorId": "2118910019",
        "name": "Tianci Li"
      },
      {
        "authorId": "2153688945",
        "name": "Xin Wang"
      },
      {
        "authorId": "50202832",
        "name": "Suzhi Zhang"
      },
      {
        "authorId": "1679173",
        "name": "Yuncheng Jiang"
      },
      {
        "authorId": "5736406",
        "name": "Yong Tang"
      }
    ],
    "abstract": "In a big data environment, traditional recommendation methods have limitations such as data sparseness and cold start, etc. In view of the rich semantics, excellent quality, and good structure of knowledge graphs, many researchers have introduced knowledge graphs into the research about recommendation systems, and studied interpretable recommendations based on knowledge graphs. Along this line, this paper proposes a scholar recommendation method based on the high-order propagation of knowledge graph (HoPKG), which analyzes the high-order semantic information in the knowledge graph, and generates richer entity representations to obtain users’ potential interest by distinguishing the importance of different entities. On this basis, a dual aggregation method of high-order propagation is proposed to enable entity information to be propagated more effectively. Through experimental analysis, compared with some baselines, such as Ripplenet, RKGE and CKE, our method has certain advantages in the evaluation indicators AUC and F1.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "64980e48aa8079d3d26208a50f5680d0e7700fac",
    "externalIds": {
      "DBLP": "journals/apin/ZhangYZCL22",
      "DOI": "10.1007/s10489-022-03359-w",
      "CorpusId": 248018903
    },
    "url": "https://www.semanticscholar.org/paper/64980e48aa8079d3d26208a50f5680d0e7700fac",
    "title": "Aggregating knowledge-aware graph neural network and adaptive relational attention for recommendation",
    "venue": "Applied intelligence (Boston)",
    "year": 2022,
    "referenceCount": 40,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10489-022-03359-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10489-022-03359-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-04-07",
    "authors": [
      {
        "authorId": "2108058987",
        "name": "Yihao Zhang"
      },
      {
        "authorId": "73346815",
        "name": "Mengze Yuan"
      },
      {
        "authorId": "2047930719",
        "name": "Chu Zhao"
      },
      {
        "authorId": "2047905659",
        "name": "Mian Chen"
      },
      {
        "authorId": "2142863049",
        "name": "Xiaoyang Liu"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "8da96c82c9618782fe427b466120ec2c12ef1f93",
    "externalIds": {
      "MAG": "3031596603",
      "DBLP": "journals/tois/LiLWHJC21",
      "ArXiv": "2005.12979",
      "DOI": "10.1145/3446427",
      "CorpusId": 218900632
    },
    "url": "https://www.semanticscholar.org/paper/8da96c82c9618782fe427b466120ec2c12ef1f93",
    "title": "Seamlessly Unifying Attributes and Items: Conversational Recommendation for Cold-start Users",
    "venue": "ACM Trans. Inf. Syst.",
    "year": 2020,
    "referenceCount": 59,
    "citationCount": 127,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2005.12979",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.12979, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-05-23",
    "authors": [
      {
        "authorId": "2137366124",
        "name": "Shijun Li"
      },
      {
        "authorId": "39165620",
        "name": "Wenqiang Lei"
      },
      {
        "authorId": "1777934740",
        "name": "Qingyun Wu"
      },
      {
        "authorId": "7792071",
        "name": "Xiangnan He"
      },
      {
        "authorId": "2061280682",
        "name": "Peng Jiang"
      },
      {
        "authorId": "144078686",
        "name": "Tat-Seng Chua"
      }
    ],
    "abstract": "Static recommendation methods like collaborative filtering suffer from the inherent limitation of performing real-time personalization for cold-start users. Online recommendation, e.g., multi-armed bandit approach, addresses this limitation by interactively exploring user preference online and pursuing the exploration-exploitation (EE) trade-off. However, existing bandit-based methods model recommendation actions homogeneously. Specifically, they only consider the items as the arms, being incapable of handling the item attributes, which naturally provide interpretable information of user’s current demands and can effectively filter out undesired items. In this work, we consider the conversational recommendation for cold-start users, where a system can both ask the attributes from and recommend items to a user interactively. This important scenario was studied in a recent work [54]. However, it employs a hand-crafted function to decide when to ask attributes or make recommendations. Such separate modeling of attributes and items makes the effectiveness of the system highly rely on the choice of the hand-crafted function, thus introducing fragility to the system. To address this limitation, we seamlessly unify attributes and items in the same arm space and achieve their EE trade-offs automatically using the framework of Thompson Sampling. Our Conversational Thompson Sampling (ConTS) model holistically solves all questions in conversational recommendation by choosing the arm with the maximal reward to play. Extensive experiments on three benchmark datasets show that ConTS outperforms the state-of-the-art methods Conversational UCB (ConUCB) [54] and Estimation—Action—Reflection model [27] in both metrics of success rate and average number of conversation turns.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "3986307b4a015ad4f6eac004d28e3bf4c77480db",
    "externalIds": {
      "ArXiv": "2412.16196",
      "DBLP": "journals/corr/abs-2412-16196",
      "DOI": "10.1109/BigData62323.2024.10825771",
      "CorpusId": 274982229
    },
    "url": "https://www.semanticscholar.org/paper/3986307b4a015ad4f6eac004d28e3bf4c77480db",
    "title": "AgroXAI: Explainable AI-Driven Crop Recommendation System for Agriculture 4.0",
    "venue": "BigData Congress [Services Society]",
    "year": 2024,
    "referenceCount": 29,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2412.16196",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.16196, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-12-15",
    "authors": [
      {
        "authorId": "2233329512",
        "name": "Özlem Turgut"
      },
      {
        "authorId": "35305688",
        "name": "Ibrahim Kok"
      },
      {
        "authorId": "2056426673",
        "name": "Suat Özdemi̇r"
      }
    ],
    "abstract": "Today, crop diversification in agriculture is a critical issue to meet the increasing demand for food and to improve food safety and quality. This issue is considered to be the most important challenge for the next generation of agriculture due to diminishing natural resources, limited arable land and unpredictable climatic conditions caused by climate change. In this paper, we employ emerging technologies such as the Internet of Things (IoT), machine learning (ML) and explainable artificial intelligence (XAI) to improve operational efficiency and productivity in the agricultural sector. Specifically, we propose an edge computing-based explainable crop recommendation system, AgroXAI, which suggests suitable crops for a region based on weather and soil conditions. In this system, we provide local and global explanations of ML model decisions with methods such as ELI5, LIME, SHAP, which we integrate into ML models. More importantly, we provide regional alternative crop recommendations with the Counterfactual explainability method. In this way, we envision that our proposed AgroXAI system will be a platform that provides regional crop diversity in the next generation agriculture.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "596be14ea711f225964cf760d3f7453be433c057",
    "externalIds": {
      "DOI": "10.1109/SPARC61891.2024.10829064",
      "CorpusId": 275437972
    },
    "url": "https://www.semanticscholar.org/paper/596be14ea711f225964cf760d3f7453be433c057",
    "title": "Enhancing Agricultural Decision-Making through an Explainable AI-Based Crop Recommendation System",
    "venue": "2024 International Conference on Signal Processing and Advance Research in Computing (SPARC)",
    "year": 2024,
    "referenceCount": 19,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/SPARC61891.2024.10829064?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SPARC61891.2024.10829064, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2024-09-12",
    "authors": [
      {
        "authorId": "2310318563",
        "name": "Surendra Kumar"
      },
      {
        "authorId": "2249066577",
        "name": "Mohit Kumar"
      }
    ],
    "abstract": "Agriculture is essential for maintaining food security and promoting economic sustainability; however, farmers frequently encounter difficulties in choosing the most appropriate crops for their land due to diverse environmental, soil, and market conditions. This paper proposes an innovative crop recommendation system powered by Explainable Artificial Intelligence (XAI), designed to enhance agricultural decision-making. The system utilizes machine-learning models to analyze diverse data inputs—such as soil properties, weather conditions, and market trends—to recommend optimal crops for specific regions. What sets this approach apart is its explainability: the XAI framework provides transparent insights into how and why certain recommendations are made, enabling farmers to trust and understand the decision-making process. The incorporation of eXplainable AI into crop recommendation systems has transformed agriculture, facilitating data-driven decision-making that leads to improved crop production and more efficient resource management. However, these models’ lack of transparency and interpretability frequently constrains their practical implementation. We explore XAI based approaches, such as LIME and SHAP, to interpret model outputs and highlight key features influencing predictions. Our experiments demonstrate that XAI improves the transparency of ML models and aids in refining model performance through informed feature selection and model adjustments. Our proposed models achieve the highest accuracy $\\mathbf{9 9. 3 9 \\%}$ using the KNN algorithm and BiLSTM model achieve the $\\mathbf{9 5. 9 1 \\%}$ accuracy.",
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "938a96df8752749057746741fc7f40986d275bf8",
    "externalIds": {
      "DOI": "10.1109/ICCIT64611.2024.11021774",
      "CorpusId": 279268815
    },
    "url": "https://www.semanticscholar.org/paper/938a96df8752749057746741fc7f40986d275bf8",
    "title": "Smart Crop Recommendation System Using Machine Learning and Explainable AI",
    "venue": "2024 27th International Conference on Computer and Information Technology (ICCIT)",
    "year": 2024,
    "referenceCount": 20,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCIT64611.2024.11021774?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCIT64611.2024.11021774, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2024-12-20",
    "authors": [
      {
        "authorId": "2366236444",
        "name": "Syed Imran Ertaza"
      },
      {
        "authorId": "2366236300",
        "name": "SK Alamgir Hossain"
      }
    ],
    "abstract": "In many countries of the world, especially in South Asian countries, most of the people's livelihood and food habits are based on agriculture, so it is very important to produce different types of crops profitably and in large quantities. However many farmers are still using primitive methods of cropping and do not understand exactly which crop is best for which land at which time due to climate variability, soil degradation, and environmental instability. The AI-based automated system is a crying need for the farmers for this problem. This study proposes a smart crop recommendation model using Machine Learning (ML) and Explainable AI to help farmers make data-driven decisions on what crops are ideal to grow. This investigation utilizes a standard crop recommendation dataset from Kaggle, which includes key soil characteristics such as nitrogen, phosphorus, potassium, and pH level; and climate parameters including temperature, humidity, and rainfall. The proposed work makes the best use of several machine learning algorithms including Support Vector Machine, Decision Tree, Random Forest, Gaussian Naive Bayes, and a boosting ensemble classifier called Adaptive Boosting (AdaBoost). Among these models, AdaBoost reveals the best performance of 99.35% with a Decision Tree oriented base learner ID3. In addition, the SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME) from explainable AI are used to detect the responsible soil and climate features for the classification of the AdaBoost ensemble model.",
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "7a5f710d7ace5aaa60fd92dceab13875a0f83d81",
    "externalIds": {
      "DOI": "10.1109/EECSI63442.2024.10776491",
      "CorpusId": 274705665
    },
    "url": "https://www.semanticscholar.org/paper/7a5f710d7ace5aaa60fd92dceab13875a0f83d81",
    "title": "Mooc Course Recommendation System Model with Explainable AI (XAI) Using Content Based Filtering Method",
    "venue": "2024 11th International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)",
    "year": 2024,
    "referenceCount": 6,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EECSI63442.2024.10776491?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EECSI63442.2024.10776491, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2024-09-26",
    "authors": [
      {
        "authorId": "116143268",
        "name": "Mugi Praseptiawan"
      },
      {
        "authorId": "2335126297",
        "name": "M. F. D. Muchtarom"
      },
      {
        "authorId": "2335584303",
        "name": "Nabila Muthia Putri"
      },
      {
        "authorId": "3161326",
        "name": "A. N. C. Pee"
      },
      {
        "authorId": "2241629440",
        "name": "Mohd Hafiz Zakaria"
      },
      {
        "authorId": "32471431",
        "name": "M. C. Untoro"
      }
    ],
    "abstract": "Massive Open Online Course (MOOC) is a type of online course that has been designed and can be accessed by all individuals via the internet. The problem that is often found in MOOCs is the lack of a recommendation system provided by the algorithm of the MOOC. This research is conducted to analyze a recommendation system that applies the Content Based Filtering approach in order to solve the problems that occur. The recommendation system analyzed will function as a media that provides recommendations to users based on their preferences. By utilizing content-based methods, the recommendations given are expected to be exactly what the user wants. The level of explainability of the recommendation system is further emphasized by XAI using ELI5. By getting a concise explanation when a recommendation is given, the system will gain more trust from users for providing an appropriate recommendation. The assessment of the accuracy of the recommendation system model is measured using MAE. By researching this recommendation system using XAI, it is hoped that it can help future systems to improve the quality of the course recommendation system.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "6f82c257989e664e38cbf8a13b7fe6ab5098bbcc",
    "externalIds": {
      "DOI": "10.1109/ISITDI62380.2024.10797073",
      "CorpusId": 274896167
    },
    "url": "https://www.semanticscholar.org/paper/6f82c257989e664e38cbf8a13b7fe6ab5098bbcc",
    "title": "Application of Collaborative Filtering and Explainable AI Methods in Recommendation System Modeling to Predict MOOC Course Preferences",
    "venue": "2024 2nd International Symposium on Information Technology and Digital Innovation (ISITDI)",
    "year": 2024,
    "referenceCount": 16,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISITDI62380.2024.10797073?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISITDI62380.2024.10797073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": null,
    "publicationDate": "2024-07-24",
    "authors": [
      {
        "authorId": "116143268",
        "name": "Mugi Praseptiawan"
      },
      {
        "authorId": "2335584303",
        "name": "Nabila Muthia Putri"
      },
      {
        "authorId": "2336494653",
        "name": "M. Fikri"
      },
      {
        "authorId": "2336508797",
        "name": "Damar Muchtarom"
      },
      {
        "authorId": "2241629440",
        "name": "Mohd Hafiz Zakaria"
      },
      {
        "authorId": "2288355850",
        "name": "Ahmad Naim"
      },
      {
        "authorId": "2336504878",
        "name": "Che Pee"
      }
    ],
    "abstract": "Unquestionably, the creation of MOOCs has transformed education by making learning accessible and inexpensive for people all over the world. However, consumers frequently find it difficult to select a course that fits their needs and interests due to the vast curriculum that MOOCs offer. This paper describes how a recommendation system that applies the Collaborative Filtering approach can be used to address these kinds of problems. This paper proposes a recommendation system modeling for MOOC platforms that allows users to be recommended courses based on their preferences. Using user interactions and choices, the Collaborative Filtering approach suggests courses that are specifically tailored to everyone. Utilizing a user-based method called collaborative filtering, users' preferences are predicted by comparing them to other users. The recommendation system's transparency and interpretability are improved by the incorporation of XAI using LIME. By gaining understanding of the reasoning behind course recommendations, users can build trust and make well-informed decisions. A quantitative assessment of the prediction accuracy is provided by the recommendation system's performance evaluation utilizing the RMSE measure. Over time, this statistic aids in system improvement and raises the caliber of course recommendations.",
    "affiliations": [],
    "countries": [],
    "num_authors": 7
  },
  {
    "paperId": "ea21bfa00ed7718a774334e33edd9f5187e09e5f",
    "externalIds": {
      "DBLP": "conf/edbt/VecchiaOQ24",
      "CorpusId": 268876761
    },
    "url": "https://www.semanticscholar.org/paper/ea21bfa00ed7718a774334e33edd9f5187e09e5f",
    "title": "ICARE: the principles of Explainable AI in a Context-aware Recommendation APP",
    "venue": "EDBT/ICDT Workshops",
    "year": 2024,
    "referenceCount": 0,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2183783740",
        "name": "Anna Dalla Vecchia"
      },
      {
        "authorId": "1728772",
        "name": "Barbara Oliboni"
      },
      {
        "authorId": "2294723982",
        "name": "Elisa Quintarelli"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "15fb60600202fe4a9baf1f6f6dfbdee373e30fe6",
    "externalIds": {
      "DBLP": "conf/mai-xai/Breuer24",
      "CorpusId": 273901860
    },
    "url": "https://www.semanticscholar.org/paper/15fb60600202fe4a9baf1f6f6dfbdee373e30fe6",
    "title": "Towards Automated Human-Centered Recommendation of Explainable AI Solutions",
    "venue": "MAI-XAI@ECAI",
    "year": 2024,
    "referenceCount": 0,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2329741857",
        "name": "Nils Ole Breuer"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 1
  },
  {
    "paperId": "edd867ee721a0e91cc0311235b29a295ebb256b2",
    "externalIds": {
      "DBLP": "journals/bspc/XiWYZH23",
      "DOI": "10.1016/j.bspc.2022.104144",
      "CorpusId": 252614822
    },
    "url": "https://www.semanticscholar.org/paper/edd867ee721a0e91cc0311235b29a295ebb256b2",
    "title": "Cancer omic data based explainable AI drug recommendation inference: A traceability perspective for explainability",
    "venue": "Biomedical Signal Processing and Control",
    "year": 2023,
    "referenceCount": 61,
    "citationCount": 21,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.bspc.2022.104144?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.bspc.2022.104144, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-01-01",
    "authors": [
      {
        "authorId": "3442932",
        "name": "Jianing Xi"
      },
      {
        "authorId": "2155683568",
        "name": "Daniel X M Wang"
      },
      {
        "authorId": "1860612",
        "name": "Xuebing Yang"
      },
      {
        "authorId": "2108167911",
        "name": "Wensheng Zhang"
      },
      {
        "authorId": "2148946979",
        "name": "Qinghua Huang"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "5ef74f7db4d5643ea87e931a6b4d00afb3c26d5e",
    "externalIds": {
      "DOI": "10.5815/ijisa.2025.01.03",
      "CorpusId": 276143230
    },
    "url": "https://www.semanticscholar.org/paper/5ef74f7db4d5643ea87e931a6b4d00afb3c26d5e",
    "title": "Role of Explainable AI in Crop Recommendation Technique of Smart Farming",
    "venue": "International Journal of Intelligent Systems and Applications",
    "year": 2025,
    "referenceCount": 56,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5815/ijisa.2025.01.03?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5815/ijisa.2025.01.03, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-02-08",
    "authors": [
      {
        "authorId": "2203839392",
        "name": "Yaganteeswarudu Akkem"
      },
      {
        "authorId": "2291195807",
        "name": "S. K. Biswas"
      },
      {
        "authorId": "2280233508",
        "name": "Aruna Varanasi"
      }
    ],
    "abstract": "Smart farming is undergoing a transformation with the integration of machine learning (ML) and artificial intelligence (AI) to improve crop recommendations. Despite the advancements, a critical gap exists in opaque ML models that need to explain their predictions, leading to a trust deficit among farmers. This research addresses the gap by implementing explainable AI (XAI) techniques, specifically focusing on the crop recommendation technique in smart farming.\nAn experiment was conducted using a Crop recommendation dataset, applying XAI algorithms such as Local Interpretable Model-agnostic Explanations (LIME), Differentiable InterCounterfactual Explanations (dice_ml), and SHapley Additive exPlanations (SHAP). These algorithms were used to generate local and counterfactual explanations, enhancing model transparency in compliance with the General Data Protection Regulation (GDPR), which mandates the right to explanation.\nThe results demonstrated the effectiveness of XAI in making ML models more interpretable and trustworthy. For instance, local explanations from LIME provided insights into individual predictions, while counterfactual scenarios from dice_ml offered alternative crop cultivation suggestions. Feature importance from SHAP gave a global perspective on the factors influencing the model's decisions. The study's statistical analysis revealed that the integration of XAI increased the farmers' understanding of the AI system's recommendations, potentially reducing food insufficiency by enabling the cultivation of alternative crops on the same land.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "d4dc21b3d90eb75474a3d19b123ca08e067484ec",
    "externalIds": {
      "DOI": "10.1109/ICDSAAI65575.2025.11011599",
      "CorpusId": 279002117
    },
    "url": "https://www.semanticscholar.org/paper/d4dc21b3d90eb75474a3d19b123ca08e067484ec",
    "title": "Personalized Cancer Drug Recommendation using GAN and Explainable AI",
    "venue": "2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)",
    "year": 2025,
    "referenceCount": 15,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDSAAI65575.2025.11011599?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDSAAI65575.2025.11011599, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2025-03-28",
    "authors": [
      {
        "authorId": "2364206831",
        "name": "A. Apoorva"
      },
      {
        "authorId": "2364207517",
        "name": "Farha Afreen I"
      },
      {
        "authorId": "2364195216",
        "name": "E. Suganya"
      }
    ],
    "abstract": "Through more accurate medication recommendations and increased treatment efficacy, machine learning has improved personalized cancer therapy. To support the treatment recommendation system for lung cancer, the study uses a Conditional Tabular Generative Adversarial Network (CTGAN) to generate synthetic patient data. Machine learning algorithms process data to recommend appropriate medications while LightGBM demonstrates superior performance to XGBoost by achieving 0.82 accuracy. Explainable AI (XAI) techniques produce greater transparency while supplying doctors with expanded treatment information. This approach aims to enhance patient treatment outcomes while establishing trust in AI-driven medical advice.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "bae1214526f2dc712546fa4b887683bc0baafa3f",
    "externalIds": {
      "DOI": "10.1109/RMKMATE64874.2025.11042366",
      "CorpusId": 279597392
    },
    "url": "https://www.semanticscholar.org/paper/bae1214526f2dc712546fa4b887683bc0baafa3f",
    "title": "An Explainable AI Framework for Diabetic Foot Ulcer Detection and Personalized Footwear Recommendation",
    "venue": "2025 2nd International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)",
    "year": 2025,
    "referenceCount": 14,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/RMKMATE64874.2025.11042366?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/RMKMATE64874.2025.11042366, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2025-05-07",
    "authors": [
      {
        "authorId": "2242945328",
        "name": "P. Velvizhy"
      },
      {
        "authorId": "2368460223",
        "name": "Pavishek Tv"
      },
      {
        "authorId": "2335125910",
        "name": "V. A"
      },
      {
        "authorId": "2368461174",
        "name": "Naveen Kumar T"
      },
      {
        "authorId": "2368450138",
        "name": "Sharath J"
      },
      {
        "authorId": "2350021677",
        "name": "Matan P"
      }
    ],
    "abstract": "Diabetic Foot Ulcers (DFUs) are a major complication of diabetes, often leading to infections, hospitalizations, and amputations if not detected early. This study proposes an integrated deep learning and IoT-based system for the early detection, classification, and prevention of DFUs. An EfficientNetB0 model is utilized to classify DFU images with high accuracy, while plantar foot type classification is performed using WaveNet, ResNet, and an Attention Mechanism to analyze plantar pressure data. For personalized footwear recommendation, a Decision Tree model is employed, enhanced with Explainable AI techniques (SHAP, LIME, and Grad-CAM) to provide transparency in decision-making. The system captures real-time foot health parameters using multiple IoT sensors (pressure, temperature, heart rate, gyroscope) embedded in smart footwear, enabling continuous monitoring and risk prediction. The developed models show promising results in improving early DFU diagnosis, personalized prevention strategies, and clinical decision support. Future enhancements aim to scale the solution with larger datasets, real-time cloud integration, and clinical validation. This research provides a novel, explainable, and wearable AI-driven framework for improving diabetic foot care outcomes.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "a5cfbc209e3387b5e5c9956f347f7d0cbcc2de42",
    "externalIds": {
      "DOI": "10.1109/ICAISS61471.2025.11041903",
      "CorpusId": 279595296
    },
    "url": "https://www.semanticscholar.org/paper/a5cfbc209e3387b5e5c9956f347f7d0cbcc2de42",
    "title": "A Data-Driven Crop Recommendation System with Explainable AI for Precision Agriculture",
    "venue": "2025 Third International Conference on Augmented Intelligence and Sustainable Systems (ICAISS)",
    "year": 2025,
    "referenceCount": 6,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICAISS61471.2025.11041903?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICAISS61471.2025.11041903, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2025-05-21",
    "authors": [
      {
        "authorId": "2368459566",
        "name": "Moduguri Karthik"
      },
      {
        "authorId": "2368458301",
        "name": "Ankenapalle Nandini"
      },
      {
        "authorId": "2368459363",
        "name": "Pochamreddy Venkata Vedha"
      },
      {
        "authorId": "2368457731",
        "name": "A. Raaga Latha"
      },
      {
        "authorId": "2368457901",
        "name": "Balaji K V"
      },
      {
        "authorId": "9425580",
        "name": "Akey Sungheetha"
      }
    ],
    "abstract": "In this paper, we presented a crop recommendation system for precision agriculture that is based on the machine learning models providing crop recommendations based on the environmental and soil context. Introducing Explainable AI (XAI) in the designed system using LIME, we hope to enhance the level of trust to its recommendations through the understanding of the process which goes into each of them by the farmers. This has been made possible by the system’s architecture that consists of backend in Flask and frontend in React. The system was subjected to a rigorous assessment of its reliability and efficiency once developed to obviate measurement errors and ensure its effectiveness and efficiency. Moreover, the feedback on the integration of XAI also reveals an enhancement of interpretability by demonstrating the explanations of such features as pH of the soil, temperature, and moisture of the soil to have a positive effect to the system. Besides improving the chances of accurate crop yields estimation, this approach enables an accurate assessment of farming inputs with regards to sustainability.",
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "3c97967c62cffe70d3880c03d05917596a7f59a1",
    "externalIds": {
      "DOI": "10.2174/0123520965401121250714075403",
      "CorpusId": 280480334
    },
    "url": "https://www.semanticscholar.org/paper/3c97967c62cffe70d3880c03d05917596a7f59a1",
    "title": "Interpretability-driven Course Recommendation: A Random Forest Approach with Explainable AI",
    "venue": "Recent Advances in Electrical &amp; Electronic Engineering (Formerly Recent Patents on Electrical &amp; Electronic Engineering)",
    "year": 2025,
    "referenceCount": 0,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2174/0123520965401121250714075403?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2174/0123520965401121250714075403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-07-28",
    "authors": [
      {
        "authorId": "117082887",
        "name": "Tasleem Nizam"
      },
      {
        "authorId": "2320928600",
        "name": "Sherin Zafar"
      },
      {
        "authorId": "2576047",
        "name": "S. Biswas"
      },
      {
        "authorId": "2320926588",
        "name": "Imran Hussain"
      }
    ],
    "abstract": "\n\nIn the current educational landscape, universities provide a vast array of\ndiverse and obscure courses. However, the primary challenge students face is decision overload,\nas the abundance of options can make it difficult to choose courses that align with their interests,\ncareer aspirations, and strengths. The development and application of proper course suggestion\nsystems might assist students in overcoming the challenges associated with choosing the right\ncourses. However, these recommendation systems still require improvement to address explainability\nissues, security issues, and cold start. Our analysis reveals that there is limited research addressing\nhow course recommendation systems can assist 12th-grade students in selecting courses\nfor higher education. Therefore, this study presents a novel personalized recommendation system,\nnamely “Interpretability-Driven Course Recommendation: A Random Forest Approach with Explainable\nAI”, that chooses the top-3 courses for students based on their intermediate class grades/-\nmarks. This research provides accurate and interpretable recommendations using the Random Forest\nalgorithm with Explainable AI that ensures transparency by explaining why a particular course\nis recommended.\n\n\n\nThe system uses a Random Forest classifier with SHAP (SHapley Additive exPlanations)\nto forecast the top-3 courses with the greatest expected scores for appropriateness. The system\nuses SHAP (SHapley Additive exPlanations) values to integrate Explainable AI (XAI) to guarantee\nopenness and trust. The proposed model solves the cold start problem and provides data security.\n\n\n\nIn terms of precision (0.90), recall (0.92), and F1-score (0.92), Random Forest fared better\nthan any other classifier. By combining predictions from several decision trees, its ensemble\nlearning technique increases stability, decreases overfitting, and improves generalization across a\nbroad range of input data patterns.\n\n\n\nA major drawback of conventional black-box machine learning models is their lack\nof transparency, which is addressed by the incorporation of SHAP into the course recommendation\nsystem. In this model, students get suggestions in addition to information on which topic\nscores had the biggest influence on their choice.\n\n\n\nThis study presents a practical and interpretable course recommendation system designed\nfor 12th-grade students transitioning into higher education. Through the use of Explainable\nAI-enhanced Random Forest, the model provides precise, transparent, and customized recommendations.\nIt addresses the main drawbacks of conventional systems, such as their lack of explainability\nand their cold start problems. The model's excellent precision, recall, and F1-score indicate its\nsuperior performance, which makes it a useful tool for student guidance and academic advice. For\neven more thorough recommendations, future research may consider combining long-term academic\nobjectives, aptitude test results, and student interests.\n",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "1f79e0f9b805dd312ba02326eea88ce3490685a9",
    "externalIds": {
      "DOI": "10.17148/ijarcce.2025.14385",
      "CorpusId": 277639920
    },
    "url": "https://www.semanticscholar.org/paper/1f79e0f9b805dd312ba02326eea88ce3490685a9",
    "title": "Sentiment-Aware and Explainable AI-Based Cross-Domain Recommendation System",
    "venue": "IJARCCE",
    "year": 2025,
    "referenceCount": 0,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.17148/ijarcce.2025.14385?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.17148/ijarcce.2025.14385, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-04-07",
    "authors": [],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 0
  },
  {
    "paperId": "d206a64c96e72f06c7178b029e4c45febfaaf548",
    "externalIds": {
      "DBLP": "conf/edm/LiLQK25",
      "CorpusId": 281306744
    },
    "url": "https://www.semanticscholar.org/paper/d206a64c96e72f06c7178b029e4c45febfaaf548",
    "title": "Improving Course Recommendation Systems with Explainable AI: LLM-Based Frameworks and Evaluations",
    "venue": "Educational Data Mining",
    "year": 2025,
    "referenceCount": 0,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2380412991",
        "name": "Jiawei Li"
      },
      {
        "authorId": "2380388771",
        "name": "Qianru Lyu"
      },
      {
        "authorId": "2260654913",
        "name": "Wei Qiu"
      },
      {
        "authorId": "2192484611",
        "name": "Andy W. H. Khong"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "a617f4d620192b8c1ae64876f6c1a8d2fffdf1f4",
    "externalIds": {
      "DOI": "10.1080/00103624.2025.2537411",
      "CorpusId": 280609546
    },
    "url": "https://www.semanticscholar.org/paper/a617f4d620192b8c1ae64876f6c1a8d2fffdf1f4",
    "title": "Soil-Specific Crop Recommendation with Explainable AI: The XGB-FuzzyIncNet Approach",
    "venue": "Communications in Soil Science and Plant Analysis",
    "year": 2025,
    "referenceCount": 15,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/00103624.2025.2537411?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/00103624.2025.2537411, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-08-05",
    "authors": [
      {
        "authorId": "2375604029",
        "name": "S. Shenbagavadivu"
      },
      {
        "authorId": "2375606467",
        "name": "M. Senthil Kumar"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 2
  },
  {
    "paperId": "567816e81dc457e982147f2ca44b771070e1380c",
    "externalIds": {
      "DBLP": "conf/ssci/ArvindTJSJAKFSA21",
      "DOI": "10.1109/SSCI50451.2021.9659869",
      "CorpusId": 246291145
    },
    "url": "https://www.semanticscholar.org/paper/567816e81dc457e982147f2ca44b771070e1380c",
    "title": "Deep Learning Based Plant Disease Classification With Explainable AI and Mitigation Recommendation",
    "venue": "IEEE Symposium Series on Computational Intelligence",
    "year": 2021,
    "referenceCount": 0,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/SSCI50451.2021.9659869?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SSCI50451.2021.9659869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-12-05",
    "authors": [
      {
        "authorId": "2285338084",
        "name": "C. Arvind"
      },
      {
        "authorId": "2044359387",
        "name": "Aditi Totla"
      },
      {
        "authorId": "153116789",
        "name": "Tanisha Jain"
      },
      {
        "authorId": "144122679",
        "name": "Nandini Sinha"
      },
      {
        "authorId": "47686112",
        "name": "R. Jyothi"
      },
      {
        "authorId": "2151210320",
        "name": "K. Aditya"
      },
      {
        "authorId": "2151209363",
        "name": "Keerthan"
      },
      {
        "authorId": "2341490993",
        "name": "Mohammed Farhan"
      },
      {
        "authorId": "2151210243",
        "name": "G. Sumukh"
      },
      {
        "authorId": "2151208174",
        "name": "Guruprasad Ak"
      }
    ],
    "abstract": "Plants show visible symptoms of getting infected with a disease. Presently an experienced plant pathologist can diagnose the condition through visual inspection of disease-affected plants. However, manual visualization is time-consuming and depends on the plant pathologist's expertise in identifying plant disease. Hence this problem can be solved by a computer-aided diagnostic system with artificial intelligence (CADS-AI). This system will aid in improving and protecting the yield of the plant, but it lacks trust as the existing system is not flawless. Hence, in this research work, a plant disease classification with an explainable AI pipeline is developed which ensures trust in the CADS solution. Furthermore, an expert recommendation system will act as an alternative to expert plant pathologists. Tomato leaf diseases data from the PlantVillage dataset is used in the proposed solution. Transfer learning technique was adopted in training deep neural network models with original and augmented data of 16,684 and 53,476 images respectively. The best model for the dataset was efficientNet B5 with best F1 score accuracy of 0.9842 and 0.9930. The predicted output of B5 was interpreted with explainable AI techniques and validated using YOLOv4. Inference of the proposed solution was a client-server interface where end-users can upload infected leaf images via mobile phones or web browsers. This entire system was tested in real-time with 250 volunteers with 4G mobile network or 100 MBPS wifi. The average throughput time of the system is around 4.3 seconds.",
    "affiliations": [],
    "countries": [],
    "num_authors": 10
  },
  {
    "paperId": "328e83f14b3258eef00605b086a335e663a17bd7",
    "externalIds": {
      "DBLP": "journals/mta/CartolanoCP24",
      "DOI": "10.1007/s11042-023-17978-z",
      "CorpusId": 267081699
    },
    "url": "https://www.semanticscholar.org/paper/328e83f14b3258eef00605b086a335e663a17bd7",
    "title": "Analyzing and assessing explainable AI models for smart agriculture environments",
    "venue": "Multim. Tools Appl.",
    "year": 2024,
    "referenceCount": 17,
    "citationCount": 27,
    "openAccessPdf": {
      "url": "https://link.springer.com/content/pdf/10.1007/s11042-023-17978-z.pdf",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11042-023-17978-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11042-023-17978-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-15",
    "authors": [
      {
        "authorId": "2182628938",
        "name": "Andrea Cartolano"
      },
      {
        "authorId": "2237999539",
        "name": "Alfredo Cuzzocrea"
      },
      {
        "authorId": "2237999771",
        "name": "Giovanni Pilato"
      }
    ],
    "abstract": "We analyze a case study in the field of smart agriculture exploiting Explainable AI (XAI) approach, a field of study that aims to provide interpretations and explanations to the behaviour of AI systems. The study regards a multiclass classification problem on the Crop Recommendation dataset. The original task is the prediction of the most adequate crop, according to seven features. In addition to the predictions, two of the most well-known XAI approaches have been used in order to obtain explanations and interpretations of the behaviour of the models: SHAP ( SH apley A dditive Ex P lanations), and LIME (Local Interpretable Model-Agnostic Explanations). Both packages provide easy-to-understand visualizations that allow common users to understand explanations of single predictions even without going into the mathematical details of the algorithms. Within the scientific community criticisms have been raised against these approaches, and recently some papers brought to light some weaknesses. However, the two algorithms are among the most popular in XAI and are still considered points of reference for this field of study.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "9784066a2acf67fcb3aa4eecddfe02415dec8c20",
    "externalIds": {
      "DOI": "10.70937/itej.v2i01.53",
      "CorpusId": 275702479
    },
    "url": "https://www.semanticscholar.org/paper/9784066a2acf67fcb3aa4eecddfe02415dec8c20",
    "title": "Explainable AI In E-Commerce: Enhancing Trust And Transparency In AI-Driven Decisions",
    "venue": "Innovatech Engineering Journal",
    "year": 2024,
    "referenceCount": 0,
    "citationCount": 13,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.70937/itej.v2i01.53?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.70937/itej.v2i01.53, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-01-08",
    "authors": [
      {
        "authorId": "2341010971",
        "name": "Malay Sarkar"
      }
    ],
    "abstract": "This study explores the transformative role of Explainable Artificial Intelligence (XAI) in e-commerce, focusing on its potential to enhance consumer trust, transparency, and regulatory compliance. Through a systematic review of 42 peer-reviewed articles, this research examines the applications, challenges, and limitations of XAI techniques such as SHAP (Shapley Additive Explanations), LIME (Local Interpretable Model-Agnostic Explanations), and other interpretability frameworks in consumer-facing AI systems. The findings reveal that XAI significantly improves user trust and satisfaction by providing interpretable explanations for AI-driven decisions in areas like recommendation engines, fraud detection, and dynamic pricing. However, critical gaps remain, including the scalability of XAI methods for handling large datasets, their limited capacity to address systemic biases, and the need for personalized, user-centric explanations tailored to diverse audiences. The study also highlights the role of XAI in ensuring compliance with regulations such as GDPR and CCPA, showcasing its dual impact on operational transparency and legal adherence. By identifying these strengths and gaps, this research contributes to a deeper understanding of XAI’s potential and provides valuable insights for its effective integration into e-commerce platforms. These findings underscore the necessity of advancing XAI methodologies to meet the evolving demands of the digital marketplace.",
    "affiliations": [],
    "countries": [],
    "num_authors": 1
  },
  {
    "paperId": "65076655c75c7f9664841817a3e8db3fb1421705",
    "externalIds": {
      "DBLP": "journals/jbi/MetschSAPKHH24",
      "DOI": "10.1101/2022.11.21.517358",
      "CorpusId": 256765680,
      "PubMed": "38301750"
    },
    "url": "https://www.semanticscholar.org/paper/65076655c75c7f9664841817a3e8db3fb1421705",
    "title": "CLARUS: An Interactive Explainable AI Platform for Manual Counterfactuals in Graph Neural Networks",
    "venue": "bioRxiv",
    "year": 2023,
    "referenceCount": 62,
    "citationCount": 38,
    "openAccessPdf": {
      "url": "https://www.biorxiv.org/content/biorxiv/early/2023/02/09/2022.11.21.517358.full.pdf",
      "status": "GREEN",
      "license": "CCBYNCND",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2022.11.21.517358?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2022.11.21.517358, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-01",
    "authors": [
      {
        "authorId": "2142485909",
        "name": "J. Beinecke"
      },
      {
        "authorId": "1947785",
        "name": "Anna Saranti"
      },
      {
        "authorId": "2162998870",
        "name": "Alessa Angerschmid"
      },
      {
        "authorId": "2152300",
        "name": "B. Pfeifer"
      },
      {
        "authorId": "2192361873",
        "name": "Vanessa Klemt"
      },
      {
        "authorId": "47596587",
        "name": "Andreas Holzinger"
      },
      {
        "authorId": "1731430",
        "name": "Anne-Christin Hauschild"
      }
    ],
    "abstract": "Background Lack of trust in artificial intelligence (AI) models in medicine is still the key blockage for the use of AI in clinical decision support systems (CDSS). Although AI models are already performing excellently in systems medicine, their black-box nature entails that patient-specific decisions are incomprehensible for the physician. This is especially true for very complex models such as graph neural networks (GNNs), a common state-of-the-art approach to model biological networks such as protein-protein-interaction graphs (PPIs) to predict clinical outcomes. The aim of explainable AI (XAI) algorithms is to “explain” to a human domain expert, which input features, such as genes, influenced a specific recommendation. However, in the clinical domain, it is essential that these explanations lead to some degree of causal understanding by a clinician in the context of a specific application. Results We developed the CLARUS platform, aiming to promote human understanding of GNN predictions by allowing the domain expert to validate and improve the decision-making process. CLARUS enables the visualisation of the patient-specific biological networks used to train and test the GNN model, where nodes and edges correspond to gene products and their interactions, for instance. XAI methods, such as GNNExplainer, compute relevance values for genes and interactions. The CLARUS graph visualisation highlights gene and interaction relevances by color intensity and line thickness, respectively. This enables domain experts to gain deeper insights into the biological network by identifying the most influential sub-graphs and molecular pathways crucial for the decision-making process. More importantly, the expert can interactively alter the patient-specific PPI network based on the acquired understanding and initiate re-prediction or retraining. This interactivity allows to ask manual counterfactual questions and analyse the resulting effects on the GNN prediction. Conclusion To the best of our knowledge, we present the first interactive XAI platform prototype, CLARUS, that allows not only the evaluation of specific human counterfactual questions based on user-defined alterations of patient PPI networks and a re-prediction of the clinical outcome but also a retraining of the entire GNN after changing the underlying graph structures. The platform is currently hosted by the GWDG on https://rshiny.gwdg.de/apps/clarus/.",
    "affiliations": [],
    "countries": [],
    "num_authors": 7
  },
  {
    "paperId": "576844456ba24457771ad4dbfe0f9c41ca94dbb5",
    "externalIds": {
      "DBLP": "journals/npjdm/NagendranFKGF24",
      "PubMedCentral": "11297294",
      "DOI": "10.1038/s41746-024-01200-x",
      "CorpusId": 271671358,
      "PubMed": "39095449"
    },
    "url": "https://www.semanticscholar.org/paper/576844456ba24457771ad4dbfe0f9c41ca94dbb5",
    "title": "Eye tracking insights into physician behaviour with safe and unsafe explainable AI recommendations",
    "venue": "npj Digit. Medicine",
    "year": 2024,
    "referenceCount": 60,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "https://www.nature.com/articles/s41746-024-01200-x.pdf",
      "status": "GOLD",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11297294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-02",
    "authors": [
      {
        "authorId": "50600402",
        "name": "M. Nagendran"
      },
      {
        "authorId": "2854788",
        "name": "Paul Festor"
      },
      {
        "authorId": "2297296338",
        "name": "Matthieu Komorowski"
      },
      {
        "authorId": "2242617182",
        "name": "Anthony C. Gordon"
      },
      {
        "authorId": "2286113515",
        "name": "A. A. Faisal"
      }
    ],
    "abstract": "We studied clinical AI-supported decision-making as an example of a high-stakes setting in which explainable AI (XAI) has been proposed as useful (by theoretically providing physicians with context for the AI suggestion and thereby helping them to reject unsafe AI recommendations). Here, we used objective neurobehavioural measures (eye-tracking) to see how physicians respond to XAI with N = 19 ICU physicians in a hospital’s clinical simulation suite. Prescription decisions were made both pre- and post-reveal of either a safe or unsafe AI recommendation and four different types of simultaneously presented XAI. We used overt visual attention as a marker for where physician mental attention was directed during the simulations. Unsafe AI recommendations attracted significantly greater attention than safe AI recommendations. However, there was no appreciably higher level of attention placed onto any of the four types of explanation during unsafe AI scenarios (i.e. XAI did not appear to ‘rescue’ decision-makers). Furthermore, self-reported usefulness of explanations by physicians did not correlate with the level of attention they devoted to the explanations reinforcing the notion that using self-reports alone to evaluate XAI tools misses key aspects of the interaction behaviour between human and machine.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "1d4acddc281471cc1f363c9d1f402d19e7ded77a",
    "externalIds": {
      "DOI": "10.1155/2023/4637678",
      "CorpusId": 264888446
    },
    "url": "https://www.semanticscholar.org/paper/1d4acddc281471cc1f363c9d1f402d19e7ded77a",
    "title": "AI Trust: Can Explainable AI Enhance Warranted Trust?",
    "venue": "Human Behavior and Emerging Technologies",
    "year": 2023,
    "referenceCount": 39,
    "citationCount": 22,
    "openAccessPdf": {
      "url": "https://downloads.hindawi.com/journals/hbet/2023/4637678.pdf",
      "status": "GOLD",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1155/2023/4637678?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1155/2023/4637678, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-31",
    "authors": [
      {
        "authorId": "2264821615",
        "name": "Regina de Brito Duarte"
      },
      {
        "authorId": "144106225",
        "name": "Filipa Correia"
      },
      {
        "authorId": "2252416777",
        "name": "P. Arriaga"
      },
      {
        "authorId": "2264824312",
        "name": "Ana Paiva"
      }
    ],
    "abstract": "Explainable artificial intelligence (XAI), known to produce explanations so that predictions from AI models can be understood, is commonly used to mitigate possible AI mistrust. The underlying premise is that the explanations of the XAI models enhance AI trust. However, such an increase may depend on many factors. This article examined how trust in an AI recommendation system is affected by the presence of explanations, the performance of the system, and the level of risk. Our experimental study, conducted with 215 participants, has shown that the presence of explanations increases AI trust, but only in certain conditions. AI trust was higher when explanations with feature importance were provided than with counterfactual explanations. Moreover, when the system performance is not guaranteed, the use of explanations seems to lead to an overreliance on the system. Lastly, system performance had a stronger impact on trust, compared to the effects of other factors (explanation and risk).",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "b768705712fe81c7b4bef94dce957552361b82f6",
    "externalIds": {
      "DBLP": "journals/corr/abs-2409-15338",
      "ArXiv": "2409.15338",
      "DOI": "10.48550/arXiv.2409.15338",
      "CorpusId": 272832230
    },
    "url": "https://www.semanticscholar.org/paper/b768705712fe81c7b4bef94dce957552361b82f6",
    "title": "Explainable AI: Definition and attributes of a good explanation for health AI",
    "venue": "AI and Ethics",
    "year": 2024,
    "referenceCount": 82,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.15338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-09",
    "authors": [
      {
        "authorId": "3444731",
        "name": "E. Kyrimi"
      },
      {
        "authorId": "2322503083",
        "name": "Scott McLachlan"
      },
      {
        "authorId": "2239205423",
        "name": "Jared M. Wohlgemut"
      },
      {
        "authorId": "2335206",
        "name": "Zane Perkins"
      },
      {
        "authorId": "1953585",
        "name": "D. Lagnado"
      },
      {
        "authorId": "2242974990",
        "name": "William Marsh"
      },
      {
        "authorId": "2326110289",
        "name": "ExAIDSS Expert Group"
      }
    ],
    "abstract": "\n Proposals of artificial intelligence (AI) solutions based on more complex and accurate predictive models are becoming ubiquitous across many disciplines. As the complexity of these models increases, there is a tendency for transparency and users’ understanding to decrease. This means accurate prediction alone is insufficient to make an AI-based solution truly useful. For the development of healthcare systems, this raises new issues for accountability and safety. How and why an AI system made a recommendation may necessitate complex explanations of the inner workings and reasoning processes. While research on explainable AI (XAI) has grown significantly in recent years, and the demand for XAI in medicine is high, determining what constitutes a good explanation is ad hoc and providing adequate explanations remains a challenge. To realise the potential of AI, it is critical to shed light on two fundamental questions of explanation for safety–critical AI such as health-AI that remain unanswered: (1) What is an explanation in health-AI? And (2) What are the attributes of a good explanation in health-AI? In this study and possibly for the first time we studied published literature, and expert opinions from a diverse group of professionals reported from a two-round Delphi study. The research outputs include (1) a proposed definition of explanation in health-AI, and (2) a comprehensive set of attributes that characterize a good explanation in health-AI.",
    "affiliations": [],
    "countries": [],
    "num_authors": 7
  },
  {
    "paperId": "f203ca98725217c2df8b429bf33289a24f59f807",
    "externalIds": {
      "DOI": "10.52783/jes.8814",
      "CorpusId": 278798149
    },
    "url": "https://www.semanticscholar.org/paper/f203ca98725217c2df8b429bf33289a24f59f807",
    "title": "Explainable AI in E-Commerce: Seller Recommendations with Ethnocentric Transparency",
    "venue": "Journal of Electrical Systems",
    "year": 2024,
    "referenceCount": 19,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.52783/jes.8814?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.52783/jes.8814, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-16",
    "authors": [
      {
        "authorId": "2362775215",
        "name": "Tauqeer Akhtar"
      }
    ],
    "abstract": "Personalized seller recommendations are fundamental to enhancing user experiences and increasing sales in e-commerce platforms. Traditional recommendation systems, however, often function as black-box models, offering limited interpretability. This paper explores the integration of Explainable AI (XAI) techniques, particularly Integrated Gradients (IG) and DeepLIFT (Deep Learning Important FeaTures), into a hybrid recommendation system. The proposed approach combines Matrix Factorization (MF) and Graph Neural Networks (GNNs) to deliver personalized and interpretable seller recommendations. Using real-world e-commerce datasets, the study evaluates how different features, such as user interaction history, social connections, and seller reputation contribute to recommendation outcomes. The system addresses the trade-offs between recommendation accuracy and interpretability, ensuring that insights are both actionable and trustworthy. Experimental results demonstrate that the hybrid model achieves substantial improvements in precision, recall, and F1-score compared to standalone MF and GNN-based approaches. Moreover, Integrated Gradients and DeepLIFT provide users with clear and intuitive explanations of the recommendation process, fostering trust in the system. This paper also introduces a comprehensive feature attribution analysis to quantify the impact of key factors, including behavioral patterns and network influence, on recommendation decisions. A comparative evaluation with state-of-the-art neural recommendation models highlights the effectiveness of the proposed system in balancing performance with interpretability. Finally, the study discusses future enhancements, such as incorporating explainability techniques tailored for multimodal data, employing reinforcement learning for adaptive personalization, and extending the model to handle dynamic user preferences. These findings underscore the importance of transparent, user-focused AI in driving innovation in e-commerce recommendation systems.",
    "affiliations": [],
    "countries": [],
    "num_authors": 1
  },
  {
    "paperId": "b451f6dde2e4b9b6d72b892a4768228c577ec4ac",
    "externalIds": {
      "ArXiv": "2404.11080",
      "CorpusId": 269187764
    },
    "url": "https://www.semanticscholar.org/paper/b451f6dde2e4b9b6d72b892a4768228c577ec4ac",
    "title": "Recommender Systems in Financial Trading: Using machine-based conviction analysis in an explainable AI investment framework",
    "venue": "",
    "year": 2024,
    "referenceCount": 39,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.11080, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": null,
    "publicationDate": "2024-04-17",
    "authors": [
      {
        "authorId": "2296991022",
        "name": "Alicia Vidler"
      }
    ],
    "abstract": "Traditionally, assets are selected for inclusion in a portfolio (long or short) by human analysts. Teams of human portfolio managers (PMs) seek to weigh and balance these securities using optimisation methods and other portfolio construction processes. Often, human PMs consider human analyst recommendations against the backdrop of the analyst's recommendation track record and the applicability of the analyst to the recommendation they provide. Many firms regularly ask analysts to provide a\"conviction\"level on their recommendations. In the eyes of PMs, understanding a human analyst's track record has typically come down to basic spread sheet tabulation or, at best, a\"virtual portfolio\"paper trading book to keep track of results of recommendations. Analysts' conviction around their recommendations and their\"paper trading\"track record are two crucial workflow components between analysts and portfolio construction. Many human PMs may not even appreciate that they factor these data points into their decision-making logic. This chapter explores how Artificial Intelligence (AI) can be used to replicate these two steps and bridge the gap between AI data analytics and AI-based portfolio construction methods. This field of AI is referred to as Recommender Systems (RS). This chapter will further explore what metadata that RS systems functionally supply to downstream systems and their features.",
    "affiliations": [],
    "countries": [],
    "num_authors": 1
  },
  {
    "paperId": "fde09ebed89633eaed16c857147bb2123c77d58a",
    "externalIds": {
      "DOI": "10.1177/10949968231200221",
      "CorpusId": 266134117
    },
    "url": "https://www.semanticscholar.org/paper/fde09ebed89633eaed16c857147bb2123c77d58a",
    "title": "When Post Hoc Explanation Knocks: Consumer Responses to Explainable AI Recommendations",
    "venue": "Journal of Interactive Marketing",
    "year": 2023,
    "referenceCount": 49,
    "citationCount": 18,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/10949968231200221?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/10949968231200221, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-07",
    "authors": [
      {
        "authorId": "2145780744",
        "name": "Changdong Chen"
      },
      {
        "authorId": "2271898801",
        "name": "Allen Ding Tian"
      },
      {
        "authorId": "2271916495",
        "name": "Ruochen Jiang"
      }
    ],
    "abstract": "Artificial intelligence (AI) recommendations are becoming increasingly prevalent, but consumers are often reluctant to trust them, in part due to the “black-box” nature of algorithm-facilitated recommendation agents. Despite the acknowledgment of the vital role of interpretability in consumer trust in AI recommendations, it remains unclear how to effectively increase interpretability perceptions and consequently enhance positive consumer responses. The current research addresses this issue by investigating the effects of the presence and type of post hoc explanations in boosting positive consumer responses to AI recommendations in different decision-making domains. Across four studies, the authors demonstrate that the presence of post hoc explanations increases interpretability perceptions, which in turn fosters positive consumer responses (e.g., trust, purchase intention, and click-through) to AI recommendations. Moreover, they show that the facilitating effect of post hoc explanations is stronger in the utilitarian (vs. hedonic) decision-making domain. Further, explanation type modulates the effectiveness of post hoc explanations such that attribute-based explanations are more effective in enhancing trust in the utilitarian decision-making domain, whereas user-based explanations are more effective in the hedonic decision-making domain.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "88bd41850a54c95e4b35a58f3f11ea4053a33bd2",
    "externalIds": {
      "DOI": "10.1109/GCAT62922.2024.10923860",
      "CorpusId": 277193818
    },
    "url": "https://www.semanticscholar.org/paper/88bd41850a54c95e4b35a58f3f11ea4053a33bd2",
    "title": "ERS: An Explainable Research Paper Recommendation System for User-Centric Discovery with LIME",
    "venue": "2024 5th IEEE Global Conference for Advancement in Technology (GCAT)",
    "year": 2024,
    "referenceCount": 29,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/GCAT62922.2024.10923860?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/GCAT62922.2024.10923860, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2024-10-04",
    "authors": [
      {
        "authorId": "2305785220",
        "name": "Parvathy P Nair"
      },
      {
        "authorId": "2305783684",
        "name": "Surabhi Sudhan"
      },
      {
        "authorId": "2305787515",
        "name": "M. Thushara"
      }
    ],
    "abstract": "In the era of growing academic research, the challenge of efficiently discovering relevant and valuable papers persists, exacerbated by the ever-growing volume of publications. Traditional recommendation systems, while offering assistance, often lack transparency, leaving users in the dark about the rationale behind suggestions. This research proposes an Explainable Research Paper Recommendation System (ERS) that leverages the Locally Interpretable Model-agnostic Explanations (LIME) algorithm. Our ERS personalizes recommendations by analyzing user profiles, research interests, and citation history. Uniquely, ERS offers transparent explanations for each recommendation, utilizing LIME to highlight the key factors influencing each suggested paper. This fosters user trust and empowers researchers with a deeper understanding of the recommendation process. Furthermore, the paper explores the potential of ERS to enhance academic collaboration, knowledge-sharing, and scalability, while contributing to the advancement of Explainable AI research. Ultimately, by delivering a positive user experience through relevant recommendations and clear explanations, such as systems can empower researchers to navigate the vast ocean of academic literature.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "b4151feb1dd2f06888027a6db96369cf0b483bd9",
    "externalIds": {
      "DOI": "10.1109/IATMSI60426.2024.10503250",
      "CorpusId": 269332680
    },
    "url": "https://www.semanticscholar.org/paper/b4151feb1dd2f06888027a6db96369cf0b483bd9",
    "title": "Integration of Explainable Artificial Intelligence (XAI) in the Development of Disease Prediction and Medicine Recommendation System",
    "venue": "2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI)",
    "year": 2024,
    "referenceCount": 11,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IATMSI60426.2024.10503250?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IATMSI60426.2024.10503250, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2024-03-14",
    "authors": [
      {
        "authorId": "2298064946",
        "name": "Joel Mathew"
      },
      {
        "authorId": "2298062803",
        "name": "Richie Suresh Koshy"
      },
      {
        "authorId": "2298090592",
        "name": "Dr. R. Chitra"
      },
      {
        "authorId": "2298061499",
        "name": "Caleb Stephen"
      }
    ],
    "abstract": "The contemporary healthcare landscape necessitates innovative solutions to improve transparency and understanding in medical decision-making. This paper proposes an advanced medicine recommendation system and a robust disease prediction model integrating explainable AI (XAI). Recognizing the prevalence of misinformation and the urgent need for user-friendly applications, the system empowers users to manage their health proactively. It can be extended beyond common diseases, encompassing rare diseases, and employs XAI algorithms, specifically SHAP and LIME, to enhance transparency. The system incorporates Random Forest Classifier and Decision Tree models, showcasing high accuracy and robustness. The explanation models contribute to user understanding, while performance metrics offer insights into model strengths and generalization abilities. Figures depict SHAP outputs for Decision Tree and Random Forest models, emphasizing transparency in medical predictions. This proposed system addresses critical healthcare challenges, fostering informed decision-making and user trust.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "62635461b44d9ff7901fbce982a5c1bbccfaf2b0",
    "externalIds": {
      "MAG": "2975495759",
      "DBLP": "conf/nlpcc/XuUDF0Z19",
      "DOI": "10.1007/978-3-030-32236-6_51",
      "CorpusId": 203620028
    },
    "url": "https://www.semanticscholar.org/paper/62635461b44d9ff7901fbce982a5c1bbccfaf2b0",
    "title": "Explainable AI: A Brief Survey on History, Research Areas, Approaches and Challenges",
    "venue": "Natural Language Processing and Chinese Computing",
    "year": 2019,
    "referenceCount": 24,
    "citationCount": 513,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-030-32236-6_51?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-030-32236-6_51, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2019-10-09",
    "authors": [
      {
        "authorId": "2724114",
        "name": "Feiyu Xu"
      },
      {
        "authorId": "1781790",
        "name": "H. Uszkoreit"
      },
      {
        "authorId": "3165307",
        "name": "Yangzhou Du"
      },
      {
        "authorId": "2088400146",
        "name": "Wei Fan"
      },
      {
        "authorId": "144060462",
        "name": "Dongyan Zhao"
      },
      {
        "authorId": "145254043",
        "name": "Jun Zhu"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 6
  },
  {
    "paperId": "173ee66ebf96b6767aa1999bc4310dc7c39d40f0",
    "externalIds": {
      "DBLP": "conf/bigmm/CartolanoCPG22",
      "DOI": "10.1109/BigMM55396.2022.00020",
      "CorpusId": 255267769
    },
    "url": "https://www.semanticscholar.org/paper/173ee66ebf96b6767aa1999bc4310dc7c39d40f0",
    "title": "Explainable AI at Work! What Can It Do for Smart Agriculture?",
    "venue": "IEEE International Conference on Multimedia Big Data",
    "year": 2022,
    "referenceCount": 39,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/BigMM55396.2022.00020?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/BigMM55396.2022.00020, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-12-01",
    "authors": [
      {
        "authorId": "2182628938",
        "name": "Andrea Cartolano"
      },
      {
        "authorId": "145046124",
        "name": "A. Cuzzocrea"
      },
      {
        "authorId": "1708205",
        "name": "G. Pilato"
      },
      {
        "authorId": "32715613",
        "name": "G. Grasso"
      }
    ],
    "abstract": "Explainable AI (XAI) is gaining the momentum, at now. While the idea is to apply it in different scenarios, including medicine, business analytics, genomics computing and so forth, in this paper we focus the attention on another emerging case, represented by so called smart agricolture. In this paper, we propose the application of some well-known XAI tools on top of the Crop Recommendation dataset. Our research efforts also involve the sensitivity analysis of retrieved results.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "ea590185f9661c311f28b9b71d6902e6ca651a2a",
    "externalIds": {
      "DOI": "10.1109/ICCTDC64446.2025.11158747",
      "CorpusId": 281509722
    },
    "url": "https://www.semanticscholar.org/paper/ea590185f9661c311f28b9b71d6902e6ca651a2a",
    "title": "Developing Explainable AI Models for Personalized Treatment Recommendations Using LLMs and EHR Data",
    "venue": "2025 International Conference on Computing Technologies & Data Communication (ICCTDC)",
    "year": 2025,
    "referenceCount": 36,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCTDC64446.2025.11158747?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCTDC64446.2025.11158747, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2025-07-04",
    "authors": [
      {
        "authorId": "2332938567",
        "name": "Sukanth Korkanti"
      }
    ],
    "abstract": "Personalized treatment recommendations remain critical for better outcomes and optimization of healthcare delivery. This paper discusses an explainable AI framework for personalized treatment recommendation using large language models in conjunction with EHR data. Using the publicly available dataset MIMIC-III, this approach integrates into a unique analysis advanced natural language processing capabilities of LLMs, interpreting complex clinical narratives along with structured data. We implement explainability techniques that provide insight into the decision-making process of the AI model to ensure transparency and trust. The results show improved accuracy and clinician satisfaction compared to traditional models, underlining the potential of explainable AI in personalized medicine. This work emphasizes the need to combine large datasets with state-of-the-art AI technologies to enable trustworthy and effective healthcare solutions.",
    "affiliations": [],
    "countries": [],
    "num_authors": 1
  },
  {
    "paperId": "dfe3408f32e4286ea284e6b4eaedb545dea5dbe9",
    "externalIds": {
      "DOI": "10.37547/tajet/volume07issue04-08",
      "CorpusId": 277648816
    },
    "url": "https://www.semanticscholar.org/paper/dfe3408f32e4286ea284e6b4eaedb545dea5dbe9",
    "title": "Explainable Ai In Customer Experience Management: Personalization Algorithms in Crm Systems",
    "venue": "The American Journal of Engineering and Technology",
    "year": 2025,
    "referenceCount": 0,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "https://doi.org/10.37547/tajet/volume07issue04-08",
      "status": "GOLD",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.37547/tajet/volume07issue04-08?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.37547/tajet/volume07issue04-08, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-04-08",
    "authors": [
      {
        "authorId": "2354437924",
        "name": "Sergei Berezin"
      }
    ],
    "abstract": "The article examines the features of integrating artificial intelligence algorithms (Explainable AI, XAI) into CRM systems aimed at enhancing customer experience (Customer Experience, CX). Based on an analysis of recent publications, the study explores the principles of personalization as well as approaches to the explainability of machine learning algorithms, including chatbots and recommendation systems. It demonstrates that transparency and interpretability of model outputs positively influence customer trust and loyalty while simultaneously improving the efficiency of internal business processes. The article analyzes the implementation experience of XAI in the banking sector, insurance call centers, and online retail, which has led to improvements in retention, conversion, and satisfaction metrics. The information presented in the article is intended for researchers and professionals in the field of artificial intelligence focused on developing interpretable machine learning algorithms, as well as for analysts seeking to optimize CRM systems to enhance customer experience management. In addition, the material is useful for professionals in corporate governance and marketing who aim to integrate advanced Explainable AI methods into personalization strategies and decision-making processes, ensuring the transparency and adaptability of services under dynamic market conditions.",
    "affiliations": [],
    "countries": [],
    "num_authors": 1
  },
  {
    "paperId": "230de5e607efa30416bab29db654f9e14415b6ad",
    "externalIds": {
      "DOI": "10.3390/electronics14183738",
      "CorpusId": 281560332
    },
    "url": "https://www.semanticscholar.org/paper/230de5e607efa30416bab29db654f9e14415b6ad",
    "title": "An Explainable AI Framework for Online Diabetes Risk Prediction with a Personalized Chatbot Assistant",
    "venue": "Electronics",
    "year": 2025,
    "referenceCount": 14,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics14183738?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics14183738, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-09-22",
    "authors": [
      {
        "authorId": "2335970300",
        "name": "Ehesan Maimaitijiang"
      },
      {
        "authorId": "2382197947",
        "name": "Muyesaier Aihaiti"
      },
      {
        "authorId": "2335035043",
        "name": "Yasin Mamatjan"
      }
    ],
    "abstract": "Background and Objective: Diabetes is a prevalent chronic disease that presents considerable health risks, making prompt diagnosis and treatment essential to avert complications. Traditional Artificial Intelligence (AI) models for diabetes prediction often operate as black boxes. A major issue caused by this is that black boxes lack interpretability, which impacts their effectiveness in clinical use cases. We introduce a novel online recommendation framework using explainable AI (XAI) to predict type II diabetes risk and provide clear, actionable analyses with a personalized chatbot assistant. Methods: To make the model, we chose the CatBoost classifier and SHapley Additive exPlanations (SHAP) due to their ability to provide accurate predictions. Using those tools, we analyzed 16 individual risk factors from a dataset of 520 patients. We applied the Synthetic Minority Over-sampling Technique (SMOTE) to reduce the effect of data imbalance. We also developed an interactive interface that allows users to input data, visualize personalized risk profiles, and understand the driving factors behind predictions. Finally, large language models (LLMs) were integrated into the interface for patient-specific recommendations for improving health and lifestyle through a personalized chatbot assistant. Results: The model demonstrated great predictive performance, with an Area Under the ROC Curve (AUC) of 0.99, a Cohen Kappa score of 0.978, and an F1 score of 0.99. For the minority class, SMOTE application improved performance metrics, resulting in an AUC of 0.98 and an F1 score of 0.91 for female patients. Conclusions: This study proposes an explainable AI framework for predicting diabetes risk online and providing patient-specific advice through a personalized chatbot assistant. This will help to facilitate better decision-making and improved management of diabetes risk.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "f9f4cdb0ff91640d0fb28025e91b89758010da74",
    "externalIds": {
      "DOI": "10.3390/pr13010102",
      "CorpusId": 275311992
    },
    "url": "https://www.semanticscholar.org/paper/f9f4cdb0ff91640d0fb28025e91b89758010da74",
    "title": "A Methodological Framework for Business Decisions with Explainable AI and the Analytic Hierarchical Process",
    "venue": "Processes",
    "year": 2025,
    "referenceCount": 37,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://doi.org/10.3390/pr13010102",
      "status": "GOLD",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/pr13010102?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/pr13010102, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-01-03",
    "authors": [
      {
        "authorId": "2137515097",
        "name": "Gabriel Marín Díaz"
      },
      {
        "authorId": "2320628073",
        "name": "Raquel Gómez Medina"
      },
      {
        "authorId": "2320631933",
        "name": "José Alberto Aijón Jiménez"
      }
    ],
    "abstract": "In today’s data-driven business landscape, effective and transparent decision making becomes relevant to maintain a competitive advantage over the competition, especially in customer service in B2B environments. This study presents a methodological framework that integrates Explainable Artificial Intelligence (XAI), C-means clustering, and the Analytic Hierarchical Process (AHP) to improve strategic decision making in business environments. The framework addresses the need to obtain interpretable information from predictions based on machine learning processes and the prioritization of key factors for decision making. C-means clustering enables flexible customer segmentation based on interaction patterns, while XAI provides transparency into model outputs, allowing support teams to understand the factors influencing each recommendation. The AHP is then applied to prioritize criteria within each customer segment, aligning support actions with organizational goals. Tested with real customer interaction data, this integrated approach proved effective in accurately segmenting customers, predicting support needs, and optimizing resource allocation. The combined use of XAI and the AHP ensures that business decisions are data-driven, interpretable, and aligned with the company’s strategic objectives, making this framework relevant for companies seeking to improve their customer service in complex B2B contexts. Future research will explore the application of the proposed model in different business processes.",
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "77c014c618465a90e00db4347eb04ce6d5f1dbf3",
    "externalIds": {
      "DOI": "10.54941/ahfe1005635",
      "CorpusId": 273904661
    },
    "url": "https://www.semanticscholar.org/paper/77c014c618465a90e00db4347eb04ce6d5f1dbf3",
    "title": "Optimizing AI System Security: An Ecosystem Recommendation to Socio-Technical Risk Management",
    "venue": "AHFE International",
    "year": 2024,
    "referenceCount": 0,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54941/ahfe1005635?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54941/ahfe1005635, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "51193905",
        "name": "Kitty Kioskli"
      },
      {
        "authorId": "2329775487",
        "name": "Antonios Ramfos"
      },
      {
        "authorId": "2330059715",
        "name": "Steve Taylor"
      },
      {
        "authorId": "2140688864",
        "name": "Leandros A. Maglaras"
      },
      {
        "authorId": "2310530389",
        "name": "Ricardo Lugo"
      }
    ],
    "abstract": "Given the sophistication of adversarial machine learning (ML) attacks on Artificial Intelligence (AI) systems, enhanced security frameworks that integrate human factors into risk assessments are critical. This paper presents a comprehensive methodology combining cybersecurity, cyberpsychology, and AI to address human-related aspects of these attacks. It introduces an AI system security optimization ecosystem to help security officers protect AI systems against various attacks, including poisoning, evasion, extraction, and inference. The risk management approach enhances NIST and ENISA frameworks by incorporating socio-technical aspects of adversarial ML threats. By creating digital clones and using explainable AI (XAI) techniques, the human elements of attackers are integrated into security risk management. An innovative conversational agent is proposed to include defenders’ perspectives, advancing the design and deployment of secure AI systems and guiding future certification schemes.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "77a09effafd8dcdfe44a709f51c12e86992e6cce",
    "externalIds": {
      "DOI": "10.1109/ISACC65211.2025.10969376",
      "CorpusId": 277999930
    },
    "url": "https://www.semanticscholar.org/paper/77a09effafd8dcdfe44a709f51c12e86992e6cce",
    "title": "Challenges and Future Perspectives in Explainable AI A Roadmap for New Scholars",
    "venue": "2025 3rd International Conference on Intelligent Systems, Advanced Computing and Communication (ISACC)",
    "year": 2025,
    "referenceCount": 35,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISACC65211.2025.10969376?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISACC65211.2025.10969376, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference",
      "Review"
    ],
    "publicationDate": "2025-02-27",
    "authors": [
      {
        "authorId": "2356859639",
        "name": "Chumki Sil"
      },
      {
        "authorId": "2261924161",
        "name": "Dr. Papri Ghosh"
      },
      {
        "authorId": "2353404768",
        "name": "Subhram Das"
      },
      {
        "authorId": "122078277",
        "name": "Md Ashifuddin Mondal"
      }
    ],
    "abstract": "Machine learning is transforming industries with its capabilities in prediction, recommendation, and decision support. However, the complexity of these systems often hides their inner workings, earning them the label \"black box\" models. This lack of transparency makes it challenging for users to understand both processes and outcomes, highlighting the need for Explainable Artificial Intelligence (XAI) to build trustworthy AI systems.This paper reviews existing literature and contributions on XAI, focusing on core concepts such as interpretability, exploitability, and intelligibility. We emphasize the importance of these aspects for fostering trust, explore the XAI lifecycle, and analyze taxonomies of XAI methods. Additionally, we discuss challenges in the field and propose future research directions, stressing responsible AI development. Our aim is to help novice researchers understand the XAI framework and apply it effectively to their work.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "01dedd96bd2d696e3d93582438f5e44a174064bd",
    "externalIds": {
      "DBLP": "journals/sle/TakamiFDO23",
      "DOI": "10.1186/s40561-023-00282-6",
      "CorpusId": 266150637
    },
    "url": "https://www.semanticscholar.org/paper/01dedd96bd2d696e3d93582438f5e44a174064bd",
    "title": "Personality-based tailored explainable recommendation for trustworthy smart learning system in the age of artificial intelligence",
    "venue": "Smart Learning Environments",
    "year": 2023,
    "referenceCount": 52,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://slejournal.springeropen.com/counter/pdf/10.1186/s40561-023-00282-6",
      "status": "GOLD",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1186/s40561-023-00282-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1186/s40561-023-00282-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-01",
    "authors": [
      {
        "authorId": "144664473",
        "name": "Kyosuke Takami"
      },
      {
        "authorId": "145667538",
        "name": "B. Flanagan"
      },
      {
        "authorId": "114887272",
        "name": "Yiling Dai"
      },
      {
        "authorId": "2240147388",
        "name": "Hiroaki Ogata"
      }
    ],
    "abstract": "In the age of artificial intelligence (AI), trust in AI systems is becoming more important. Explainable recommenders, which explain why an item is recommended, have recently been proposed in the field of learning technology to improve transparency, persuasiveness, and trustworthiness. However, the methods for generating explanations are limited and do not consider the learner’s cognitive perceptions or personality. This study draws inspiration from tailored intervention research in public health and investigates the effectiveness of personality-based tailored explanations by implementing them for the recommended quizzes in an explainable recommender system. High school students (n = 217) were clustered into three distinct profiles labeled Diligent (n = 77), Fearful (n = 72), and Agreeable (n = 68), based on the Big Five personality traits. The students were divided into a tailored intervention group (n = 106) and a control group (n = 111). In the tailored intervention group, personalized explanations for recommended quizzes were provided based on student profiles, with explanations based on quiz characteristics. In the control group, only non-personalized explanations based on quiz characteristics were provided. An 18-day A/B experiment showed that the tailored intervention group had significantly higher recommendation usage than the control group. These results suggest that personality-based tailored explanations with a recommender approach are effective for e-learning engagement and imply improved trustworthiness of AI learning systems.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "8dd0727601d40d437a182f05548da563ac60d4b3",
    "externalIds": {
      "DBLP": "conf/sigir/GeTZXL0FGLZ22",
      "ArXiv": "2204.11159",
      "DOI": "10.1145/3477495.3531973",
      "CorpusId": 248376909
    },
    "url": "https://www.semanticscholar.org/paper/8dd0727601d40d437a182f05548da563ac60d4b3",
    "title": "Explainable Fairness in Recommendation",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2022,
    "referenceCount": 79,
    "citationCount": 44,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2204.11159",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.11159, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2022-04-24",
    "authors": [
      {
        "authorId": "152988336",
        "name": "Yingqiang Ge"
      },
      {
        "authorId": "2110449137",
        "name": "Juntao Tan"
      },
      {
        "authorId": "2153095583",
        "name": "Yangchun Zhu"
      },
      {
        "authorId": "35846319",
        "name": "Yinglong Xia"
      },
      {
        "authorId": "33642939",
        "name": "Jiebo Luo"
      },
      {
        "authorId": "50152132",
        "name": "Shuchang Liu"
      },
      {
        "authorId": "2011378",
        "name": "Zuohui Fu"
      },
      {
        "authorId": "1947101",
        "name": "Shijie Geng"
      },
      {
        "authorId": "2109968285",
        "name": "Zelong Li"
      },
      {
        "authorId": "2145038716",
        "name": "Yongfeng Zhang"
      }
    ],
    "abstract": "Existing research on fairness-aware recommendation has mainly focused on the quantification of fairness and the development of fair recommendation models, neither of which studies a more substantial problem--identifying the underlying reason of model disparity in recommendation. This information is critical for recommender system designers to understand the intrinsic recommendation mechanism and provides insights on how to improve model fairness to decision makers. Fortunately, with the rapid development of Explainable AI, we can use model explainability to gain insights into model (un)fairness. In this paper, we study the problem ofexplainable fairness, which helps to gain insights about why a system is fair or unfair, and guides the design of fair recommender systems with a more informed and unified methodology. Particularly, we focus on a common setting with feature-aware recommendation and exposure unfairness, but the proposed explainable fairness framework is general and can be applied to other recommendation settings and fairness definitions. We propose a Counterfactual Explainable Fairness framework, called CEF, which generates explanations about model fairness that can improve the fairness without significantly hurting the performance. The CEF framework formulates an optimization problem to learn the \"minimal'' change of the input features that changes the recommendation results to a certain level of fairness. Based on the counterfactual recommendation result of each feature, we calculate an explainability score in terms of the fairness-utility trade-off to rank all the feature-based explanations, and select the top ones as fairness explanations. Experimental results on several real-world datasets validate that our method is able to effectively provide explanations to the model disparities and these explanations can achieve better fairness-utility trade-off when using them for recommendation than all the baselines.",
    "affiliations": [],
    "countries": [],
    "num_authors": 10
  },
  {
    "paperId": "42bfda85ef5e2baf171fce33523e7923f80ba827",
    "externalIds": {
      "ArXiv": "2206.05368",
      "DBLP": "journals/corr/abs-2206-05368",
      "DOI": "10.48550/arXiv.2206.05368",
      "CorpusId": 249626597
    },
    "url": "https://www.semanticscholar.org/paper/42bfda85ef5e2baf171fce33523e7923f80ba827",
    "title": "Learning to Rank Rationales for Explainable Recommendation",
    "venue": "arXiv.org",
    "year": 2022,
    "referenceCount": 48,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2206.05368",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.05368, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-06-10",
    "authors": [
      {
        "authorId": "2284763197",
        "name": "Zhichao Xu"
      },
      {
        "authorId": "1475721571",
        "name": "Yi Han"
      },
      {
        "authorId": "1958895984",
        "name": "Tao Yang"
      },
      {
        "authorId": "2077430797",
        "name": "Anh Tran"
      },
      {
        "authorId": "144922928",
        "name": "Qingyao Ai"
      }
    ],
    "abstract": "State-of-the-art recommender system (RS) mostly rely on complex deep neural network (DNN) model structure, which makes it difficult to provide explanations along with RS decisions. Previous researchers have proved that providing explanations along with recommended items can help users make informed decisions and improve their trust towards the uninterpretable blackbox system. In model-agnostic explainable recommendation, system designers deploy a separate explanation model to take as input from the decision model, and generate explanations to meet the goal of persuasiveness. In this work, we explore the task of ranking textual rationales (supporting evidences) for model-agnostic explainable recommendation. Most of existing rationales ranking algorithms only utilize the rationale IDs and interaction matrices to build latent factor representations; and the semantic information within the textual rationales are not learned effectively. We argue that such design is suboptimal as the important semantic information within the textual rationales may be used to better profile user preferences and item features. Seeing this gap, we propose a model named Semantic-Enhanced Bayesian Personalized Explanation Ranking (SE-BPER) to effectively combine the interaction information and semantic information. SE-BPER first initializes the latent factor representations with contextualized embeddings generated by transformer model, then optimizes them with the interaction data. Extensive experiments show that such methodology improves the rationales ranking performance while simplifying the model training process (fewer hyperparameters and faster convergence). We conclude that the optimal way to combine semantic and interaction information remains an open question in the task of rationales ranking.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "d9ec78a71231900b76b33472937db53189247953",
    "externalIds": {
      "DOI": "10.32604/cmes.2023.023897",
      "CorpusId": 256660616
    },
    "url": "https://www.semanticscholar.org/paper/d9ec78a71231900b76b33472937db53189247953",
    "title": "Explainable Rules and Heuristics in AI Algorithm Recommendation Approaches—A Systematic Literature Review and Mapping Study",
    "venue": "Computer Modeling in Engineering &amp; Sciences",
    "year": 2023,
    "referenceCount": 0,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "https://file.techscience.com/files/CMES/2023/TSP_CMES-136-2/TSP_CMES_23897/TSP_CMES_23897.pdf",
      "status": "GOLD",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.32604/cmes.2023.023897?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.32604/cmes.2023.023897, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2204989581",
        "name": "Francisco Jos�Garc韆-Pe馻lvo"
      },
      {
        "authorId": "2204999384",
        "name": "Andrea V醶quez-Ingelmo"
      },
      {
        "authorId": "2204989567",
        "name": "Alicia Garc韆-Holgado"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 3
  },
  {
    "paperId": "e632f1191b3c261cdae48698c97eb2e9a5a45c78",
    "externalIds": {
      "PubMedCentral": "8327305",
      "DBLP": "journals/www/NaisehAJA21",
      "DOI": "10.1007/s11280-021-00916-0",
      "CorpusId": 236882240,
      "PubMed": "34366701"
    },
    "url": "https://www.semanticscholar.org/paper/e632f1191b3c261cdae48698c97eb2e9a5a45c78",
    "title": "Explainable recommendation: when design meets trust calibration",
    "venue": "World wide web (Bussum)",
    "year": 2021,
    "referenceCount": 105,
    "citationCount": 31,
    "openAccessPdf": {
      "url": "https://link.springer.com/content/pdf/10.1007/s11280-021-00916-0.pdf",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8327305, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-08-02",
    "authors": [
      {
        "authorId": "2122587261",
        "name": "Mohammad Naiseh"
      },
      {
        "authorId": "1403767870",
        "name": "Dena Al-Thani"
      },
      {
        "authorId": "144623186",
        "name": "Nan Jiang"
      },
      {
        "authorId": "2114136995",
        "name": "Raian Ali"
      }
    ],
    "abstract": "Human-AI collaborative decision-making tools are being increasingly applied in critical domains such as healthcare. However, these tools are often seen as closed and intransparent for human decision-makers. An essential requirement for their success is the ability to provide explanations about themselves that are understandable and meaningful to the users. While explanations generally have positive connotations, studies showed that the assumption behind users interacting and engaging with these explanations could introduce trust calibration errors such as facilitating irrational or less thoughtful agreement or disagreement with the AI recommendation. In this paper, we explore how to help trust calibration through explanation interaction design. Our research method included two main phases. We first conducted a think-aloud study with 16 participants aiming to reveal main trust calibration errors concerning explainability in AI-Human collaborative decision-making tools. Then, we conducted two co-design sessions with eight participants to identify design principles and techniques for explanations that help trust calibration. As a conclusion of our research, we provide five design principles: Design for engagement, challenging habitual actions, attention guidance, friction and support training and learning. Our findings are meant to pave the way towards a more integrated framework for designing explanations with trust calibration as a primary goal.",
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "a88055f98ea9d5171e8c32ca3199cececf772fdc",
    "externalIds": {
      "DOI": "10.53759/7669/jmc202505210",
      "CorpusId": 281413216
    },
    "url": "https://www.semanticscholar.org/paper/a88055f98ea9d5171e8c32ca3199cececf772fdc",
    "title": "A Personalized and Explainable News Recommendation Framework Leveraging User Clickstreams and Content Semantics",
    "venue": "Journal of Machine and Computing",
    "year": 2025,
    "referenceCount": 18,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.53759/7669/jmc202505210?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.53759/7669/jmc202505210, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-10-05",
    "authors": [
      {
        "authorId": "2341762828",
        "name": "Kalyan Chakravarthy N S"
      },
      {
        "authorId": "2381275892",
        "name": "Muthuramya C"
      },
      {
        "authorId": "2381719466",
        "name": "Soujanya M"
      },
      {
        "authorId": "2212450613",
        "name": "Jafar Ali Ibrahim Syed Masood"
      },
      {
        "authorId": "9116000",
        "name": "Raenu A. L. Kolandaisamy"
      }
    ],
    "abstract": "MindReader introduces an interpretable and personalized news recommender system that integrates clickstream data to summarize transformer-based content semantics and provide token-level biased attribution. Unlike classical recommender systems, which often operate as black boxes, MindReader offers actionable interpretability by utilizing Shapley Additive explanations (SHAP) to reveal the word-level contributions behind each recommendation decision. This model combines both user reading history and article content extraction through a unified framework, incorporating temporal patterns and semantic embeddings. MindReader demonstrates state-of-the-art AUC and coherence scores on real-world news datasets, outperforming several strong baselines, including TF-IDF and neural content models. Human evaluation confirms its superiority. SHAP-based overlays closely align with user attention patterns, while error case analysis highlights resilience against linguistic noise and clickbait content. A key differentiator of MindReader lies in its commitment to not only achieving high performance but also ensuring transparency and trust qualities vital for the deployment of AI in sensitive areas such as journalism, education, and civic communication. This transparency allows users to not only see what content is recommended but also understand the reasoning behind it. In alignment with the UN Sustainable Development Goals (SDG 4 – Quality Education, SDG 9 – Innovation and Infrastructure, and SDG 16 – Strong Institutions), MindReader advocates for an interpretable AI framework for public information dissemination. The architecture proposed in this work offers a scalable, user-centric, and SDG-compliant approach to the implementation of explainable recommended systems.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "8eac5a4671f8e797e1f7abfae2be4e6c6c99e2bd",
    "externalIds": {
      "DOI": "10.1109/EDGE67623.2025.00027",
      "CorpusId": 280695166
    },
    "url": "https://www.semanticscholar.org/paper/8eac5a4671f8e797e1f7abfae2be4e6c6c99e2bd",
    "title": "A Personalized and Explainable Federated Learning Approach for Recommendation Systems",
    "venue": "International Conference on Edge Computing [Services Society]",
    "year": 2025,
    "referenceCount": 31,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EDGE67623.2025.00027?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EDGE67623.2025.00027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2025-07-07",
    "authors": [
      {
        "authorId": "12147027",
        "name": "Sadi Alawadi"
      },
      {
        "authorId": "17663383",
        "name": "Feras M. Awaysheh"
      },
      {
        "authorId": "2376343805",
        "name": "Thambugala Athukoralalage Jayani Sandunka Athukorala"
      },
      {
        "authorId": "2376345646",
        "name": "Saket Gande"
      },
      {
        "authorId": "11033180",
        "name": "Fahed Alkhabbas"
      }
    ],
    "abstract": "The growing adoption of wearable fitness devices and health applications has led to an exponential increase in fitness recommendations. However, privacy concerns remain significant barriers to user trust and regulatory compliance. Federated Learning (FL) offers a privacy-preserving paradigm by training models across decentralized devices without exposing raw data. However, FL introduces new challenges, including data heterogeneity, computational overhead, and the need for explainable AI (XAI). This work presents XFL, an integrated, explainable FL approach for personalized fitness recommendation systems. Our approach integrates FL with XAI techniques, SHAP, and LIME, to enhance transparency and interpretability while preserving privacy. By leveraging global and client-specific explanations, our framework empowers users to understand the rationale behind personalized recommendations, fostering trust and usability. Experimental results demonstrate that XFL performs better than centralized models while maintaining strong privacy guarantees. Furthermore, we evaluated the computational impact of integrating XAI in FL environments, providing insights into the efficiency of different explainability techniques. Our findings contribute to developing user-centric, privacy-aware, and interpretable AI-driven fitness solutions.",
    "affiliations": [],
    "countries": [],
    "num_authors": 5
  },
  {
    "paperId": "e978c8f33e75917be10447c8c889983755012cc6",
    "externalIds": {
      "DOI": "10.1109/ITIKD63574.2025.11004930",
      "CorpusId": 278779379
    },
    "url": "https://www.semanticscholar.org/paper/e978c8f33e75917be10447c8c889983755012cc6",
    "title": "AI-Enhanced Cross-Modal Anime Recommendation System with Explainable Deep Learning",
    "venue": "2024 International Conference on IT Innovation and Knowledge Discovery (ITIKD)",
    "year": 2025,
    "referenceCount": 21,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ITIKD63574.2025.11004930?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ITIKD63574.2025.11004930, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2025-04-13",
    "authors": [
      {
        "authorId": "2321383040",
        "name": "Jayabhaduri Radhakrishnan"
      },
      {
        "authorId": "2362622459",
        "name": "H. Naga"
      },
      {
        "authorId": "2362594356",
        "name": "R. G. Vardhan"
      },
      {
        "authorId": "9425580",
        "name": "Akey Sungheetha"
      },
      {
        "authorId": "2321686830",
        "name": "K. Dinesh"
      },
      {
        "authorId": "2353334012",
        "name": "K.Indu Reddy,"
      },
      {
        "authorId": "2363147176",
        "name": "K. D. Kumar"
      },
      {
        "authorId": "2362640828",
        "name": "B. C. Reddy"
      }
    ],
    "abstract": "This paper presents an innovative approach to anime recommendation systems by integrating multi-modal deep learning with explainable AI techniques. We propose a novel framework that combines visual features, textual content, and user interaction data to create more accurate and interpretable recommendations. Our system addresses key challenges in ex-isting recommendation systems, including the cold-start problem and limited content understanding, through a hybrid architecture that leverages BERT-based natural language processing and convolutional neural networks for visual analysis. Experimental results demonstrate a 27% improvement in recommendation accuracy compared to traditional methods, while providing transparent explanations for recommendations through attention visualization.",
    "affiliations": [],
    "countries": [],
    "num_authors": 8
  },
  {
    "paperId": "af92abaa9460d414559982b07d37ddc582654d6c",
    "externalIds": {
      "DOI": "10.1101/2025.02.03.25321592",
      "CorpusId": 276157741,
      "PubMed": "40107091"
    },
    "url": "https://www.semanticscholar.org/paper/af92abaa9460d414559982b07d37ddc582654d6c",
    "title": "Development of an artificial intelligence-generated, explainable treatment recommendation system for urothelial carcinoma and renal cell carcinoma to support multidisciplinary cancer conferences",
    "venue": "medRxiv",
    "year": 2025,
    "referenceCount": 19,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2025.02.03.25321592?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2025.02.03.25321592, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-02-06",
    "authors": [
      {
        "authorId": "2238763867",
        "name": "G. Duwe"
      },
      {
        "authorId": "2321582460",
        "name": "D. Mercier"
      },
      {
        "authorId": "2308244131",
        "name": "V. Kauth"
      },
      {
        "authorId": "2308244200",
        "name": "K. Moench"
      },
      {
        "authorId": "1696614145",
        "name": "Vikas Rajashekar"
      },
      {
        "authorId": "2308244191",
        "name": "Markus Junker"
      },
      {
        "authorId": "2205389703",
        "name": "A. Dengel"
      },
      {
        "authorId": "2238765725",
        "name": "A. Haferkamp"
      },
      {
        "authorId": "6942928",
        "name": "T. Höfner"
      }
    ],
    "abstract": "Background and Objective: Decisions on the best available treatment in clinical oncology are based on expert opinions in multidisciplinary cancer conferences (MCC). Artificial intelligence (AI) could increase evidence-based treatment by generating additional treatment recommendations (TR). We aimed to develop such an AI system for urothelial carcinoma (UC) and renal cell carcinoma (RCC). Methods: Comprehensive data of patients with histologically confirmed UC and RCC who received MCC recommendations in the years 2015 to 2022 were transformed into machine readable representations. Development of a two-step process to train a classifier to mimic TR. Identification of superordinate categories of recommendations followed by specification of detailed TR. Machine learning (CatBoost, XGBoost, Random Forest) and deep learning (TabPFN, TabNet, SoftOrdering CNN, FCN) techniques were trained. Results were measured by F1-scores for accuracy weights. Additionally, clinical trial data for drugs were included. Key Findings and Limitations: AI training was performed with 1617 (UC) and 880 (RCC) MCC recommendations (77 and 76 patient input parameters). AI system generated fully automated TR with excellent F1-scores for UC (e.g. Surgery 0.81, Anti-cancer drug 0.83, Gemcitabine/Cisplatin 0.88) and RCC (e.g. Anti-cancer drug 0.92 Nivolumab 0.78, Pembrolizumab/Axitinib 0.89). Explainability is provided by clinical features and their importance score. TR and explainability were visualized on a dashboard. Main limitations: single-centre and retrospective study. Conclusions and Clinical Implications: First AI-generated explainable TR in UC and RCC with excellent performance results. Potential support tool for high-quality, evidence-based TR in MCC. Study sets global reference standards for AI development in MCC recommendations in clinical oncology.",
    "affiliations": [],
    "countries": [],
    "num_authors": 9
  },
  {
    "paperId": "71a2503540229315188d53acd4b2bb1e2f61242c",
    "externalIds": {
      "MAG": "2959955824",
      "DBLP": "conf/hci/SeguraBFB19",
      "DOI": "10.1007/978-3-030-23541-3_28",
      "CorpusId": 196613745
    },
    "url": "https://www.semanticscholar.org/paper/71a2503540229315188d53acd4b2bb1e2f61242c",
    "title": "Towards Explainable AI Using Similarity: An Analogues Visualization System",
    "venue": "Interacción",
    "year": 2019,
    "referenceCount": 8,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-030-23541-3_28?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-030-23541-3_28, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2019-07-26",
    "authors": [
      {
        "authorId": "33713332",
        "name": "Vinicius Segura"
      },
      {
        "authorId": "2076970127",
        "name": "Bruna Brandão"
      },
      {
        "authorId": "8763940",
        "name": "Ana Fucs"
      },
      {
        "authorId": "2178217",
        "name": "E. V. Brazil"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 4
  },
  {
    "paperId": "e42b9cf5367c882381746dac1767ee747a97217a",
    "externalIds": {
      "DOI": "10.30574/ijsra.2025.14.1.0061",
      "CorpusId": 275555803
    },
    "url": "https://www.semanticscholar.org/paper/e42b9cf5367c882381746dac1767ee747a97217a",
    "title": "Evolution of recommendation systems in the age of Generative AI",
    "venue": "International Journal of Science and Research Archive",
    "year": 2025,
    "referenceCount": 15,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.30574/ijsra.2025.14.1.0061?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.30574/ijsra.2025.14.1.0061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-01-30",
    "authors": [
      {
        "authorId": "2383992695",
        "name": "Ankur Aggarwal"
      }
    ],
    "abstract": "This article examines the transformative evolution of recommendation systems in the era of Generative AI, exploring how these advanced technologies have revolutionized user experience and business outcomes across digital platforms. The article investigates the transition from traditional rule-based approaches to sophisticated model-based systems, highlighting the impact of deep learning technologies, explainable AI mechanisms, and multimodal integration. Through comprehensive analysis of recent developments, the article demonstrates how Generative AI has enhanced personalization capabilities, improved recommendation accuracy, and enabled more contextually relevant suggestions while addressing crucial aspects of user privacy and system transparency. The article encompasses various domains, including e-commerce, content streaming, and digital marketplaces, offering insights into both technical advancements and practical implementations of modern recommendation systems.",
    "affiliations": [],
    "countries": [],
    "num_authors": 1
  },
  {
    "paperId": "eccaae7d657b436a44c0c2a5693c591454b16c98",
    "externalIds": {
      "CorpusId": 251437625
    },
    "url": "https://www.semanticscholar.org/paper/eccaae7d657b436a44c0c2a5693c591454b16c98",
    "title": "Issues in Explainable AI: Blackboxes, Recommendations, and Levels of Explanation",
    "venue": "",
    "year": 2019,
    "referenceCount": 0,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null
    },
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "80378192",
        "name": "Rune Nyrup"
      }
    ],
    "abstract": null,
    "affiliations": [],
    "countries": [],
    "num_authors": 1
  }
]