<!DOCTYPE html>
<html lang="en" class="no-js">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="applicable-device" content="pc,mobile">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        
        
        
            <meta name="robots" content="max-image-preview:large">
            <meta name="access" content="Yes">

        
        <meta name="360-site-verification" content="1268d79b5e96aecf3ff2a7dac04ad990" />

        <title>Explainable AI for Text Classification: Lessons from a Comprehensive Evaluation of Post Hoc Methods | Cognitive Computation</title>

        
            
    
    <meta name="twitter:site" content="@SpringerLink"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:title" content="Explainable AI for Text Classification: Lessons from a Comprehensive Evaluation of Post Hoc Methods"/>
    <meta name="twitter:description" content="Cognitive Computation - This paper addresses the notable gap in evaluating eXplainable Artificial Intelligence (XAI) methods for text classification. While existing frameworks focus on assessing..."/>
    <meta name="twitter:image" content="https://static-content.springer.com/image/art%3A10.1007%2Fs12559-024-10325-w/MediaObjects/12559_2024_10325_Fig1_HTML.png"/>
    <meta name="journal_id" content="12559"/>
    <meta name="dc.title" content="Explainable AI for Text Classification: Lessons from a Comprehensive Evaluation of Post Hoc Methods"/>
    <meta name="dc.source" content="Cognitive Computation 2024 16:6"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="Springer"/>
    <meta name="dc.date" content="2024-08-06"/>
    <meta name="dc.type" content="ReviewPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2024 The Author(s)"/>
    <meta name="dc.rights" content="2024 The Author(s)"/>
    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="dc.description" content="This paper addresses the notable gap in evaluating eXplainable Artificial Intelligence (XAI) methods for text classification. While existing frameworks focus on assessing XAI in areas such as recommender systems and visual analytics, a comprehensive evaluation is missing. Our study surveys and categorises recent post hoc XAI methods according to their scope of explanation and output format. We then conduct a systematic evaluation, assessing the effectiveness of these methods across varying scopes and levels of output granularity using a combination of objective metrics and user studies. Key findings reveal that feature-based explanations exhibit higher fidelity than rule-based ones. While global explanations are perceived as more satisfying and trustworthy, they are less practical than local explanations. These insights enhance understanding of XAI in text classification and offer valuable guidance for developing effective XAI systems, enabling users to evaluate each explainer&#8217;s pros and cons and select the most suitable one for their needs."/>
    <meta name="prism.issn" content="1866-9964"/>
    <meta name="prism.publicationName" content="Cognitive Computation"/>
    <meta name="prism.publicationDate" content="2024-08-06"/>
    <meta name="prism.volume" content="16"/>
    <meta name="prism.number" content="6"/>
    <meta name="prism.section" content="ReviewPaper"/>
    <meta name="prism.startingPage" content="3077"/>
    <meta name="prism.endingPage" content="3095"/>
    <meta name="prism.copyright" content="2024 The Author(s)"/>
    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s12559-024-10325-w"/>
    <meta name="prism.doi" content="doi:10.1007/s12559-024-10325-w"/>
    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s12559-024-10325-w.pdf"/>
    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s12559-024-10325-w"/>
    <meta name="citation_journal_title" content="Cognitive Computation"/>
    <meta name="citation_journal_abbrev" content="Cogn Comput"/>
    <meta name="citation_publisher" content="Springer US"/>
    <meta name="citation_issn" content="1866-9964"/>
    <meta name="citation_title" content="Explainable AI for Text Classification: Lessons from a Comprehensive Evaluation of Post Hoc Methods"/>
    <meta name="citation_volume" content="16"/>
    <meta name="citation_issue" content="6"/>
    <meta name="citation_publication_date" content="2024/11"/>
    <meta name="citation_online_date" content="2024/08/06"/>
    <meta name="citation_firstpage" content="3077"/>
    <meta name="citation_lastpage" content="3095"/>
    <meta name="citation_article_type" content="Review"/>
    <meta name="citation_fulltext_world_readable" content=""/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1007/s12559-024-10325-w"/>
    <meta name="DOI" content="10.1007/s12559-024-10325-w"/>
    <meta name="size" content="286034"/>
    <meta name="citation_doi" content="10.1007/s12559-024-10325-w"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1007/s12559-024-10325-w&amp;api_key="/>
    <meta name="description" content="This paper addresses the notable gap in evaluating eXplainable Artificial Intelligence (XAI) methods for text classification. While existing frameworks foc"/>
    <meta name="dc.creator" content="Cesarini, Mirko"/>
    <meta name="dc.creator" content="Malandri, Lorenzo"/>
    <meta name="dc.creator" content="Pallucchini, Filippo"/>
    <meta name="dc.creator" content="Seveso, Andrea"/>
    <meta name="dc.creator" content="Xing, Frank"/>
    <meta name="dc.subject" content="Artificial Intelligence"/>
    <meta name="dc.subject" content="Computation by Abstract Devices"/>
    <meta name="dc.subject" content="Computational Biology/Bioinformatics"/>
    <meta name="citation_reference" content="citation_journal_title=Sci Robot; citation_title=XAI&#8212;explainable artificial intelligence; citation_author=D Gunning, M Stefik, J Choi, T Miller, S Stumpf, G-Z Yang; citation_volume=4; citation_issue=37; citation_publication_date=2019; citation_pages=7120; citation_doi=10.1126/scirobotics.aay7120; citation_id=CR1"/>
    <meta name="citation_reference" content="citation_journal_title=Knowl Based Syst; citation_title=XAI for myo-controlled prosthesis: Explaining EMG data for hand gesture classification; citation_author=N Gozzi, L Malandri, F Mercorio, A Pedrocchi; citation_volume=240; citation_publication_date=2022; citation_pages=108053; citation_doi=10.1016/j.knosys.2021.108053; citation_id=CR2"/>
    <meta name="citation_reference" content="Xing F, Malandri L, Zhang Y, Cambria E. Financial sentiment analysis: An investigation into common mistakes and silver bullets. In: Proceedings of the 28th International Conference on Computational Linguistics. 2020. pp. 978&#8211;87."/>
    <meta name="citation_reference" content="citation_journal_title=Cogn Comput; citation_title=Interpreting black-box models: a review on explainable artificial intelligence; citation_author=V Hassija, V Chamola, A Mahapatra, A Singal, D Goel, K Huang, S Scardapane, I Spinelli, M Mahmud, A Hussain; citation_volume=16; citation_issue=1; citation_publication_date=2024; citation_pages=45-74; citation_doi=10.1007/s12559-023-10179-8; citation_id=CR4"/>
    <meta name="citation_reference" content="citation_journal_title=Decis Support Syst; citation_title=Model-contrastive explanations through symbolic reasoning; citation_author=L Malandri, F Mercorio, M Mezzanzanica, A Seveso; citation_volume=176; citation_publication_date=2024; citation_pages=114040; citation_doi=10.1016/j.dss.2023.114040; citation_id=CR5"/>
    <meta name="citation_reference" content="citation_journal_title=Inf Process Manag; citation_title=A survey on XAI and natural language explanations; citation_author=E Cambria, L Malandri, F Mercorio, M Mezzanzanica, N Nobani; citation_volume=60; citation_issue=1; citation_publication_date=2023; citation_pages=103111; citation_doi=10.1016/j.ipm.2022.103111; citation_id=CR6"/>
    <meta name="citation_reference" content="citation_journal_title=Inf Fusion; citation_title=Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI; citation_author=AB Arrieta, N D&#237;az-Rodr&#237;guez, J Ser, A Bennetot, S Tabik, A Barbado, S Garc&#237;a, S Gil-L&#243;pez, D Molina, R Benjamins; citation_volume=58; citation_publication_date=2020; citation_pages=82-115; citation_doi=10.1016/j.inffus.2019.12.012; citation_id=CR7"/>
    <meta name="citation_reference" content="citation_journal_title=ACM Trans Intell Syst Technol (TIST); citation_title=A survey on text classification: From traditional to deep learning; citation_author=Q Li, H Peng, J Li, C Xia, R Yang, L Sun, PS Yu, L He; citation_volume=13; citation_issue=2; citation_publication_date=2022; citation_pages=1-41; citation_id=CR8"/>
    <meta name="citation_reference" content="citation_journal_title=ACM Comput Surv (CSUR); citation_title=Deep learning-based text classification: a comprehensive review; citation_author=S Minaee, N Kalchbrenner, E Cambria, N Nikzad, M Chenaghlu, J Gao; citation_volume=54; citation_issue=3; citation_publication_date=2021; citation_pages=1-40; citation_doi=10.1145/3439726; citation_id=CR9"/>
    <meta name="citation_reference" content="Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56&#8211;67."/>
    <meta name="citation_reference" content="citation_journal_title=J Artif Intell Res; citation_title=A survey on the explainability of supervised machine learning; citation_author=N Burkart, MF Huber; citation_volume=70; citation_publication_date=2021; citation_pages=245-317; citation_doi=10.1613/jair.1.12228; citation_id=CR11"/>
    <meta name="citation_reference" content="citation_journal_title=ACM Trans Interact Intell Syst; citation_title=A multidisciplinary survey and framework for design and evaluation of explainable ai systems; citation_author=S Mohseni, N Zarei, ED Ragan; citation_volume=11; citation_issue=3&#8211;4; citation_publication_date=2021; citation_pages=1-45; citation_doi=10.1145/3387166; citation_id=CR12"/>
    <meta name="citation_reference" content="citation_journal_title=Electronics; citation_title=Evaluating the quality of machine learning explanations: A survey on methods and metrics; citation_author=J Zhou, AH Gandomi, F Chen, A Holzinger; citation_volume=10; citation_issue=5; citation_publication_date=2021; citation_pages=593; citation_doi=10.3390/electronics10050593; citation_id=CR13"/>
    <meta name="citation_reference" content="citation_journal_title=ACM Trans Manag Inf Syst; citation_title=Incorporating multiple knowledge sources for targeted aspect-based financial sentiment analysis; citation_author=K Du, F Xing, E Cambria; citation_volume=14; citation_issue=3; citation_publication_date=2023; citation_pages=23; citation_doi=10.1145/3580480; citation_id=CR14"/>
    <meta name="citation_reference" content="Keele S, et al. Guidelines for performing systematic literature reviews in software engineering. Technical Report, ver. 2.3 ebse technical report. ebse. 2007."/>
    <meta name="citation_reference" content="Guidotti R, Monreale A, Ruggieri S, Pedreschi D, Turini F, Giannotti F. Local rule-based explanations of black box decision systems. 
                  arXiv:1805.10820
                  
                 [Preprint]. 2018. Available from: 
                  http://arxiv.org/abs/1805.10820
                  
                ."/>
    <meta name="citation_reference" content="Craven M, Shavlik J. Extracting tree-structured representations of trained networks. Adv Neural Inf Process Syst. 1995;8."/>
    <meta name="citation_reference" content="citation_journal_title=Adv Neural Inf Process Syst.; citation_title=Understanding global feature contributions with additive importance measures; citation_author=I Covert, SM Lundberg, S-I Lee; citation_volume=33; citation_publication_date=2020; citation_pages=17212-23; citation_id=CR18"/>
    <meta name="citation_reference" content="Dhurandhar A, Shanmugam K, Luss R, Olsen PA. Improving simple models with confidence profiles. Adv Neural Inf Process Syst. 2018;31."/>
    <meta name="citation_reference" content="Wei D, Dash S, Gao T, Gunluk O. Generalized linear rule models. In: International Conference on Machine Learning. PMLR; 2019. pp. 6687&#8211;96."/>
    <meta name="citation_reference" content="Sushil M, &#352;uster S, Daelemans W. Rule induction for global explanation of trained models. In: Analyzing and Interpreting Neural Networks for NLP (BlackBoxNLP), Workshop at EMNLP. 2018. pp. 82&#8211;97."/>
    <meta name="citation_reference" content="Ribeiro MT, Singh S, Guestrin C. &#8220;Why should I trust you?&#8221; Explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2016. pp. 1135&#8211;44."/>
    <meta name="citation_reference" content="Lundberg SM, Lee S-I. A unified approach to interpreting model predictions. Adv Neural Inf Process Syst. 2017;30."/>
    <meta name="citation_reference" content="van der Waa J, Robeer M, van Diggelen J, Brinkhuis M. Neerincx M. Contrastive explanations with local foil trees. In: Proceedings of the ICML Workshop on Human Interpretability in Machine Learning (WHI 2018), Stockholm, Sweden. 2018. p. 37."/>
    <meta name="citation_reference" content="Elenberg E, Dimakis AG, Feldman M, Karbasi A. Streaming weak submodularity: Interpreting neural networks on the fly. Adv Neural Inf Process Syst. 2017;30."/>
    <meta name="citation_reference" content="Lei T, Barzilay R, Jaakkola T. Rationalizing neural predictions. In: Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. 2016. pp. 107&#8211;17."/>
    <meta name="citation_reference" content="Kim B, Wattenberg M, Gilmer J, Cai C, Wexler J, Viegas F, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In: International Conference on Machine Learning. PMLR; 2018. pp. 2668&#8211;77."/>
    <meta name="citation_reference" content="Ribeiro MT, Wu T, Guestrin C, Singh S. Beyond accuracy: Behavioral testing of NLP models with checklist. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020. pp. 4902&#8211;12."/>
    <meta name="citation_reference" content="Datta A, Sen S, Zick Y. Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. In: 2016 IEEE Symposium on Security and Privacy (SP). IEEE; 2016. pp. 598&#8211;617."/>
    <meta name="citation_reference" content="Hind M, Wei D, Campbell M, Codella NC, Dhurandhar A, Mojsilovi&#263; A, Natesan&#160;Ramamurthy K, Varshney KR. Ted: Teaching AI to explain its decisions. In: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 2019. pp. 123&#8211;9."/>
    <meta name="citation_reference" content="Staniak M, Biecek P. Explanations of model predictions with live and breakdown packages. R J. 2018;10(2)."/>
    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=Classifier-agnostic saliency map extraction; citation_author=K Zolna, KJ Geras, K Cho; citation_volume=196; citation_publication_date=2020; citation_pages=102969; citation_doi=10.1016/j.cviu.2020.102969; citation_id=CR32"/>
    <meta name="citation_reference" content="Dash S, Gunluk O, Wei D. Boolean decision rules via column generation. Adv Neural Inf Process Syst. 2018;31."/>
    <meta name="citation_reference" content="Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra D. Grad-cam: Visual explanations from deep networks via gradient-based localization. In: Proceedings of the IEEE International Conference on Computer Vision. 2017. pp. 618&#8211;26."/>
    <meta name="citation_reference" content="Singh C, Murdoch WJ, Yu B. Hierarchical interpretations for neural network predictions. In: International Conference on Learning Representations. 2018."/>
    <meta name="citation_reference" content="Dhurandhar A, Chen P-Y, Luss R, Tu C-C, Ting P, Shanmugam K, Das P. Explanations based on the missing: Towards contrastive explanations with pertinent negatives. Adv Neural Inf Process Syst. 2018;31."/>
    <meta name="citation_reference" content="Shrikumar A, Greenside P, Kundaje A. Learning important features through propagating activation differences. In: International Conference on Machine Learning. PMLR; 2017. pp. 3145&#8211;53."/>
    <meta name="citation_reference" content="citation_journal_title=J Mach Learn Res; citation_title=The LRP toolbox for artificial neural networks; citation_author=S Lapuschkin, A Binder, G Montavon, K-R M&#252;ller, W Samek; citation_volume=17; citation_issue=114; citation_publication_date=2016; citation_pages=1-5; citation_id=CR38"/>
    <meta name="citation_reference" content="Hu L, Jian S, Cao L, Chen Q. Interpretable recommendation via attraction modeling: Learning multilevel attractiveness over multimodal movie contents. In: IJCAI International Joint Conference on Artificial Intelligence. 2018."/>
    <meta name="citation_reference" content="Petsiuk V, Das A. Saenko K. Rise: Randomized input sampling for explanation of black-box models. In: Proceedings of the British Machine Vision Conference (BMVC). 2018."/>
    <meta name="citation_reference" content="citation_journal_title=J Mach Learn Res; citation_title=A bayesian framework for learning rule sets for interpretable classification; citation_author=T Wang, C Rudin, F Doshi-Velez, Y Liu, E Klampfl, P MacNeille; citation_volume=18; citation_issue=70; citation_publication_date=2017; citation_pages=1-37; citation_id=CR41"/>
    <meta name="citation_reference" content="Ribeiro MT, Singh S, Guestrin C. Anchors: High-precision model-agnostic explanations. In: Proceedings of the AAAI Conference on Artificial Intelligence (vol. 32). 2018."/>
    <meta name="citation_reference" content="Hong D, Wang T, Baek S. Protorynet - interpretable text classification via prototype trajectories. J Mach Learn Res. 2023;24(264):1&#8211;39."/>
    <meta name="citation_reference" content="citation_title=The co-12 recipe for evaluating interpretable part-prototype image classifiers; citation_inbook_title=Explainable Artificial Intelligence; citation_publication_date=2023; citation_pages=397-420; citation_id=CR44; citation_author=M Nauta; citation_author=C Seifert; citation_publisher=Springer"/>
    <meta name="citation_reference" content="Datta P, Kibler D. Learning prototypical concept descriptions. In: Machine Learning Proceedings 1995. 1995. pp. 158&#8211;66."/>
    <meta name="citation_reference" content="Wang F, Rudin C. Falling rule lists. In: Artificial Intelligence and Statistics. PMLR; 2015. pp. 1013&#8211;22."/>
    <meta name="citation_reference" content="Mothilal RK, Sharma A, Tan C. Explaining machine learning classifiers through diverse counterfactual explanations. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 607&#8211;17."/>
    <meta name="citation_reference" content="Longo L, Brcic M, Cabitza F, Choi J, Confalonieri R, Ser JD, Guidotti R, Hayashi Y, Herrera F, Holzinger A, Jiang R, Khosravi H, Lecue F, Malgieri G, P&#225;ez A, Samek W, Schneider J, Speith T, Stumpf S. Explainable artificial intelligence (XAI) 2.0: A manifesto of open challenges and interdisciplinary research directions. Inf Fusion. 2024;106:102301. 
                  https://doi.org/10.1016/j.inffus.2024.102301
                  
                ."/>
    <meta name="citation_reference" content="Vilone G, Rizzo L, Longo L. A comparative analysis of rule-based, model-agnostic methods for explainable artificial intelligence. 2020."/>
    <meta name="citation_reference" content="citation_journal_title=Front Artif Intell; citation_title=A quantitative evaluation of global, rule-based explanations of post-hoc, model agnostic methods; citation_author=G Vilone, L Longo; citation_volume=4; citation_publication_date=2021; citation_pages=717899; citation_doi=10.3389/frai.2021.717899; citation_id=CR50"/>
    <meta name="citation_reference" content="Belaid MK, Bornemann R, Rabus M, Krestel R, H&#252;llermeier E. Compare-XAI: Toward unifying functional testing methods for post-hoc XAI algorithms into a multi-dimensional benchmark. In: World Conference on Explainable Artificial Intelligence. Springer; 2023. pp. 88&#8211;109."/>
    <meta name="citation_reference" content="Rasouli P, Yu IC. Explan: Explaining black-box classifiers using adaptive neighborhood generation. In: 2020 International Joint Conference on Neural Networks (IJCNN). IEEE; 2020. pp. 1&#8211;9."/>
    <meta name="citation_reference" content="Dwivedi R, Dave D, Naik H, Singhal S, Omer R, Patel P, Qian B, Wen Z, Shah T, Morgan G, Ranjan R. Explainable AI (XAI): Core ideas, techniques, and solutions. ACM Comput Surv. 2023;55(9). 
                  https://doi.org/10.1145/3561048
                  
                ."/>
    <meta name="citation_reference" content="citation_journal_title=Data Min Knowl Disc; citation_title=A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts; citation_author=G Schwalbe, B Finzel; citation_volume=1; citation_publication_date=2023; citation_pages=1-59; citation_id=CR54"/>
    <meta name="citation_reference" content="citation_journal_title=Knowl Based Syst; citation_title=Explainable AI (XAI): A systematic meta-survey of current challenges and future opportunities; citation_author=W Saeed, C Omlin; citation_volume=263; citation_publication_date=2023; citation_pages=110273; citation_doi=10.1016/j.knosys.2023.110273; citation_id=CR55"/>
    <meta name="citation_reference" content="citation_journal_title=Hum Centric Intell Syst; citation_title=Survey on explainable AI: From approaches, limitations and applications aspects; citation_author=W Yang, Y Wei, H Wei, Y Chen, G Huang, X Li, R Li, N Yao, X Wang, X Gu; citation_volume=3; citation_issue=3; citation_publication_date=2023; citation_pages=161-188; citation_doi=10.1007/s44230-023-00038-y; citation_id=CR56"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Towards human-centered explainable AI: A survey of user studies for model explanations; citation_author=Y Rong, T Leemann, T-T Nguyen, L Fiedler, P Qian, V Unhelkar, T Seidel, G Kasneci, E Kasneci; citation_volume=46; citation_issue=4; citation_publication_date=2024; citation_pages=2104-2122; citation_doi=10.1109/TPAMI.2023.3331846; citation_id=CR57"/>
    <meta name="citation_reference" content="Fauvel K, Masson V, Fromont E. A performance-explainability framework to benchmark machine learning methods: Application to multivariate time series classifiers. In: Proceedings of the IJCAI-PRICAI 2020 Workshop on Explainable AI. 2021. pp. 1&#8211;8."/>
    <meta name="citation_reference" content="citation_journal_title=Inf Fusion; citation_title=Notions of explainability and evaluation approaches for explainable artificial intelligence; citation_author=G Vilone, L Longo; citation_volume=76; citation_publication_date=2021; citation_pages=89-106; citation_doi=10.1016/j.inffus.2021.05.009; citation_id=CR59"/>
    <meta name="citation_reference" content="Keane MT, Kenny EM, Delaney E, Smyth B. If only we had better counterfactual explanations: Five key deficits to rectify in the evaluation of counterfactual XAI techniques. In: IJCAI. 2021. pp. 4467&#8211;74."/>
    <meta name="citation_reference" content="citation_journal_title=Artif Intell; citation_title=Evaluating XAI: a comparison of rule-based and example-based explanations; citation_author=J Waa, E Nieuwburg, A Cremers, M Neerincx; citation_volume=291; citation_publication_date=2021; citation_pages=103404; citation_doi=10.1016/j.artint.2020.103404; citation_id=CR61"/>
    <meta name="citation_reference" content="Yeh C-K, Hsieh C-Y, Suggala A, Inouye DI, Ravikumar PK. On the (in) fidelity and sensitivity of explanations. Adv Neural Inf Process Syst. 2019;32."/>
    <meta name="citation_reference" content="Bhatt U, Weller A, Moura JM. Evaluating and aggregating feature-based model explanations. 
                  arXiv:2005.00631
                  
                 [Preprint]. 2020. Available from: 
                  http://arxiv.org/abs/2005.00631
                  
                ."/>
    <meta name="citation_reference" content="Ma E. NLP Augmentation. 2019. 
                  https://github.com/makcedward/nlpaug
                  
                ."/>
    <meta name="citation_reference" content="Maas A, Daly RE, Pham PT, Huang D, Ng AY, Potts C. Learning word vectors for sentiment analysis. In: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. 2011. pp. 142&#8211;50."/>
    <meta name="citation_reference" content="Kumar H, Harish B, Darshan H. Sentiment analysis on imdb movie reviews using hybrid feature extraction method. Int J Interact Multimed Artif Intell. 2019;5(5)."/>
    <meta name="citation_reference" content="Bird S, Klein E, Loper E. Natural Language Processing with Python: Analyzing text with the natural language toolkit. 2009."/>
    <meta name="citation_reference" content="Li Q, Peng H, Li J, Xia C, Yang R, Sun L, Yu PS, He L. A survey on text classification: From traditional to deep learning. ACM Trans Intell Syst Technol. 2022;13(2). 
                  https://doi.org/10.1145/3495162
                  
                ."/>
    <meta name="citation_reference" content="citation_journal_title=Inf Syst; citation_title=Is text preprocessing still worth the time? A comparative survey on the influence of popular preprocessing methods on transformers and traditional classifiers; citation_author=M Siino, I Tinnirello, M Cascia; citation_volume=121; citation_publication_date=2024; citation_pages=102342; citation_doi=10.1016/j.is.2023.102342; citation_id=CR69"/>
    <meta name="citation_reference" content="Minaee S, Kalchbrenner N, Cambria E, Nikzad N, Chenaghlu M, Gao J. Deep learning-based text classification: A comprehensive review. ACM Comput Surv. 2021;54(3). 
                  https://doi.org/10.1145/3439726
                  
                ."/>
    <meta name="citation_reference" content="Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V, Vanderplas J, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay E. Scikit-learn: Machine learning in Python. J Mach Learn Res. 2011;12:2825&#8211;30."/>
    <meta name="citation_reference" content="Breiman L. Classification and regression trees. 2017."/>
    <meta name="citation_reference" content="citation_journal_title=Am Stat.; citation_title=Violin plots: a box plot-density trace synergism; citation_author=JL Hintze, RD Nelson; citation_volume=52; citation_issue=2; citation_publication_date=1998; citation_pages=181-4; citation_doi=10.1080/00031305.1998.10480559; citation_id=CR73"/>
    <meta name="citation_author" content="Cesarini, Mirko"/>
    <meta name="citation_author_email" content="cesarini.mirko@unimib.it"/>
    <meta name="citation_author_institution" content="Department of Statistics and Quantitative Methods, University of Milan-Bicocca, Milan, Italy"/>
    <meta name="citation_author" content="Malandri, Lorenzo"/>
    <meta name="citation_author_email" content="lorenzo.malandri@unimib.it"/>
    <meta name="citation_author_institution" content="Department of Statistics and Quantitative Methods, University of Milan-Bicocca, Milan, Italy"/>
    <meta name="citation_author" content="Pallucchini, Filippo"/>
    <meta name="citation_author_email" content="filippo.pallucchini@unimib.it"/>
    <meta name="citation_author_institution" content="Department of Statistics and Quantitative Methods, University of Milan-Bicocca, Milan, Italy"/>
    <meta name="citation_author" content="Seveso, Andrea"/>
    <meta name="citation_author_email" content="andrea.seveso@unimib.it"/>
    <meta name="citation_author_institution" content="Department of Statistics and Quantitative Methods, University of Milan-Bicocca, Milan, Italy"/>
    <meta name="citation_author" content="Xing, Frank"/>
    <meta name="citation_author_email" content="xing@nus.edu.sg"/>
    <meta name="citation_author_institution" content="School of Computing, National University of Singapore, Singapore, Singapore"/>
    <meta name="format-detection" content="telephone=no"/>
    <meta name="citation_cover_date" content="2024/11/01"/>
    

            
    
    <meta property="og:url" content="https://link.springer.com/article/10.1007/s12559-024-10325-w"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="SpringerLink"/>
    <meta property="og:title" content="Explainable AI for Text Classification: Lessons from a Comprehensive Evaluation of Post Hoc Methods - Cognitive Computation"/>
    <meta property="og:description" content="This paper addresses the notable gap in evaluating eXplainable Artificial Intelligence (XAI) methods for text classification. While existing frameworks focus on assessing XAI in areas such as recommender systems and visual analytics, a comprehensive evaluation is missing. Our study surveys and categorises recent post hoc XAI methods according to their scope of explanation and output format. We then conduct a systematic evaluation, assessing the effectiveness of these methods across varying scopes and levels of output granularity using a combination of objective metrics and user studies. Key findings reveal that feature-based explanations exhibit higher fidelity than rule-based ones. While global explanations are perceived as more satisfying and trustworthy, they are less practical than local explanations. These insights enhance understanding of XAI in text classification and offer valuable guidance for developing effective XAI systems, enabling users to evaluate each explainer&#8217;s pros and cons and select the most suitable one for their needs."/>
    <meta property="og:image" content="https://static-content.springer.com/image/art%3A10.1007%2Fs12559-024-10325-w/MediaObjects/12559_2024_10325_Fig1_HTML.png"/>
    

        

        <meta name="format-detection" content="telephone=no">

        
    
        
    
    
    

    


        <link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/img/favicons/darwin/apple-touch-icon-6ef0829b9c.png>
<link rel="icon" type="image/png" sizes="192x192" href=/oscar-static/img/favicons/darwin/android-chrome-192x192.png>
<link rel="icon" type="image/png" sizes="32x32" href=/oscar-static/img/favicons/darwin/favicon-32x32.png>
<link rel="icon" type="image/png" sizes="16x16" href=/oscar-static/img/favicons/darwin/favicon-16x16.png>
<link rel="shortcut icon" data-test="shortcut-icon" href=/oscar-static/img/favicons/darwin/favicon-de0c289efe.ico>

<meta name="theme-color" content="#e6e6e6">


        



<link rel="stylesheet" media="print" href=/oscar-static/app-springerlink/css/print-b8af42253b.css>



    
        
            
    <style> html{line-height:1.15;text-size-adjust:100%}body{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.8;margin:0}details,main{display:block}h1{font-size:2em;margin:.67em 0}a{background-color:transparent;color:#025e8d}b{font-weight:bolder}sub{bottom:-.25em;font-size:75%;line-height:0;position:relative;vertical-align:baseline}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input{font-family:inherit;font-size:100%;line-height:1.15;margin:0;overflow:visible}button{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}summary{display:list-item}[hidden]{display:none}button{cursor:pointer}svg{height:1rem;width:1rem}.eds-c-header__brand img{max-width:100%}@media only screen and (min-width:768px){.eds-c-header__brand img{max-width:340px}} </style>
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  body{background:#fff;color:#222;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.8;min-height:100%}a{color:#025e8d;padding:initial;text-decoration:underline;text-decoration-skip-ink:auto}button{cursor:pointer}img{border:0;height:auto;max-width:100%;vertical-align:middle}html{box-sizing:border-box;font-size:100%;height:100%;overflow-y:scroll}h1{font-size:2.25rem}h2{font-size:1.75rem}h1,h2,h3{font-weight:700;line-height:1.2}h3{font-size:1.5rem}body{font-size:1.125rem}*{box-sizing:inherit}p{margin-bottom:2rem;margin-top:0}p:last-of-type{margin-bottom:0}.c-ad{text-align:center}@media only screen and (min-width:480px){.c-ad{padding:8px}}.c-ad--728x90{display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:876px){.js .c-ad--728x90{display:none}}.c-ad__label,.c-status-message{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-ad__label{color:#333;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-status-message{align-items:center;box-sizing:border-box;display:flex;position:relative;width:100%}.c-status-message--boxed{background-color:#fff;border:1px solid #ccc;line-height:1.4;padding:16px}.c-status-message__heading{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.c-status-message :last-child{margin-bottom:0}.eds-c-button{border-radius:32px;cursor:pointer;display:inline-block;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;font-weight:700;line-height:1.5;margin:0;padding:.5rem 1.5rem;position:relative;text-align:center;text-decoration:none;transition:all .2s ease 0s;width:100%}.eds-c-button span,.eds-c-button svg{vertical-align:middle}.eds-c-button svg{height:1.5rem;width:1.5rem}.eds-c-button svg:first-child{margin-right:8px}@media only screen and (min-width:480px){.eds-c-button{width:auto}}.eds-c-button--primary{background-color:#025e8d;background-image:none;border:2px solid transparent;box-shadow:none;color:#fff;text-decoration:none}.eds-c-button--primary svg{fill:currentcolor}.eds-c-header{background-color:#fff;border-bottom:2px solid #01324b;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;line-height:1.5;padding:8px 0 0}.eds-c-header__container{align-items:center;display:flex;flex-wrap:nowrap;gap:8px 16px;justify-content:space-between;margin:0 auto 8px;max-width:1280px;padding:0 8px;position:relative}.eds-c-header__nav{border-top:2px solid #c5e0f4;padding-top:4px;position:relative}.eds-c-header__nav-container{align-items:center;display:flex;flex-wrap:wrap;margin:0 auto 4px;max-width:1280px;padding:0 8px;position:relative}.eds-c-header__nav-container>:not(:last-child){margin-right:32px}.eds-c-header__link-container{align-items:center;display:flex;flex:1 0 auto;gap:8px 16px;justify-content:space-between}.eds-c-header__list{list-style:none;margin:0;padding:0}.eds-c-header__list-item{font-weight:700;margin:0 auto;max-width:1280px;padding:8px}.eds-c-header__list-item:not(:last-child){border-bottom:2px solid #c5e0f4}.eds-c-header__item{color:inherit}@media only screen and (min-width:768px){.eds-c-header__item--menu{display:none;visibility:hidden}.eds-c-header__item--menu:first-child+*{margin-block-start:0}}.eds-c-header__item--inline-links{display:none;visibility:hidden}@media only screen and (min-width:768px){.eds-c-header__item--inline-links{display:flex;gap:16px 16px;visibility:visible}}.eds-c-header__item--divider:before{border-left:2px solid #c5e0f4;content:"";height:calc(100% - 16px);margin-left:-15px;position:absolute;top:8px}.eds-c-header__brand{padding:16px 8px}.eds-c-header__brand a{display:block;line-height:1;text-decoration:none}.eds-c-header__brand img{height:1.5rem;width:auto}.eds-c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.eds-c-header__icon{fill:currentcolor;display:inline-block;font-size:1.5rem;height:1em;transform:translate(0);vertical-align:bottom;width:1em}.eds-c-header__icon+*{margin-left:8px}.eds-c-header__expander{background-color:#f0f7fc}.eds-c-header__search{display:block;padding:24px 0}@media only screen and (min-width:768px){.eds-c-header__search{max-width:70%}}.eds-c-header__search-container{position:relative}.eds-c-header__search-label{color:inherit;display:inline-block;font-weight:700;margin-bottom:8px}.eds-c-header__search-input{background-color:#fff;border:1px solid #000;padding:8px 48px 8px 8px;width:100%}.eds-c-header__search-button{background-color:transparent;border:0;color:inherit;height:100%;padding:0 8px;position:absolute;right:0}.has-tethered.eds-c-header__expander{border-bottom:2px solid #01324b;left:0;margin-top:-2px;top:100%;width:100%;z-index:10}@media only screen and (min-width:768px){.has-tethered.eds-c-header__expander--menu{display:none;visibility:hidden}}.has-tethered .eds-c-header__heading{display:none;visibility:hidden}.has-tethered .eds-c-header__heading:first-child+*{margin-block-start:0}.has-tethered .eds-c-header__search{margin:auto}.eds-c-header__heading{margin:0 auto;max-width:1280px;padding:16px 16px 0}.eds-c-pagination{align-items:center;display:flex;flex-wrap:wrap;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;gap:16px 0;justify-content:center;line-height:1.4;list-style:none;margin:0;padding:32px 0}@media only screen and (min-width:480px){.eds-c-pagination{padding:32px 16px}}.eds-c-pagination__item{margin-right:8px}.eds-c-pagination__item--prev{margin-right:16px}.eds-c-pagination__item--next .eds-c-pagination__link,.eds-c-pagination__item--prev .eds-c-pagination__link{padding:16px 8px}.eds-c-pagination__item--next{margin-left:8px}.eds-c-pagination__item:last-child{margin-right:0}.eds-c-pagination__link{align-items:center;color:#222;cursor:pointer;display:inline-block;font-size:1rem;margin:0;padding:16px 24px;position:relative;text-align:center;transition:all .2s ease 0s}.eds-c-pagination__link:visited{color:#222}.eds-c-pagination__link--disabled{border-color:#555;color:#555;cursor:default}.eds-c-pagination__link--active{background-color:#01324b;background-image:none;border-radius:8px;color:#fff}.eds-c-pagination__link--active:focus,.eds-c-pagination__link--active:hover,.eds-c-pagination__link--active:visited{color:#fff}.eds-c-pagination__link-container{align-items:center;display:flex}.eds-c-pagination__icon{fill:#222;height:1.5rem;width:1.5rem}.eds-c-pagination__icon--disabled{fill:#555}.eds-c-pagination__visually-hidden{border:0;clip:rect(0,0,0,0);clip-path:inset(50%);height:1px;overflow:hidden;padding:0;position:absolute!important;white-space:nowrap;width:1px}.c-breadcrumbs{color:#333;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs>li{display:inline}svg.c-breadcrumbs__chevron{margin:0 .25rem;fill:#333;height:10px;width:10px}.c-breadcrumbs--contrast,.c-breadcrumbs--contrast .c-breadcrumbs__link{color:#fff}.c-breadcrumbs--contrast svg.c-breadcrumbs__chevron{fill:#fff}@media only screen and (max-width:479px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-skip-link{background:#01324b;bottom:auto;color:#fff;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);width:100%;z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:active,.c-skip-link:hover,.c-skip-link:link,.c-skip-link:visited{color:#fff}.c-skip-link:focus{transform:translateY(0)}.l-with-sidebar{display:flex;flex-wrap:wrap}.l-with-sidebar>*{margin:0}.l-with-sidebar__sidebar{flex-basis:var(--with-sidebar--basis,400px);flex-grow:1}.l-with-sidebar>:not(.l-with-sidebar__sidebar){flex-basis:0px;flex-grow:999;min-width:var(--with-sidebar--min,53%)}.l-with-sidebar>:first-child{padding-right:4rem}@supports (gap:1em){.l-with-sidebar>:first-child{padding-right:0}.l-with-sidebar{gap:var(--with-sidebar--gap,4rem)}}.c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.app-masthead__colour-4{--background-color:#ff9500;--gradient-light:rgba(0,0,0,.5);--gradient-dark:rgba(0,0,0,.8)}.app-masthead{background:var(--background-color,#0070a8);position:relative}.app-masthead:after{background:radial-gradient(circle at top right,var(--gradient-light,rgba(0,0,0,.4)),var(--gradient-dark,rgba(0,0,0,.7)));bottom:0;content:"";left:0;position:absolute;right:0;top:0}@media only screen and (max-width:479px){.app-masthead:after{background:linear-gradient(225deg,var(--gradient-light,rgba(0,0,0,.4)),var(--gradient-dark,rgba(0,0,0,.7)))}}.app-masthead__container{color:var(--masthead-color,#fff);margin:0 auto;max-width:1280px;padding:0 16px;position:relative;z-index:1}.u-clear-both{clear:both}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-visually-hidden{border:0;clip:rect(0,0,0,0);clip-path:inset(50%);height:1px;overflow:hidden;padding:0;position:absolute!important;white-space:nowrap;width:1px}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-ma-16{margin:16px}.u-mt-0{margin-top:0}.u-mt-24{margin-top:24px}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-32{margin-bottom:32px}.u-sans-serif{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.u-serif{font-family:Merriweather,serif}h1,h2,h3{-webkit-font-smoothing:antialiased}p{overflow-wrap:break-word;word-break:break-word}.u-h4{font-size:1.25rem;font-weight:700;line-height:1.2}.u-mbs-0{margin-block-start:0!important}.c-article-header{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;padding:0}.c-article-identifiers__item{list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{margin-right:0;padding-right:0}@media only screen and (min-width:876px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:767px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#025e8d;border-color:transparent;color:#fff}.c-article-body .c-article-access-provider{padding:8px 16px}.c-article-body .c-article-access-provider,.c-notes{border:1px solid #d5d5d5;border-image:initial;border-left:none;border-right:none;margin:24px 0}.c-article-body .c-article-access-provider__text{color:#555}.c-article-body .c-article-access-provider__text,.c-notes__text{font-size:1rem;margin-bottom:0;padding-bottom:2px;padding-top:2px;text-align:center}.c-article-body .c-article-author-affiliation__address{color:inherit;font-weight:700;margin:0}.c-article-body .c-article-author-affiliation__authors-list{list-style:none;margin:0;padding:0}.c-article-body .c-article-author-affiliation__authors-item{display:inline;margin-left:0}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-article-body .c-article-subject-list--no-mb{margin-bottom:0}.c-code-block{border:1px solid #fff;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;margin-bottom:24px}.c-article-share-box__description{font-size:1rem;margin-bottom:8px}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__additional-info{color:#626262;font-size:.813rem}.c-article-share-box__button{background:#fff;box-sizing:content-box;text-align:center}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#025e8d;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{font-size:1rem}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;font-size:1.25rem;font-weight:700;line-height:1.2;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-article-section__figure-caption{display:block;margin-bottom:8px;word-break:break-word}.c-article-section__figure .video,p.app-article-masthead__access--above-download{margin:0 0 16px}.c-cod{display:block;font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{background-color:#025e8d;border:1px solid #025e8d;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#025e8d}.save-data .c-article-author-institutional-author__sub-division,.save-data .c-article-equation__number,.save-data .c-article-figure-description,.save-data .c-article-fullwidth-content,.save-data .c-article-main-column,.save-data .c-article-satellite-article-link,.save-data .c-article-satellite-subtitle,.save-data .c-article-table-container,.save-data .c-blockquote__body,.save-data .c-code-block__heading,.save-data .c-reading-companion__figure-title,.save-data .c-reading-companion__reference-citation,.save-data .c-site-messages--nature-briefing-email-variant .serif,.save-data .c-site-messages--nature-briefing-email-variant.serif,.save-data .serif,.save-data .u-serif,.save-data h1,.save-data h2,.save-data h3{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-pdf-download__link{display:flex;flex:1 1 0%;padding:13px 24px}.c-pdf-download__link:hover{text-decoration:none}@media only screen and (min-width:768px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.c-article-body .c-article-recommendations-list,.c-book-body .c-article-recommendations-list{display:flex;flex-direction:row;gap:16px 16px;margin:0;max-width:100%;padding:16px 0 0}.c-article-body .c-article-recommendations-list__item,.c-book-body .c-article-recommendations-list__item{flex:1 1 0%}@media only screen and (max-width:767px){.c-article-body .c-article-recommendations-list,.c-book-body .c-article-recommendations-list{flex-direction:column}}.c-article-body .c-article-recommendations-card__authors{display:none;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;line-height:1.5;margin:0 0 8px}@media only screen and (max-width:767px){.c-article-body .c-article-recommendations-card__authors{display:block;margin:0}}.c-article-body .c-article-history{margin-top:24px}.app-article-metrics-bar p{margin:0}.app-article-masthead{display:flex;flex-direction:column;gap:16px 16px;padding:16px 0 24px}.app-article-masthead__info{display:flex;flex-direction:column;flex-grow:1}.app-article-masthead__brand{border-top:1px solid hsla(0,0%,100%,.8);display:flex;flex-direction:column;flex-shrink:0;gap:8px 8px;min-height:96px;padding:16px 0 0}.app-article-masthead__brand img{border:1px solid #fff;border-radius:8px;box-shadow:0 4px 15px 0 hsla(0,0%,50%,.25);height:auto;left:0;position:absolute;width:72px}.app-article-masthead__journal-link{display:block;font-size:1.125rem;font-weight:700;margin:0 0 8px;max-width:400px;padding:0 0 0 88px;position:relative}.app-article-masthead__journal-title{display:-webkit-box;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:3}.app-article-masthead__submission-link{align-items:center;display:flex;font-size:1rem;gap:4px 4px;margin:0 0 0 88px}.app-article-masthead__access{align-items:center;display:flex;flex-wrap:wrap;font-size:.875rem;font-weight:300;gap:4px 4px;margin:0}.app-article-masthead__buttons{display:flex;flex-flow:column wrap;gap:16px 16px}.app-article-masthead__access svg{fill:currentcolor}.app-article-masthead a{color:#fff}.app-article-masthead a.c-pdf-download__link,.app-article-masthead__syndicated-card a,.app-article-masthead__syndicated-card a:visited,.app-masthead--pastel .app-article-masthead a,.app-masthead--pastel .app-article-masthead a:visited{color:#000}.app-masthead--pastel .c-pdf-download a.u-button--primary,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--primary{background-color:#025e8d;border:2px solid transparent;box-shadow:none;color:#fff;font-weight:700}.app-masthead--pastel .c-pdf-download a.u-button--primary:focus,.app-masthead--pastel .c-pdf-download a.u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--primary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--primary:hover{background:0 0;border:2px solid #025e8d;box-shadow:none;color:#025e8d}.app-masthead--pastel .c-pdf-download a.u-button--secondary,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--secondary{background:0 0;border:2px solid #025e8d;color:#025e8d}.app-masthead--pastel .c-pdf-download a.u-button--secondary:focus,.app-masthead--pastel .c-pdf-download a.u-button--secondary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--secondary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download a.u-button--secondary:hover{background-color:#025e8d;border:2px solid transparent;color:#fff}@media only screen and (min-width:768px){.app-article-masthead{flex-direction:row;gap:64px 64px;padding:24px 0}.app-article-masthead__brand{border:0;padding:0}.app-article-masthead__brand img{height:auto;position:static;width:auto}.app-article-masthead__buttons{align-items:center;flex-direction:row;margin-top:auto}.app-article-masthead__journal-link{display:flex;flex-direction:column;gap:24px 24px;margin:0 0 8px;padding:0}.app-article-masthead__submission-link{margin:0}}@media only screen and (min-width:1024px){.app-article-masthead__brand{flex-basis:400px}}.app-article-masthead .c-article-identifiers{font-size:.875rem;font-weight:300;line-height:1;margin:0 0 8px;overflow:hidden;padding:0}.app-article-masthead .c-article-identifiers--cite-list{margin:0 0 16px}.app-article-masthead .c-article-identifiers *{color:#fff}.app-article-masthead .c-cod{display:none}.app-article-masthead .c-article-identifiers__item{border-left:1px solid #fff;border-right:0;margin:0 17px 8px -9px;padding:0 0 0 8px}.app-article-masthead .c-article-identifiers__item--cite{border-left:0}.app-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;padding:16px 0 0;row-gap:24px}.app-article-metrics-bar__item{padding:0 16px 0 0}.app-article-metrics-bar__count{font-weight:700}.app-article-metrics-bar__label{font-weight:400;padding-left:4px}.app-article-metrics-bar__icon{height:auto;margin-right:4px;margin-top:-4px;width:auto}.app-article-metrics-bar__arrow-icon{margin:4px 0 0 4px}.app-article-metrics-bar a{color:#000}.app-article-metrics-bar .app-article-metrics-bar__item--metrics{padding-right:0}.app-overview-section .c-article-author-list,.app-overview-section__authors{line-height:2}.app-article-metrics-bar{margin-top:8px}.c-book-toc-pagination+.c-book-section__back-to-top{margin-top:0}.c-article-body .c-article-access-provider__text--chapter{color:#222;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;padding:20px 0}.c-article-body .c-article-access-provider__text--chapter svg.c-status-message__icon{fill:#003f8d;vertical-align:middle}.c-article-body-section__content--separator{padding-top:40px}.c-pdf-download__link{max-height:44px}.app-article-access .u-button--primary,.app-article-access .u-button--primary:visited{color:#fff}.c-article-authors-search__list{align-items:center;display:flex;flex-wrap:wrap;gap:16px 16px;justify-content:center}@media only screen and (min-width:480px){.c-article-authors-search__list{justify-content:normal}}.c-article-authors-search__text{align-items:center;display:flex;flex-flow:column wrap;font-size:14px;justify-content:center}@media only screen and (min-width:480px){.c-article-authors-search__text{flex-direction:row;font-size:16px}}.c-article-authors-search__links-text{font-weight:700;margin-right:8px;text-align:center}@media only screen and (min-width:480px){.c-article-authors-search__links-text{text-align:left}}.c-article-authors-search__list-item--left{flex:1 1 100%}@media only screen and (min-width:480px){.c-article-authors-search__list-item--left{flex-basis:auto}}.c-article-authors-search__list-item--right{flex:1 1 auto}.c-article-identifiers{margin:0}.c-article-identifiers__item{border-right:2px solid #cedbe0;color:#222;font-size:14px}@media only screen and (min-width:480px){.c-article-identifiers__item{font-size:16px}}.c-article-identifiers__item:last-child{border-right:none}.c-article-authors-search__cta-link{font-size:14px}@media only screen and (min-width:480px){.c-article-authors-search__cta-link{font-size:16px}}.c-article-sidebar{display:none}@media only screen and (min-width:1024px){.c-article-sidebar{display:block}}.c-cod__form{border-radius:12px}.c-cod__label{font-size:.875rem}.c-cod .c-status-message{align-items:center;justify-content:center;margin-bottom:16px;padding-bottom:16px}@media only screen and (min-width:1024px){.c-cod .c-status-message{align-items:inherit}}.c-cod .c-status-message__icon{margin-top:4px}.c-cod .c-cod__prompt{font-size:1rem;margin-bottom:16px}.c-spp-access-message .c-status-message__icon{color:#00a69d;margin-top:8px}.c-article-body .app-article-access,.c-book-body .app-article-access{display:block}@media only screen and (min-width:1024px){.c-article-body .app-article-access,.c-book-body .app-article-access{display:none}}.c-article-body .app-card-service{margin-bottom:32px}@media only screen and (min-width:1024px){.c-article-body .app-card-service{display:none}}.app-article-access .buybox__buy .u-button--secondary,.app-article-access .u-button--primary,.c-cod__row .u-button--primary{background-color:#025e8d;border:2px solid #025e8d;box-shadow:none;font-size:1rem;font-weight:700;gap:8px 8px;justify-content:center;line-height:1.4;padding:8px 24px}.app-article-access .buybox__buy .u-button--secondary,.app-article-access .u-button--primary:hover,.c-cod__row .u-button--primary:hover{background-color:#fff;color:#025e8d}.app-article-access .buybox__buy .u-button--secondary:hover{background-color:#025e8d;color:#fff}.buybox__buy .c-notes__text{color:#666;font-size:.875rem;padding:0 16px 8px}.c-cod__input{flex-basis:auto;width:100%}.c-article-title{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:2.25rem;font-weight:700;line-height:1.2;margin:12px 0}.c-reading-companion__figure-item figure{margin:0}@media only screen and (min-width:768px){.c-article-title{margin:16px 0}}.app-article-access{border:1px solid #cedbe0;border-radius:12px;margin:0 0 32px}.app-article-access__heading{border-bottom:1px solid #cedbe0;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1.125rem;font-weight:700;margin:0;padding:16px;text-align:center}.c-article-body .app-article-access p{margin-bottom:0}@media only screen and (min-width:1024px){.app-article-access{margin:0 0 24px}}.c-status-message{font-size:1rem}.c-article-body{font-size:1.125rem}.c-article-body dl,.c-article-body ol,.c-article-body p,.c-article-body ul{margin-bottom:32px;margin-top:0}.c-article-access-provider__text:last-of-type,.c-article-body .c-notes__text:last-of-type{margin-bottom:0}.c-article-body ol p,.c-article-body ul p{margin-bottom:16px}.c-article-section__figure-caption{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-reading-companion__figure-item{border-top-color:#cedbe0}.c-reading-companion__sticky{max-width:400px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;padding:16px 0}.c-reading-companion__reference-item:first-child{padding-top:0}.c-article-share-box__button,.js .c-article-authors-search__item .c-article-button{background:0 0;border:2px solid #025e8d;border-radius:32px;box-shadow:none;color:#025e8d;font-size:1rem;font-weight:700;line-height:1.4;margin:0;padding:8px 24px;transition:all .2s ease 0s}.c-article-authors-search__item .c-article-button{width:100%}.c-pdf-download .c-pdf-download__link{align-items:center;background-color:#fff;border:2px solid #fff;border-radius:32px;box-shadow:none;color:#01324b;cursor:pointer;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;font-weight:700;justify-content:center;line-height:1.4;padding:8px 24px;text-decoration:none}.c-context-bar__container .c-pdf-download .c-pdf-download__link{background-color:#025e8d;background-image:none;border:2px solid #025e8d;box-shadow:none;color:#fff;font-size:1rem;font-weight:700;line-height:1.4;padding:8px 24px}.c-pdf-download .c-pdf-download__link:hover{background:0 0;border:2px solid #fff;box-shadow:none;color:#fff}.c-pdf-download .c-pdf-download__link:focus{background:0 0;box-shadow:none;color:#fff}.c-context-bar__container .c-pdf-download .c-pdf-download__link:hover{border:2px solid #025e8d;box-shadow:none;color:#025e8d}.c-context-bar__container .c-pdf-download .c-pdf-download__link:focus,.c-pdf-download .c-pdf-download__link:focus{border:2px solid #025e8d}.c-article-share-box__button:focus:focus,.c-article__pill-button:focus:focus,.c-context-bar__container .c-pdf-download .c-pdf-download__link:focus:focus,.c-pdf-download .c-pdf-download__link:focus:focus{outline:3px solid #08c;will-change:transform}.c-pdf-download__link .u-icon{padding-top:0}.c-bibliographic-information__column button{margin-bottom:16px}.c-article-body .c-article-author-affiliation__list p,.c-article-body .c-article-author-information__list p,figure{margin:0}.c-article-share-box__button{margin-right:16px}.c-status-message--boxed{border-radius:12px}.c-article-associated-content__collection-title{font-size:1rem}.app-card-service__description,.c-article-body .app-card-service__description{color:#222;margin-bottom:0;margin-top:8px}.app-article-access__subscriptions a,.app-article-access__subscriptions a:visited,.app-book-series-listing__item a,.app-book-series-listing__item a:hover,.app-book-series-listing__item a:visited,.c-article-author-list a,.c-article-author-list a:visited,.c-article-buy-box a,.c-article-buy-box a:visited,.c-article-peer-review a,.c-article-peer-review a:visited,.c-article-satellite-subtitle a,.c-article-satellite-subtitle a:visited,.c-breadcrumbs__link,.c-breadcrumbs__link:hover,.c-breadcrumbs__link:visited{color:#000}.c-article-author-list svg{height:24px;margin:0 0 0 6px;width:24px}.c-article-header{margin-bottom:32px}@media only screen and (min-width:876px){.js .c-ad--conditional{display:block}}.u-lazy-ad-wrapper{background-color:#fff;display:none;min-height:149px}@media only screen and (min-width:876px){.u-lazy-ad-wrapper{display:block}}p.c-ad__label{margin-bottom:4px}.c-ad--728x90{background-color:#fff;border-bottom:2px solid #cedbe0} } </style>
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .eds-c-header__brand img{height:24px;width:203px}.app-article-masthead__journal-link img{height:93px;width:72px}@media only screen and (min-width:769px){.app-article-masthead__journal-link img{height:161px;width:122px}} } </style>

        
        <link rel="stylesheet" data-test="critical-css-handler" data-inline-css-source="critical-css" href=/oscar-static/app-springerlink/css/core-darwin-9fe647df8f.css media="print" onload="this.media='all';this.onload=null">
        <link rel="stylesheet" data-test="critical-css-handler" data-inline-css-source="critical-css" href="/oscar-static/app-springerlink/css/enhanced-darwin-article-2fcb434597.css" media="print" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
    

        
        
    <script type="text/javascript">
        config = {
            env: 'live',
            site: '12559.springer.com',
            siteWithPath: '12559.springer.com' + window.location.pathname,
            twitterHashtag: '12559',
            cmsPrefix: 'https://studio-cms.springernature.com/studio/',
            
            
            
            
            publisherBrand: 'Springer',
            mustardcut: false
        };
    </script>

        




    <script>
        window.dataLayer = [{"GA Key":"UA-26408784-1","DOI":"10.1007/s12559-024-10325-w","Page":"article","springerJournal":true,"Publishing Model":"Hybrid Access","Country":"FI","japan":false,"doi":"10.1007-s12559-024-10325-w","Journal Id":12559,"Journal Title":"Cognitive Computation","imprint":"Springer","Keywords":"Explainable AI, XAI evaluation, Text classification, Interpretability, Human-computer interaction","kwrd":["Explainable_AI","XAI_evaluation","Text_classification","Interpretability","Human-computer_interaction"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":[],"Open Access":"Y","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":[],"businessPartnerIDString":""}},"Access Type":"open","Bpids":"","Bpnames":"","BPID":["1"],"VG Wort Identifier":"vgzm.415900-10.1007-s12559-024-10325-w","Full HTML":"Y","Subject Codes":["SCI","SCI21000","SCI16013","SCI23050"],"pmc":["I","I21000","I16013","I23050"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"entitlement":{"accessDecision":"OpenAccess"},"content":{"serial":{"eissn":"1866-9964","pissn":"1866-9956"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Artificial Intelligence","2":"Computation by Abstract Devices","3":"Artificial Intelligence","4":"Computational Biology/Bioinformatics"},"secondarySubjectCodes":{"1":"I21000","2":"I16013","3":"I21000","4":"I23050"}},"sucode":"SC6","articleType":"Review","snt":["Machine Learning","Artificial Intelligence","Computational Intelligence","Learning algorithms","Categorization","Machine Translation"]},"attributes":{"deliveryPlatform":"oscar"}},"page":{"attributes":{"environment":"live"},"category":{"pageType":"article"}},"Event Category":"Article"}];
    </script>











    <script data-test="springer-link-article-datalayer">
        window.dataLayer = window.dataLayer || [];
        window.dataLayer.push({
            ga4MeasurementId: 'G-B3E4QL2TPR',
            ga360TrackingId: 'UA-26408784-1',
            twitterId: 'o47a7',
            baiduId: 'aef3043f025ccf2305af8a194652d70b',
            ga4ServerUrl: 'https://collect.springer.com',
            imprint: 'springerlink',
                page: {
                    attributes:{
                        featureFlags: [
                            
                                { name: 'darwin-orion', active: true },
                            
                                { name: 'show-profile-page-links', active: true },
                            
                        ],
                        darwinAvailable: true
                    }
                }
            
        });
    </script>



        <script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>


        <script class="js-entry">
    if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
                window.suppressShareButton = false;
                window.onArticlePage = true;
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-b4356fa7f5.js', 'async': false}
            ];

            var bodyScripts = [
                
                    
                    {'src': '/oscar-static/js/global-article-es5-bundle-c7bdbee6e7.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-8cfe1d6fdf.js', 'async': false, 'module': true}
                    
                
                
                    
                
                
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
</script>



        
            
            
                
<script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                    j = d.createElement(s),
                    dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://sgtm.springer.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
                performance.mark('SN GPT Ads gtm-container-fired');
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>

            
            
            
        

        <script>
(function (w, d, t) {
    function cc() {
        var h = w.location.hostname;
        var e = d.createElement(t),
        s = d.getElementsByTagName(t)[0];

        
        if (h.indexOf('springer.com') > -1 && h.indexOf('biomedcentral.com') === -1 && h.indexOf('springeropen.com') === -1) {
            e.src = 'https://cmp.springer.com/production_live/en/consent-bundle-17-70.js';
            e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
        } else if (h.indexOf('biomedcentral.com') > -1) {
            e.src = 'https://cmp.biomedcentral.com/production_live/en/consent-bundle-15-45.js';
            e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
        } else if (h.indexOf('springeropen.com') > -1) {
            e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-16-40.js';
            e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
        } else if (h.indexOf('springernature.com') > -1) {
            e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-49-43.js';
            e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-NK22KLS')");
        } else {
            e.src = '/oscar-static/js/cookie-consent-es5-bundle-8d962b73c2.js';
            e.setAttribute('data-consent', h);
        }
        s.insertAdjacentElement('afterend', e);
    }

    cc();
})(window, document, 'script');
</script>


        
        
        
    
        
    

        
    
    <link rel="canonical" href="https://link.springer.com/article/10.1007/s12559-024-10325-w"/>
    

        
        
        
        
        
    <script type="application/ld+json">{"mainEntity":{"headline":"Explainable AI for Text Classification: Lessons from a Comprehensive Evaluation of Post Hoc Methods","description":"This paper addresses the notable gap in evaluating eXplainable Artificial Intelligence (XAI) methods for text classification. While existing frameworks focus on assessing XAI in areas such as recommender systems and visual analytics, a comprehensive evaluation is missing. Our study surveys and categorises recent post hoc XAI methods according to their scope of explanation and output format. We then conduct a systematic evaluation, assessing the effectiveness of these methods across varying scopes and levels of output granularity using a combination of objective metrics and user studies. Key findings reveal that feature-based explanations exhibit higher fidelity than rule-based ones. While global explanations are perceived as more satisfying and trustworthy, they are less practical than local explanations. These insights enhance understanding of XAI in text classification and offer valuable guidance for developing effective XAI systems, enabling users to evaluate each explainer’s pros and cons and select the most suitable one for their needs.","datePublished":"2024-08-06T00:00:00Z","dateModified":"2024-08-06T00:00:00Z","pageStart":"3077","pageEnd":"3095","license":"http://creativecommons.org/licenses/by/4.0/","sameAs":"https://doi.org/10.1007/s12559-024-10325-w","keywords":["Explainable AI","XAI evaluation","Text classification","Interpretability","Human-computer interaction","Artificial Intelligence","Computation by Abstract Devices","Computational Biology/Bioinformatics"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs12559-024-10325-w/MediaObjects/12559_2024_10325_Fig1_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs12559-024-10325-w/MediaObjects/12559_2024_10325_Fig2_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs12559-024-10325-w/MediaObjects/12559_2024_10325_Fig3_HTML.png"],"isPartOf":{"name":"Cognitive Computation","issn":["1866-9964","1866-9956"],"volumeNumber":"16","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Springer US","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Mirko Cesarini","affiliation":[{"name":"University of Milan-Bicocca","address":{"name":"Department of Statistics and Quantitative Methods, University of Milan-Bicocca, Milan, Italy","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Lorenzo Malandri","affiliation":[{"name":"University of Milan-Bicocca","address":{"name":"Department of Statistics and Quantitative Methods, University of Milan-Bicocca, Milan, Italy","@type":"PostalAddress"},"@type":"Organization"}],"email":"lorenzo.malandri@unimib.it","@type":"Person"},{"name":"Filippo Pallucchini","affiliation":[{"name":"University of Milan-Bicocca","address":{"name":"Department of Statistics and Quantitative Methods, University of Milan-Bicocca, Milan, Italy","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Andrea Seveso","affiliation":[{"name":"University of Milan-Bicocca","address":{"name":"Department of Statistics and Quantitative Methods, University of Milan-Bicocca, Milan, Italy","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Frank Xing","affiliation":[{"name":"National University of Singapore","address":{"name":"School of Computing, National University of Singapore, Singapore, Singapore","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"isAccessibleForFree":true,"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>

        
        
    </head>

    <body class=""
    
          >
        <div class="u-visually-hidden" aria-hidden="true" data-test="darwin-icons">
    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><clipPath id="a"><path d="M.5 0h24v24H.5z"/></clipPath></defs><symbol id="icon-eds-i-accesses-medium" viewBox="0 0 24 24"><path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742H15a1 1 0 0 1 0-2h4.455a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM8 13c2.052 0 4.66 1.61 6.36 3.4l.124.141c.333.41.516.925.516 1.459 0 .6-.232 1.178-.64 1.599C12.666 21.388 10.054 23 8 23c-2.052 0-4.66-1.61-6.353-3.393A2.31 2.31 0 0 1 1 18c0-.6.232-1.178.64-1.6C3.34 14.61 5.948 13 8 13Zm0 2c-1.369 0-3.552 1.348-4.917 2.785A.31.31 0 0 0 3 18c0 .083.031.161.09.222C4.447 19.652 6.631 21 8 21c1.37 0 3.556-1.35 4.917-2.785A.31.31 0 0 0 13 18a.32.32 0 0 0-.048-.17l-.042-.052C11.553 16.348 9.369 15 8 15Zm0 1a2 2 0 1 1 0 4 2 2 0 0 1 0-4Z"/></symbol><symbol id="icon-eds-i-altmetric-medium" viewBox="0 0 24 24"><path d="M12 1c5.978 0 10.843 4.77 10.996 10.712l.004.306-.002.022-.002.248C22.843 18.23 17.978 23 12 23 5.925 23 1 18.075 1 12S5.925 1 12 1Zm-1.726 9.246L8.848 12.53a1 1 0 0 1-.718.461L8.003 13l-4.947.014a9.001 9.001 0 0 0 17.887-.001L16.553 13l-2.205 3.53a1 1 0 0 1-1.735-.068l-.05-.11-2.289-6.106ZM12 3a9.001 9.001 0 0 0-8.947 8.013l4.391-.012L9.652 7.47a1 1 0 0 1 1.784.179l2.288 6.104 1.428-2.283a1 1 0 0 1 .722-.462l.129-.008 4.943.012A9.001 9.001 0 0 0 12 3Z"/></symbol><symbol id="icon-eds-i-arrow-bend-down-medium" viewBox="0 0 24 24"><path d="m11.852 20.989.058.007L12 21l.075-.003.126-.017.111-.03.111-.044.098-.052.104-.074.082-.073 6-6a1 1 0 0 0-1.414-1.414L13 17.585v-12.2C13 4.075 11.964 3 10.667 3H4a1 1 0 1 0 0 2h6.667c.175 0 .333.164.333.385v12.2l-4.293-4.292a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414l6 6c.035.036.073.068.112.097l.11.071.114.054.105.035.118.025Z"/></symbol><symbol id="icon-eds-i-arrow-bend-down-small" viewBox="0 0 16 16"><path d="M1 2a1 1 0 0 0 1 1h5v8.585L3.707 8.293a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414l5 5 .063.059.093.069.081.048.105.048.104.035.105.022.096.01h.136l.122-.018.113-.03.103-.04.1-.053.102-.07.052-.043 5.04-5.037a1 1 0 1 0-1.415-1.414L9 11.583V3a2 2 0 0 0-2-2H2a1 1 0 0 0-1 1Z"/></symbol><symbol id="icon-eds-i-arrow-bend-up-medium" viewBox="0 0 24 24"><path d="m11.852 3.011.058-.007L12 3l.075.003.126.017.111.03.111.044.098.052.104.074.082.073 6 6a1 1 0 1 1-1.414 1.414L13 6.415v12.2C13 19.925 11.964 21 10.667 21H4a1 1 0 0 1 0-2h6.667c.175 0 .333-.164.333-.385v-12.2l-4.293 4.292a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l6-6c.035-.036.073-.068.112-.097l.11-.071.114-.054.105-.035.118-.025Z"/></symbol><symbol id="icon-eds-i-arrow-bend-up-small" viewBox="0 0 16 16"><path d="M1 13.998a1 1 0 0 1 1-1h5V4.413L3.707 7.705a1 1 0 0 1-1.32.084l-.094-.084a1 1 0 0 1 0-1.414l5-5 .063-.059.093-.068.081-.05.105-.047.104-.035.105-.022L7.94 1l.136.001.122.017.113.03.103.04.1.053.102.07.052.043 5.04 5.037a1 1 0 1 1-1.415 1.414L9 4.415v8.583a2 2 0 0 1-2 2H2a1 1 0 0 1-1-1Z"/></symbol><symbol id="icon-eds-i-arrow-diagonal-medium" viewBox="0 0 24 24"><path d="M14 3h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L21 4v6a1 1 0 0 1-2 0V6.414l-4.293 4.293a1 1 0 0 1-1.414-1.414L17.584 5H14a1 1 0 0 1-.993-.883L13 4a1 1 0 0 1 1-1ZM4 13a1 1 0 0 1 1 1v3.584l4.293-4.291a1 1 0 1 1 1.414 1.414L6.414 19H10a1 1 0 0 1 .993.883L11 20a1 1 0 0 1-1 1l-6.075-.003-.126-.017-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08a1.01 1.01 0 0 1-.097-.112l-.071-.11-.054-.114-.035-.105-.025-.118-.007-.058L3 20v-6a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-arrow-diagonal-small" viewBox="0 0 16 16"><path d="m2 15-.082-.004-.119-.016-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08a1.008 1.008 0 0 1-.097-.112l-.071-.11-.031-.062-.034-.081-.024-.076-.025-.118-.007-.058L1 14.02V9a1 1 0 1 1 2 0v2.584l2.793-2.791a1 1 0 1 1 1.414 1.414L4.414 13H7a1 1 0 0 1 .993.883L8 14a1 1 0 0 1-1 1H2ZM14 1l.081.003.12.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.031.062.034.081.024.076.03.148L15 2v5a1 1 0 0 1-2 0V4.414l-2.96 2.96A1 1 0 1 1 8.626 5.96L11.584 3H9a1 1 0 0 1-.993-.883L8 2a1 1 0 0 1 1-1h5Z"/></symbol><symbol id="icon-eds-i-arrow-down-medium" viewBox="0 0 24 24"><path d="m20.707 12.728-7.99 7.98a.996.996 0 0 1-.561.281l-.157.011a.998.998 0 0 1-.788-.384l-7.918-7.908a1 1 0 0 1 1.414-1.416L11 17.576V4a1 1 0 0 1 2 0v13.598l6.293-6.285a1 1 0 0 1 1.32-.082l.095.083a1 1 0 0 1-.001 1.414Z"/></symbol><symbol id="icon-eds-i-arrow-down-small" viewBox="0 0 16 16"><path d="m1.293 8.707 6 6 .063.059.093.069.081.048.105.049.104.034.056.013.118.017L8 15l.076-.003.122-.017.113-.03.085-.032.063-.03.098-.058.06-.043.05-.043 6.04-6.037a1 1 0 0 0-1.414-1.414L9 11.583V2a1 1 0 1 0-2 0v9.585L2.707 7.293a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414Z"/></symbol><symbol id="icon-eds-i-arrow-left-medium" viewBox="0 0 24 24"><path d="m11.272 3.293-7.98 7.99a.996.996 0 0 0-.281.561L3 12.001c0 .32.15.605.384.788l7.908 7.918a1 1 0 0 0 1.416-1.414L6.424 13H20a1 1 0 0 0 0-2H6.402l6.285-6.293a1 1 0 0 0 .082-1.32l-.083-.095a1 1 0 0 0-1.414.001Z"/></symbol><symbol id="icon-eds-i-arrow-left-small" viewBox="0 0 16 16"><path d="m7.293 1.293-6 6-.059.063-.069.093-.048.081-.049.105-.034.104-.013.056-.017.118L1 8l.003.076.017.122.03.113.032.085.03.063.058.098.043.06.043.05 6.037 6.04a1 1 0 0 0 1.414-1.414L4.417 9H14a1 1 0 0 0 0-2H4.415l4.292-4.293a1 1 0 0 0 .083-1.32l-.083-.094a1 1 0 0 0-1.414 0Z"/></symbol><symbol id="icon-eds-i-arrow-right-medium" viewBox="0 0 24 24"><path d="m12.728 3.293 7.98 7.99a.996.996 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001Z"/></symbol><symbol id="icon-eds-i-arrow-right-small" viewBox="0 0 16 16"><path d="m8.707 1.293 6 6 .059.063.069.093.048.081.049.105.034.104.013.056.017.118L15 8l-.003.076-.017.122-.03.113-.032.085-.03.063-.058.098-.043.06-.043.05-6.037 6.04a1 1 0 0 1-1.414-1.414L11.583 9H2a1 1 0 1 1 0-2h9.585L7.293 2.707a1 1 0 0 1-.083-1.32l.083-.094a1 1 0 0 1 1.414 0Z"/></symbol><symbol id="icon-eds-i-arrow-up-medium" viewBox="0 0 24 24"><path d="m3.293 11.272 7.99-7.98a.996.996 0 0 1 .561-.281L12.001 3c.32 0 .605.15.788.384l7.918 7.908a1 1 0 0 1-1.414 1.416L13 6.424V20a1 1 0 0 1-2 0V6.402l-6.293 6.285a1 1 0 0 1-1.32.082l-.095-.083a1 1 0 0 1 .001-1.414Z"/></symbol><symbol id="icon-eds-i-arrow-up-small" viewBox="0 0 16 16"><path d="m1.293 7.293 6-6 .063-.059.093-.069.081-.048.105-.049.104-.034.056-.013.118-.017L8 1l.076.003.122.017.113.03.085.032.063.03.098.058.06.043.05.043 6.04 6.037a1 1 0 0 1-1.414 1.414L9 4.417V14a1 1 0 0 1-2 0V4.415L2.707 8.707a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414Z"/></symbol><symbol id="icon-eds-i-article-medium" viewBox="0 0 24 24"><path d="M8 7a1 1 0 0 0 0 2h4a1 1 0 1 0 0-2H8ZM8 11a1 1 0 1 0 0 2h8a1 1 0 1 0 0-2H8ZM7 16a1 1 0 0 1 1-1h8a1 1 0 1 1 0 2H8a1 1 0 0 1-1-1Z"/><path d="M5.545 1A2.542 2.542 0 0 0 3 3.538v16.924A2.542 2.542 0 0 0 5.545 23h12.91A2.542 2.542 0 0 0 21 20.462V3.5A2.5 2.5 0 0 0 18.5 1H5.545ZM5 3.538C5 3.245 5.24 3 5.545 3H18.5a.5.5 0 0 1 .5.5v16.962c0 .293-.24.538-.546.538H5.545A.542.542 0 0 1 5 20.462V3.538Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-book-medium" viewBox="0 0 24 24"><path d="M18.5 1A2.5 2.5 0 0 1 21 3.5v12c0 1.16-.79 2.135-1.86 2.418l-.14.031V21h1a1 1 0 0 1 .993.883L21 22a1 1 0 0 1-1 1H6.5A3.5 3.5 0 0 1 3 19.5v-15A3.5 3.5 0 0 1 6.5 1h12ZM17 18H6.5a1.5 1.5 0 0 0-1.493 1.356L5 19.5A1.5 1.5 0 0 0 6.5 21H17v-3Zm1.5-15h-12A1.5 1.5 0 0 0 5 4.5v11.837l.054-.025a3.481 3.481 0 0 1 1.254-.307L6.5 16h12a.5.5 0 0 0 .492-.41L19 15.5v-12a.5.5 0 0 0-.5-.5ZM15 6a1 1 0 0 1 0 2H9a1 1 0 1 1 0-2h6Z"/></symbol><symbol id="icon-eds-i-book-series-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M1 3.786C1 2.759 1.857 2 2.82 2H6.18c.964 0 1.82.759 1.82 1.786V4h3.168c.668 0 1.298.364 1.616.938.158-.109.333-.195.523-.252l3.216-.965c.923-.277 1.962.204 2.257 1.187l4.146 13.82c.296.984-.307 1.957-1.23 2.234l-3.217.965c-.923.277-1.962-.203-2.257-1.187L13 10.005v10.21c0 1.04-.878 1.785-1.834 1.785H7.833c-.291 0-.575-.07-.83-.195A1.849 1.849 0 0 1 6.18 22H2.821C1.857 22 1 21.241 1 20.214V3.786ZM3 4v11h3V4H3Zm0 16v-3h3v3H3Zm15.075-.04-.814-2.712 2.874-.862.813 2.712-2.873.862Zm1.485-5.49-2.874.862-2.634-8.782 2.873-.862 2.635 8.782ZM8 20V6h3v14H8Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-calendar-acceptance-medium" viewBox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-.534 7.747a1 1 0 0 1 .094 1.412l-4.846 5.538a1 1 0 0 1-1.352.141l-2.77-2.076a1 1 0 0 1 1.2-1.6l2.027 1.519 4.236-4.84a1 1 0 0 1 1.411-.094ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-calendar-date-medium" viewBox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1ZM8 15a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm-4-4a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-calendar-decision-medium" viewBox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-2.935 8.246 2.686 2.645c.34.335.34.883 0 1.218l-2.686 2.645a.858.858 0 0 1-1.213-.009.854.854 0 0 1 .009-1.21l1.05-1.035H7.984a.992.992 0 0 1-.984-1c0-.552.44-1 .984-1h5.928l-1.051-1.036a.854.854 0 0 1-.085-1.121l.076-.088a.858.858 0 0 1 1.213-.009ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-calendar-impact-factor-medium" viewBox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-3.2 6.924a.48.48 0 0 1 .125.544l-1.52 3.283h2.304c.27 0 .491.215.491.483a.477.477 0 0 1-.13.327l-4.18 4.484a.498.498 0 0 1-.69.031.48.48 0 0 1-.125-.544l1.52-3.284H9.291a.487.487 0 0 1-.491-.482c0-.121.047-.238.13-.327l4.18-4.484a.498.498 0 0 1 .69-.031ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-call-papers-medium" viewBox="0 0 24 24"><g><path d="m20.707 2.883-1.414 1.414a1 1 0 0 0 1.414 1.414l1.414-1.414a1 1 0 0 0-1.414-1.414Z"/><path d="M6 16.054c0 2.026 1.052 2.943 3 2.943a1 1 0 1 1 0 2c-2.996 0-5-1.746-5-4.943v-1.227a4.068 4.068 0 0 1-1.83-1.189 4.553 4.553 0 0 1-.87-1.455 4.868 4.868 0 0 1-.3-1.686c0-1.17.417-2.298 1.17-3.14.38-.426.834-.767 1.338-1 .51-.237 1.06-.36 1.617-.36L6.632 6H7l7.932-2.895A2.363 2.363 0 0 1 18 5.36v9.28a2.36 2.36 0 0 1-3.069 2.25l.084.03L7 14.997H6v1.057Zm9.637-11.057a.415.415 0 0 0-.083.008L8 7.638v5.536l7.424 1.786.104.02c.035.01.072.02.109.02.2 0 .363-.16.363-.36V5.36c0-.2-.163-.363-.363-.363Zm-9.638 3h-.874a1.82 1.82 0 0 0-.625.111l-.15.063a2.128 2.128 0 0 0-.689.517c-.42.47-.661 1.123-.661 1.81 0 .34.06.678.176.992.114.308.28.585.485.816.4.447.925.691 1.464.691h.874v-5Z" clip-rule="evenodd"/><path d="M20 8.997h2a1 1 0 1 1 0 2h-2a1 1 0 1 1 0-2ZM20.707 14.293l1.414 1.414a1 1 0 0 1-1.414 1.414l-1.414-1.414a1 1 0 0 1 1.414-1.414Z"/></g></symbol><symbol id="icon-eds-i-card-medium" viewBox="0 0 24 24"><path d="M19.615 2c.315 0 .716.067 1.14.279.76.38 1.245 1.107 1.245 2.106v15.23c0 .315-.067.716-.279 1.14-.38.76-1.107 1.245-2.106 1.245H4.385a2.56 2.56 0 0 1-1.14-.279C2.485 21.341 2 20.614 2 19.615V4.385c0-.315.067-.716.279-1.14C2.659 2.485 3.386 2 4.385 2h15.23Zm0 2H4.385c-.213 0-.265.034-.317.14A.71.71 0 0 0 4 4.385v15.23c0 .213.034.265.14.317a.71.71 0 0 0 .245.068h15.23c.213 0 .265-.034.317-.14a.71.71 0 0 0 .068-.245V4.385c0-.213-.034-.265-.14-.317A.71.71 0 0 0 19.615 4ZM17 16a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h10Zm0-3a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h10Zm-.5-7A1.5 1.5 0 0 1 18 7.5v3a1.5 1.5 0 0 1-1.5 1.5h-9A1.5 1.5 0 0 1 6 10.5v-3A1.5 1.5 0 0 1 7.5 6h9ZM16 8H8v2h8V8Z"/></symbol><symbol id="icon-eds-i-cart-medium" viewBox="0 0 24 24"><path d="M5.76 1a1 1 0 0 1 .994.902L7.155 6h13.34c.18 0 .358.02.532.057l.174.045a2.5 2.5 0 0 1 1.693 3.103l-2.069 7.03c-.36 1.099-1.398 1.823-2.49 1.763H8.65c-1.272.015-2.352-.927-2.546-2.244L4.852 3H2a1 1 0 0 1-.993-.883L1 2a1 1 0 0 1 1-1h3.76Zm2.328 14.51a.555.555 0 0 0 .55.488l9.751.001a.533.533 0 0 0 .527-.357l2.059-7a.5.5 0 0 0-.48-.642H7.351l.737 7.51ZM18 19a2 2 0 1 1 0 4 2 2 0 0 1 0-4ZM8 19a2 2 0 1 1 0 4 2 2 0 0 1 0-4Z"/></symbol><symbol id="icon-eds-i-check-circle-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm5.125 4.72a1 1 0 0 1 .156 1.405l-6 7.5a1 1 0 0 1-1.421.143l-3-2.5a1 1 0 0 1 1.28-1.536l2.217 1.846 5.362-6.703a1 1 0 0 1 1.406-.156Z"/></symbol><symbol id="icon-eds-i-check-filled-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm5.125 6.72a1 1 0 0 0-1.406.155l-5.362 6.703-2.217-1.846a1 1 0 1 0-1.28 1.536l3 2.5a1 1 0 0 0 1.42-.143l6-7.5a1 1 0 0 0-.155-1.406Z"/></symbol><symbol id="icon-eds-i-chevron-down-medium" viewBox="0 0 24 24"><path d="M3.305 8.28a1 1 0 0 0-.024 1.415l7.495 7.762c.314.345.757.543 1.224.543.467 0 .91-.198 1.204-.522l7.515-7.783a1 1 0 1 0-1.438-1.39L12 15.845l-7.28-7.54A1 1 0 0 0 3.4 8.2l-.096.082Z"/></symbol><symbol id="icon-eds-i-chevron-down-small" viewBox="0 0 16 16"><path d="M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.491 1.491 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111l.096.081Z"/></symbol><symbol id="icon-eds-i-chevron-left-medium" viewBox="0 0 24 24"><path d="M15.72 3.305a1 1 0 0 0-1.415-.024l-7.762 7.495A1.655 1.655 0 0 0 6 12c0 .467.198.91.522 1.204l7.783 7.515a1 1 0 1 0 1.39-1.438L8.155 12l7.54-7.28A1 1 0 0 0 15.8 3.4l-.082-.096Z"/></symbol><symbol id="icon-eds-i-chevron-left-small" viewBox="0 0 16 16"><path d="M10.722 2.308a1 1 0 0 0-1.414-.03L4.49 6.897a1.491 1.491 0 0 0-.019 2.188l4.838 4.637a1 1 0 1 0 1.384-1.444L6.229 8l4.463-4.278a1 1 0 0 0 .111-1.318l-.081-.096Z"/></symbol><symbol id="icon-eds-i-chevron-right-medium" viewBox="0 0 24 24"><path d="M8.28 3.305a1 1 0 0 1 1.415-.024l7.762 7.495c.345.314.543.757.543 1.224 0 .467-.198.91-.522 1.204l-7.783 7.515a1 1 0 1 1-1.39-1.438L15.845 12l-7.54-7.28A1 1 0 0 1 8.2 3.4l.082-.096Z"/></symbol><symbol id="icon-eds-i-chevron-right-small" viewBox="0 0 16 16"><path d="M5.278 2.308a1 1 0 0 1 1.414-.03l4.819 4.619a1.491 1.491 0 0 1 .019 2.188l-4.838 4.637a1 1 0 1 1-1.384-1.444L9.771 8 5.308 3.722a1 1 0 0 1-.111-1.318l.081-.096Z"/></symbol><symbol id="icon-eds-i-chevron-up-medium" viewBox="0 0 24 24"><path d="M20.695 15.72a1 1 0 0 0 .024-1.415l-7.495-7.762A1.655 1.655 0 0 0 12 6c-.467 0-.91.198-1.204.522l-7.515 7.783a1 1 0 1 0 1.438 1.39L12 8.155l7.28 7.54a1 1 0 0 0 1.319.106l.096-.082Z"/></symbol><symbol id="icon-eds-i-chevron-up-small" viewBox="0 0 16 16"><path d="M13.692 10.722a1 1 0 0 0 .03-1.414L9.103 4.49a1.491 1.491 0 0 0-2.188-.019L2.278 9.308a1 1 0 0 0 1.444 1.384L8 6.229l4.278 4.463a1 1 0 0 0 1.318.111l.096-.081Z"/></symbol><symbol id="icon-eds-i-citations-medium" viewBox="0 0 24 24"><path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM5.483 14.35c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Zm5 0c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Z"/></symbol><symbol id="icon-eds-i-clipboard-check-medium" viewBox="0 0 24 24"><path d="M14.4 1c1.238 0 2.274.865 2.536 2.024L18.5 3C19.886 3 21 4.14 21 5.535v14.93C21 21.86 19.886 23 18.5 23h-13C4.114 23 3 21.86 3 20.465V5.535C3 4.14 4.114 3 5.5 3h1.57c.27-1.147 1.3-2 2.53-2h4.8Zm4.115 4-1.59.024A2.601 2.601 0 0 1 14.4 7H9.6c-1.23 0-2.26-.853-2.53-2H5.5c-.27 0-.5.234-.5.535v14.93c0 .3.23.535.5.535h13c.27 0 .5-.234.5-.535V5.535c0-.3-.23-.535-.485-.535Zm-1.909 4.205a1 1 0 0 1 .19 1.401l-5.334 7a1 1 0 0 1-1.344.23l-2.667-1.75a1 1 0 1 1 1.098-1.672l1.887 1.238 4.769-6.258a1 1 0 0 1 1.401-.19ZM14.4 3H9.6a.6.6 0 0 0-.6.6v.8a.6.6 0 0 0 .6.6h4.8a.6.6 0 0 0 .6-.6v-.8a.6.6 0 0 0-.6-.6Z"/></symbol><symbol id="icon-eds-i-clipboard-report-medium" viewBox="0 0 24 24"><path d="M14.4 1c1.238 0 2.274.865 2.536 2.024L18.5 3C19.886 3 21 4.14 21 5.535v14.93C21 21.86 19.886 23 18.5 23h-13C4.114 23 3 21.86 3 20.465V5.535C3 4.14 4.114 3 5.5 3h1.57c.27-1.147 1.3-2 2.53-2h4.8Zm4.115 4-1.59.024A2.601 2.601 0 0 1 14.4 7H9.6c-1.23 0-2.26-.853-2.53-2H5.5c-.27 0-.5.234-.5.535v14.93c0 .3.23.535.5.535h13c.27 0 .5-.234.5-.535V5.535c0-.3-.23-.535-.485-.535Zm-2.658 10.929a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h7.857Zm0-3.929a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h7.857ZM14.4 3H9.6a.6.6 0 0 0-.6.6v.8a.6.6 0 0 0 .6.6h4.8a.6.6 0 0 0 .6-.6v-.8a.6.6 0 0 0-.6-.6Z"/></symbol><symbol id="icon-eds-i-close-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18ZM8.707 7.293 12 10.585l3.293-3.292a1 1 0 0 1 1.414 1.414L13.415 12l3.292 3.293a1 1 0 0 1-1.414 1.414L12 13.415l-3.293 3.292a1 1 0 1 1-1.414-1.414L10.585 12 7.293 8.707a1 1 0 0 1 1.414-1.414Z"/></symbol><symbol id="icon-eds-i-cloud-upload-medium" viewBox="0 0 24 24"><path d="m12.852 10.011.028-.004L13 10l.075.003.126.017.086.022.136.052.098.052.104.074.082.073 3 3a1 1 0 0 1 0 1.414l-.094.083a1 1 0 0 1-1.32-.083L14 13.416V20a1 1 0 0 1-2 0v-6.586l-1.293 1.293a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l3-3 .112-.097.11-.071.114-.054.105-.035.118-.025Zm.587-7.962c3.065.362 5.497 2.662 5.992 5.562l.013.085.207.073c2.117.782 3.496 2.845 3.337 5.097l-.022.226c-.297 2.561-2.503 4.491-5.124 4.502a1 1 0 1 1-.009-2c1.619-.007 2.967-1.186 3.147-2.733.179-1.542-.86-2.979-2.487-3.353-.512-.149-.894-.579-.981-1.165-.21-2.237-2-4.035-4.308-4.308-2.31-.273-4.497 1.06-5.25 3.19l-.049.113c-.234.468-.718.756-1.176.743-1.418.057-2.689.857-3.32 2.084a3.668 3.668 0 0 0 .262 3.798c.796 1.136 2.169 1.764 3.583 1.635a1 1 0 1 1 .182 1.992c-2.125.194-4.193-.753-5.403-2.48a5.668 5.668 0 0 1-.403-5.86c.85-1.652 2.449-2.79 4.323-3.092l.287-.039.013-.028c1.207-2.741 4.125-4.404 7.186-4.042Z"/></symbol><symbol id="icon-eds-i-collection-medium" viewBox="0 0 24 24"><path d="M21 7a1 1 0 0 1 1 1v12.5a2.5 2.5 0 0 1-2.5 2.5H8a1 1 0 0 1 0-2h11.5a.5.5 0 0 0 .5-.5V8a1 1 0 0 1 1-1Zm-5.5-5A2.5 2.5 0 0 1 18 4.5v12a2.5 2.5 0 0 1-2.5 2.5h-11A2.5 2.5 0 0 1 2 16.5v-12A2.5 2.5 0 0 1 4.5 2h11Zm0 2h-11a.5.5 0 0 0-.5.5v12a.5.5 0 0 0 .5.5h11a.5.5 0 0 0 .5-.5v-12a.5.5 0 0 0-.5-.5ZM13 13a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h6Zm0-3.5a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h6ZM13 6a1 1 0 0 1 0 2H7a1 1 0 1 1 0-2h6Z"/></symbol><symbol id="icon-eds-i-conference-series-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M4.5 2A2.5 2.5 0 0 0 2 4.5v11A2.5 2.5 0 0 0 4.5 18h2.37l-2.534 2.253a1 1 0 0 0 1.328 1.494L9.88 18H11v3a1 1 0 1 0 2 0v-3h1.12l4.216 3.747a1 1 0 0 0 1.328-1.494L17.13 18h2.37a2.5 2.5 0 0 0 2.5-2.5v-11A2.5 2.5 0 0 0 19.5 2h-15ZM20 6V4.5a.5.5 0 0 0-.5-.5h-15a.5.5 0 0 0-.5.5V6h16ZM4 8v7.5a.5.5 0 0 0 .5.5h15a.5.5 0 0 0 .5-.5V8H4Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-delivery-medium" viewBox="0 0 24 24"><path d="M8.51 20.598a3.037 3.037 0 0 1-3.02 0A2.968 2.968 0 0 1 4.161 19L3.5 19A2.5 2.5 0 0 1 1 16.5v-11A2.5 2.5 0 0 1 3.5 3h10a2.5 2.5 0 0 1 2.45 2.004L16 5h2.527c.976 0 1.855.585 2.27 1.49l2.112 4.62a1 1 0 0 1 .091.416v4.856C23 17.814 21.889 19 20.484 19h-.523a1.01 1.01 0 0 1-.121-.007 2.96 2.96 0 0 1-1.33 1.605 3.037 3.037 0 0 1-3.02 0A2.968 2.968 0 0 1 14.161 19H9.838a2.968 2.968 0 0 1-1.327 1.597Zm-2.024-3.462a.955.955 0 0 0-.481.73L5.999 18l.001.022a.944.944 0 0 0 .388.777l.098.065c.316.181.712.181 1.028 0A.97.97 0 0 0 8 17.978a.95.95 0 0 0-.486-.842 1.037 1.037 0 0 0-1.028 0Zm10 0a.955.955 0 0 0-.481.73l-.005.156a.944.944 0 0 0 .388.777l.098.065c.316.181.712.181 1.028 0a.97.97 0 0 0 .486-.886.95.95 0 0 0-.486-.842 1.037 1.037 0 0 0-1.028 0ZM21 12h-5v3.17a3.038 3.038 0 0 1 2.51.232 2.993 2.993 0 0 1 1.277 1.45l.058.155.058-.005.581-.002c.27 0 .516-.263.516-.618V12Zm-7.5-7h-10a.5.5 0 0 0-.5.5v11a.5.5 0 0 0 .5.5h.662a2.964 2.964 0 0 1 1.155-1.491l.172-.107a3.037 3.037 0 0 1 3.022 0A2.987 2.987 0 0 1 9.843 17H13.5a.5.5 0 0 0 .5-.5v-11a.5.5 0 0 0-.5-.5Zm5.027 2H16v3h4.203l-1.224-2.677a.532.532 0 0 0-.375-.316L18.527 7Z"/></symbol><symbol id="icon-eds-i-download-medium" viewBox="0 0 24 24"><path d="M22 18.5a3.5 3.5 0 0 1-3.5 3.5h-13A3.5 3.5 0 0 1 2 18.5V18a1 1 0 0 1 2 0v.5A1.5 1.5 0 0 0 5.5 20h13a1.5 1.5 0 0 0 1.5-1.5V18a1 1 0 0 1 2 0v.5Zm-3.293-7.793-6 6-.063.059-.093.069-.081.048-.105.049-.104.034-.056.013-.118.017L12 17l-.076-.003-.122-.017-.113-.03-.085-.032-.063-.03-.098-.058-.06-.043-.05-.043-6.04-6.037a1 1 0 0 1 1.414-1.414l4.294 4.29L11 3a1 1 0 0 1 2 0l.001 10.585 4.292-4.292a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414Z"/></symbol><symbol id="icon-eds-i-edit-medium" viewBox="0 0 24 24"><path d="M17.149 2a2.38 2.38 0 0 1 1.699.711l2.446 2.46a2.384 2.384 0 0 1 .005 3.38L10.01 19.906a1 1 0 0 1-.434.257l-6.3 1.8a1 1 0 0 1-1.237-1.237l1.8-6.3a1 1 0 0 1 .257-.434L15.443 2.718A2.385 2.385 0 0 1 17.15 2Zm-3.874 5.689-7.586 7.536-1.234 4.319 4.318-1.234 7.54-7.582-3.038-3.039ZM17.149 4a.395.395 0 0 0-.286.126L14.695 6.28l3.029 3.029 2.162-2.173a.384.384 0 0 0 .106-.197L20 6.864c0-.103-.04-.2-.119-.278l-2.457-2.47A.385.385 0 0 0 17.149 4Z"/></symbol><symbol id="icon-eds-i-education-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M12.41 2.088a1 1 0 0 0-.82 0l-10 4.5a1 1 0 0 0 0 1.824L3 9.047v7.124A3.001 3.001 0 0 0 4 22a3 3 0 0 0 1-5.83V9.948l1 .45V14.5a1 1 0 0 0 .087.408L7 14.5c-.913.408-.912.41-.912.41l.001.003.003.006.007.015a1.988 1.988 0 0 0 .083.16c.054.097.131.225.236.373.21.297.53.68.993 1.057C8.351 17.292 9.824 18 12 18c2.176 0 3.65-.707 4.589-1.476.463-.378.783-.76.993-1.057a4.162 4.162 0 0 0 .319-.533l.007-.015.003-.006v-.003h.002s0-.002-.913-.41l.913.408A1 1 0 0 0 18 14.5v-4.103l4.41-1.985a1 1 0 0 0 0-1.824l-10-4.5ZM16 11.297l-3.59 1.615a1 1 0 0 1-.82 0L8 11.297v2.94a3.388 3.388 0 0 0 .677.739C9.267 15.457 10.294 16 12 16s2.734-.543 3.323-1.024a3.388 3.388 0 0 0 .677-.739v-2.94ZM4.437 7.5 12 4.097 19.563 7.5 12 10.903 4.437 7.5ZM3 19a1 1 0 1 1 2 0 1 1 0 0 1-2 0Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-error-diamond-medium" viewBox="0 0 24 24"><path d="M12.002 1c.702 0 1.375.279 1.871.775l8.35 8.353a2.646 2.646 0 0 1 .001 3.744l-8.353 8.353a2.646 2.646 0 0 1-3.742 0l-8.353-8.353a2.646 2.646 0 0 1 0-3.744l8.353-8.353.156-.142c.424-.362.952-.58 1.507-.625l.21-.008Zm0 2a.646.646 0 0 0-.38.123l-.093.08-8.34 8.34a.646.646 0 0 0-.18.355L3 12c0 .171.068.336.19.457l8.353 8.354a.646.646 0 0 0 .914 0l8.354-8.354a.646.646 0 0 0-.001-.914l-8.351-8.354A.646.646 0 0 0 12.002 3ZM12 14.5a1.5 1.5 0 0 1 .144 2.993L12 17.5a1.5 1.5 0 0 1 0-3ZM12 6a1 1 0 0 1 1 1v5a1 1 0 0 1-2 0V7a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-error-filled-medium" viewBox="0 0 24 24"><path d="M12.002 1c.702 0 1.375.279 1.871.775l8.35 8.353a2.646 2.646 0 0 1 .001 3.744l-8.353 8.353a2.646 2.646 0 0 1-3.742 0l-8.353-8.353a2.646 2.646 0 0 1 0-3.744l8.353-8.353.156-.142c.424-.362.952-.58 1.507-.625l.21-.008ZM12 14.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 14.5ZM12 6a1 1 0 0 0-1 1v5a1 1 0 0 0 2 0V7a1 1 0 0 0-1-1Z"/></symbol><symbol id="icon-eds-i-external-link-medium" viewBox="0 0 24 24"><path d="M9 2a1 1 0 1 1 0 2H4.6c-.371 0-.6.209-.6.5v15c0 .291.229.5.6.5h14.8c.371 0 .6-.209.6-.5V15a1 1 0 0 1 2 0v4.5c0 1.438-1.162 2.5-2.6 2.5H4.6C3.162 22 2 20.938 2 19.5v-15C2 3.062 3.162 2 4.6 2H9Zm6 0h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L22 3v6a1 1 0 0 1-2 0V5.414l-6.693 6.693a1 1 0 0 1-1.414-1.414L18.584 4H15a1 1 0 0 1-.993-.883L14 3a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-external-link-small" viewBox="0 0 16 16"><path d="M5 1a1 1 0 1 1 0 2l-2-.001V13L13 13v-2a1 1 0 0 1 2 0v2c0 1.15-.93 2-2.067 2H3.067C1.93 15 1 14.15 1 13V3c0-1.15.93-2 2.067-2H5Zm4 0h5l.075.003.126.017.111.03.111.044.098.052.096.067.09.08.044.047.073.093.051.083.054.113.035.105.03.148L15 2v5a1 1 0 0 1-2 0V4.414L9.107 8.307a1 1 0 0 1-1.414-1.414L11.584 3H9a1 1 0 0 1-.993-.883L8 2a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-file-download-medium" viewBox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3ZM12 7a1 1 0 0 1 1 1v6.585l2.293-2.292a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414l-4 4a1.008 1.008 0 0 1-.112.097l-.11.071-.114.054-.105.035-.149.03L12 18l-.075-.003-.126-.017-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08-4-4a1 1 0 0 1 1.414-1.414L11 14.585V8a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-file-report-medium" viewBox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742H5.545c-.674 0-1.32-.267-1.798-.742A2.535 2.535 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .142.057.278.158.379.102.102.242.159.387.159h12.91a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.915L14.085 3ZM16 17a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm0-3a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm-4.793-6.207L13 9.585l1.793-1.792a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414l-2.5 2.5a1 1 0 0 1-1.414 0L10.5 9.915l-1.793 1.792a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l2.5-2.5a1 1 0 0 1 1.414 0Z"/></symbol><symbol id="icon-eds-i-file-text-medium" viewBox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3ZM16 15a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm0-4a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm-5-4a1 1 0 0 1 0 2H8a1 1 0 1 1 0-2h3Z"/></symbol><symbol id="icon-eds-i-file-upload-medium" viewBox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3Zm-2.233 4.011.058-.007L12 7l.075.003.126.017.111.03.111.044.098.052.104.074.082.073 4 4a1 1 0 0 1 0 1.414l-.094.083a1 1 0 0 1-1.32-.083L13 10.415V17a1 1 0 0 1-2 0v-6.585l-2.293 2.292a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l4-4 .112-.097.11-.071.114-.054.105-.035.118-.025Z"/></symbol><symbol id="icon-eds-i-filter-medium" viewBox="0 0 24 24"><path d="M21 2a1 1 0 0 1 .82 1.573L15 13.314V18a1 1 0 0 1-.31.724l-.09.076-4 3A1 1 0 0 1 9 21v-7.684L2.18 3.573a1 1 0 0 1 .707-1.567L3 2h18Zm-1.921 2H4.92l5.9 8.427a1 1 0 0 1 .172.45L11 13v6l2-1.5V13a1 1 0 0 1 .117-.469l.064-.104L19.079 4Z"/></symbol><symbol id="icon-eds-i-funding-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M23 8A7 7 0 1 0 9 8a7 7 0 0 0 14 0ZM9.006 12.225A4.07 4.07 0 0 0 6.12 11.02H2a.979.979 0 1 0 0 1.958h4.12c.558 0 1.094.222 1.489.617l2.207 2.288c.27.27.27.687.012.944a.656.656 0 0 1-.928 0L7.744 15.67a.98.98 0 0 0-1.386 1.384l1.157 1.158c.535.536 1.244.791 1.946.765l.041.002h6.922c.874 0 1.597.748 1.597 1.688 0 .203-.146.354-.309.354H7.755c-.487 0-.96-.178-1.339-.504L2.64 17.259a.979.979 0 0 0-1.28 1.482L5.137 22c.733.631 1.66.979 2.618.979h9.957c1.26 0 2.267-1.043 2.267-2.312 0-2.006-1.584-3.646-3.555-3.646h-4.529a2.617 2.617 0 0 0-.681-2.509l-2.208-2.287ZM16 3a5 5 0 1 0 0 10 5 5 0 0 0 0-10Zm.979 3.5a.979.979 0 1 0-1.958 0v3a.979.979 0 1 0 1.958 0v-3Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-hashtag-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18ZM9.52 18.189a1 1 0 1 1-1.964-.378l.437-2.274H6a1 1 0 1 1 0-2h2.378l.592-3.076H6a1 1 0 0 1 0-2h3.354l.51-2.65a1 1 0 1 1 1.964.378l-.437 2.272h3.04l.51-2.65a1 1 0 1 1 1.964.378l-.438 2.272H18a1 1 0 0 1 0 2h-1.917l-.592 3.076H18a1 1 0 0 1 0 2h-2.893l-.51 2.652a1 1 0 1 1-1.964-.378l.437-2.274h-3.04l-.51 2.652Zm.895-4.652h3.04l.591-3.076h-3.04l-.591 3.076Z"/></symbol><symbol id="icon-eds-i-home-medium" viewBox="0 0 24 24"><path d="M5 22a1 1 0 0 1-1-1v-8.586l-1.293 1.293a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l10-10a1 1 0 0 1 1.414 0l10 10a1 1 0 0 1-1.414 1.414L20 12.415V21a1 1 0 0 1-1 1H5Zm7-17.585-6 5.999V20h5v-4a1 1 0 0 1 2 0v4h5v-9.585l-6-6Z"/></symbol><symbol id="icon-eds-i-image-medium" viewBox="0 0 24 24"><path d="M19.615 2A2.385 2.385 0 0 1 22 4.385v15.23A2.385 2.385 0 0 1 19.615 22H4.385A2.385 2.385 0 0 1 2 19.615V4.385A2.385 2.385 0 0 1 4.385 2h15.23Zm0 2H4.385A.385.385 0 0 0 4 4.385v15.23c0 .213.172.385.385.385h1.244l10.228-8.76a1 1 0 0 1 1.254-.037L20 13.392V4.385A.385.385 0 0 0 19.615 4Zm-3.07 9.283L8.703 20h10.912a.385.385 0 0 0 .385-.385v-3.713l-3.455-2.619ZM9.5 6a3.5 3.5 0 1 1 0 7 3.5 3.5 0 0 1 0-7Zm0 2a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3Z"/></symbol><symbol id="icon-eds-i-impact-factor-medium" viewBox="0 0 24 24"><path d="M16.49 2.672c.74.694.986 1.765.632 2.712l-.04.1-1.549 3.54h1.477a2.496 2.496 0 0 1 2.485 2.34l.005.163c0 .618-.23 1.21-.642 1.675l-7.147 7.961a2.48 2.48 0 0 1-3.554.165 2.512 2.512 0 0 1-.633-2.712l.042-.103L9.108 15H7.46c-1.393 0-2.379-1.11-2.455-2.369L5 12.473c0-.593.142-1.145.628-1.692l7.307-7.944a2.48 2.48 0 0 1 3.555-.165ZM14.43 4.164l-7.33 7.97c-.083.093-.101.214-.101.34 0 .277.19.526.46.526h4.163l.097-.009c.015 0 .03.003.046.009.181.078.264.32.186.5l-2.554 5.817a.512.512 0 0 0 .127.552.48.48 0 0 0 .69-.033l7.155-7.97a.513.513 0 0 0 .13-.34.497.497 0 0 0-.49-.502h-3.988a.355.355 0 0 1-.328-.497l2.555-5.844a.512.512 0 0 0-.127-.552.48.48 0 0 0-.69.033Z"/></symbol><symbol id="icon-eds-i-info-circle-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm0 7a1 1 0 0 1 1 1v5h1.5a1 1 0 0 1 0 2h-5a1 1 0 0 1 0-2H11v-4h-.5a1 1 0 0 1-.993-.883L9.5 11a1 1 0 0 1 1-1H12Zm0-4.5a1.5 1.5 0 0 1 .144 2.993L12 8.5a1.5 1.5 0 0 1 0-3Z"/></symbol><symbol id="icon-eds-i-info-filled-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 9h-1.5a1 1 0 0 0-1 1l.007.117A1 1 0 0 0 10.5 12h.5v4H9.5a1 1 0 0 0 0 2h5a1 1 0 0 0 0-2H13v-5a1 1 0 0 0-1-1Zm0-4.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 5.5Z"/></symbol><symbol id="icon-eds-i-journal-medium" viewBox="0 0 24 24"><path d="M18.5 1A2.5 2.5 0 0 1 21 3.5v14a2.5 2.5 0 0 1-2.5 2.5h-13a.5.5 0 1 0 0 1H20a1 1 0 0 1 0 2H5.5A2.5 2.5 0 0 1 3 20.5v-17A2.5 2.5 0 0 1 5.5 1h13ZM7 3H5.5a.5.5 0 0 0-.5.5v14.549l.016-.002c.104-.02.211-.035.32-.042L5.5 18H7V3Zm11.5 0H9v15h9.5a.5.5 0 0 0 .5-.5v-14a.5.5 0 0 0-.5-.5ZM16 5a1 1 0 0 1 1 1v4a1 1 0 0 1-1 1h-5a1 1 0 0 1-1-1V6a1 1 0 0 1 1-1h5Zm-1 2h-3v2h3V7Z"/></symbol><symbol id="icon-eds-i-mail-medium" viewBox="0 0 24 24"><path d="M20.462 3C21.875 3 23 4.184 23 5.619v12.762C23 19.816 21.875 21 20.462 21H3.538C2.125 21 1 19.816 1 18.381V5.619C1 4.184 2.125 3 3.538 3h16.924ZM21 8.158l-7.378 6.258a2.549 2.549 0 0 1-3.253-.008L3 8.16v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619V8.158ZM20.462 5H3.538c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516Z"/></symbol><symbol id="icon-eds-i-mail-send-medium" viewBox="0 0 24 24"><path d="M20.444 5a2.562 2.562 0 0 1 2.548 2.37l.007.078.001.123v7.858A2.564 2.564 0 0 1 20.444 18H9.556A2.564 2.564 0 0 1 7 15.429l.001-7.977.007-.082A2.561 2.561 0 0 1 9.556 5h10.888ZM21 9.331l-5.46 3.51a1 1 0 0 1-1.08 0L9 9.332v6.097c0 .317.251.571.556.571h10.888a.564.564 0 0 0 .556-.571V9.33ZM20.444 7H9.556a.543.543 0 0 0-.32.105l5.763 3.706 5.766-3.706a.543.543 0 0 0-.32-.105ZM4.308 5a1 1 0 1 1 0 2H2a1 1 0 1 1 0-2h2.308Zm0 5.5a1 1 0 0 1 0 2H2a1 1 0 0 1 0-2h2.308Zm0 5.5a1 1 0 0 1 0 2H2a1 1 0 0 1 0-2h2.308Z"/></symbol><symbol id="icon-eds-i-mentions-medium" viewBox="0 0 24 24"><path d="m9.452 1.293 5.92 5.92 2.92-2.92a1 1 0 0 1 1.415 1.414l-2.92 2.92 5.92 5.92a1 1 0 0 1 0 1.415 10.371 10.371 0 0 1-10.378 2.584l.652 3.258A1 1 0 0 1 12 23H2a1 1 0 0 1-.874-1.486l4.789-8.62C4.194 9.074 4.9 4.43 8.038 1.292a1 1 0 0 1 1.414 0Zm-2.355 13.59L3.699 21h7.081l-.689-3.442a10.392 10.392 0 0 1-2.775-2.396l-.22-.28Zm1.69-11.427-.07.09a8.374 8.374 0 0 0 11.737 11.737l.089-.071L8.787 3.456Z"/></symbol><symbol id="icon-eds-i-menu-medium" viewBox="0 0 24 24"><path d="M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2h18Zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h14Zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h18Z"/></symbol><symbol id="icon-eds-i-metrics-medium" viewBox="0 0 24 24"><path d="M3 22a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v7h4V8a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v13a1 1 0 0 1-.883.993L21 22H3Zm17-2V9h-4v11h4Zm-6-8h-4v8h4v-8ZM8 4H4v16h4V4Z"/></symbol><symbol id="icon-eds-i-news-medium" viewBox="0 0 24 24"><path d="M17.384 3c.975 0 1.77.787 1.77 1.762v13.333c0 .462.354.846.815.899l.107.006.109-.006a.915.915 0 0 0 .809-.794l.006-.105V8.19a1 1 0 0 1 2 0v9.905A2.914 2.914 0 0 1 20.077 21H3.538a2.547 2.547 0 0 1-1.644-.601l-.147-.135A2.516 2.516 0 0 1 1 18.476V4.762C1 3.787 1.794 3 2.77 3h14.614Zm-.231 2H3v13.476c0 .11.035.216.1.304l.054.063c.101.1.24.157.384.157l13.761-.001-.026-.078a2.88 2.88 0 0 1-.115-.655l-.004-.17L17.153 5ZM14 15.021a.979.979 0 1 1 0 1.958H6a.979.979 0 1 1 0-1.958h8Zm0-8c.54 0 .979.438.979.979v4c0 .54-.438.979-.979.979H6A.979.979 0 0 1 5.021 12V8c0-.54.438-.979.979-.979h8Zm-.98 1.958H6.979v2.041h6.041V8.979Z"/></symbol><symbol id="icon-eds-i-newsletter-medium" viewBox="0 0 24 24"><path d="M21 10a1 1 0 0 1 1 1v9.5a2.5 2.5 0 0 1-2.5 2.5h-15A2.5 2.5 0 0 1 2 20.5V11a1 1 0 0 1 2 0v.439l8 4.888 8-4.889V11a1 1 0 0 1 1-1Zm-1 3.783-7.479 4.57a1 1 0 0 1-1.042 0l-7.48-4.57V20.5a.5.5 0 0 0 .501.5h15a.5.5 0 0 0 .5-.5v-6.717ZM15 9a1 1 0 0 1 0 2H9a1 1 0 0 1 0-2h6Zm2.5-8A2.5 2.5 0 0 1 20 3.5V9a1 1 0 0 1-2 0V3.5a.5.5 0 0 0-.5-.5h-11a.5.5 0 0 0-.5.5V9a1 1 0 1 1-2 0V3.5A2.5 2.5 0 0 1 6.5 1h11ZM15 5a1 1 0 0 1 0 2H9a1 1 0 1 1 0-2h6Z"/></symbol><symbol id="icon-eds-i-notifcation-medium" viewBox="0 0 24 24"><path d="M14 20a1 1 0 0 1 0 2h-4a1 1 0 0 1 0-2h4ZM3 18l-.133-.007c-1.156-.124-1.156-1.862 0-1.986l.3-.012C4.32 15.923 5 15.107 5 14V9.5C5 5.368 8.014 2 12 2s7 3.368 7 7.5V14c0 1.107.68 1.923 1.832 1.995l.301.012c1.156.124 1.156 1.862 0 1.986L21 18H3Zm9-14C9.17 4 7 6.426 7 9.5V14c0 .671-.146 1.303-.416 1.858L6.51 16h10.979l-.073-.142a4.192 4.192 0 0 1-.412-1.658L17 14V9.5C17 6.426 14.83 4 12 4Z"/></symbol><symbol id="icon-eds-i-publish-medium" viewBox="0 0 24 24"><g><path d="M16.296 1.291A1 1 0 0 0 15.591 1H5.545A2.542 2.542 0 0 0 3 3.538V13a1 1 0 1 0 2 0V3.538l.007-.087A.543.543 0 0 1 5.545 3h9.633L20 7.8v12.662a.534.534 0 0 1-.158.379.548.548 0 0 1-.387.159H11a1 1 0 1 0 0 2h8.455c.674 0 1.32-.267 1.798-.742A2.534 2.534 0 0 0 22 20.462V7.385a1 1 0 0 0-.294-.709l-5.41-5.385Z"/><path d="M10.762 16.647a1 1 0 0 0-1.525-1.294l-4.472 5.271-2.153-1.665a1 1 0 1 0-1.224 1.582l2.91 2.25a1 1 0 0 0 1.374-.144l5.09-6ZM16 10a1 1 0 1 1 0 2H8a1 1 0 1 1 0-2h8ZM12 7a1 1 0 0 0-1-1H8a1 1 0 1 0 0 2h3a1 1 0 0 0 1-1Z"/></g></symbol><symbol id="icon-eds-i-refresh-medium" viewBox="0 0 24 24"><g><path d="M7.831 5.636H6.032A8.76 8.76 0 0 1 9 3.631 8.549 8.549 0 0 1 12.232 3c.603 0 1.192.063 1.76.182C17.979 4.017 21 7.632 21 12a1 1 0 1 0 2 0c0-5.296-3.674-9.746-8.591-10.776A10.61 10.61 0 0 0 5 3.851V2.805a1 1 0 0 0-.987-1H4a1 1 0 0 0-1 1v3.831a1 1 0 0 0 1 1h3.831a1 1 0 0 0 .013-2h-.013ZM17.968 18.364c-1.59 1.632-3.784 2.636-6.2 2.636C6.948 21 3 16.993 3 12a1 1 0 1 0-2 0c0 6.053 4.799 11 10.768 11 2.788 0 5.324-1.082 7.232-2.85v1.045a1 1 0 1 0 2 0v-3.831a1 1 0 0 0-1-1h-3.831a1 1 0 0 0 0 2h1.799Z"/></g></symbol><symbol id="icon-eds-i-search-medium" viewBox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z"/></symbol><symbol id="icon-eds-i-settings-medium" viewBox="0 0 24 24"><path d="M11.382 1h1.24a2.508 2.508 0 0 1 2.334 1.63l.523 1.378 1.59.933 1.444-.224c.954-.132 1.89.3 2.422 1.101l.095.155.598 1.066a2.56 2.56 0 0 1-.195 2.848l-.894 1.161v1.896l.92 1.163c.6.768.707 1.812.295 2.674l-.09.17-.606 1.08a2.504 2.504 0 0 1-2.531 1.25l-1.428-.223-1.589.932-.523 1.378a2.512 2.512 0 0 1-2.155 1.625L12.65 23h-1.27a2.508 2.508 0 0 1-2.334-1.63l-.524-1.379-1.59-.933-1.443.225c-.954.132-1.89-.3-2.422-1.101l-.095-.155-.598-1.066a2.56 2.56 0 0 1 .195-2.847l.891-1.161v-1.898l-.919-1.162a2.562 2.562 0 0 1-.295-2.674l.09-.17.606-1.08a2.504 2.504 0 0 1 2.531-1.25l1.43.223 1.618-.938.524-1.375.07-.167A2.507 2.507 0 0 1 11.382 1Zm.003 2a.509.509 0 0 0-.47.338l-.65 1.71a1 1 0 0 1-.434.51L7.6 6.85a1 1 0 0 1-.655.123l-1.762-.275a.497.497 0 0 0-.498.252l-.61 1.088a.562.562 0 0 0 .04.619l1.13 1.43a1 1 0 0 1 .216.62v2.585a1 1 0 0 1-.207.61L4.15 15.339a.568.568 0 0 0-.036.634l.601 1.072a.494.494 0 0 0 .484.26l1.78-.278a1 1 0 0 1 .66.126l2.2 1.292a1 1 0 0 1 .43.507l.648 1.71a.508.508 0 0 0 .467.338h1.263a.51.51 0 0 0 .47-.34l.65-1.708a1 1 0 0 1 .428-.507l2.201-1.292a1 1 0 0 1 .66-.126l1.763.275a.497.497 0 0 0 .498-.252l.61-1.088a.562.562 0 0 0-.04-.619l-1.13-1.43a1 1 0 0 1-.216-.62v-2.585a1 1 0 0 1 .207-.61l1.105-1.437a.568.568 0 0 0 .037-.634l-.601-1.072a.494.494 0 0 0-.484-.26l-1.78.278a1 1 0 0 1-.66-.126l-2.2-1.292a1 1 0 0 1-.43-.507l-.649-1.71A.508.508 0 0 0 12.62 3h-1.234ZM12 8a4 4 0 1 1 0 8 4 4 0 0 1 0-8Zm0 2a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"/></symbol><symbol id="icon-eds-i-shipping-medium" viewBox="0 0 24 24"><path d="M16.515 2c1.406 0 2.706.728 3.352 1.902l2.02 3.635.02.042.036.089.031.105.012.058.01.073.004.075v11.577c0 .64-.244 1.255-.683 1.713a2.356 2.356 0 0 1-1.701.731H4.386a2.356 2.356 0 0 1-1.702-.731 2.476 2.476 0 0 1-.683-1.713V7.948c.01-.217.083-.43.22-.6L4.2 3.905C4.833 2.755 6.089 2.032 7.486 2h9.029ZM20 9H4v10.556a.49.49 0 0 0 .075.26l.053.07a.356.356 0 0 0 .257.114h15.23c.094 0 .186-.04.258-.115a.477.477 0 0 0 .127-.33V9Zm-2 7.5a1 1 0 0 1 0 2h-4a1 1 0 0 1 0-2h4ZM16.514 4H13v3h6.3l-1.183-2.13c-.288-.522-.908-.87-1.603-.87ZM11 3.999H7.51c-.679.017-1.277.36-1.566.887L4.728 7H11V3.999Z"/></symbol><symbol id="icon-eds-i-step-guide-medium" viewBox="0 0 24 24"><path d="M11.394 9.447a1 1 0 1 0-1.788-.894l-.88 1.759-.019-.02a1 1 0 1 0-1.414 1.415l1 1a1 1 0 0 0 1.601-.26l1.5-3ZM12 11a1 1 0 0 1 1-1h3a1 1 0 1 1 0 2h-3a1 1 0 0 1-1-1ZM12 17a1 1 0 0 1 1-1h3a1 1 0 1 1 0 2h-3a1 1 0 0 1-1-1ZM10.947 14.105a1 1 0 0 1 .447 1.342l-1.5 3a1 1 0 0 1-1.601.26l-1-1a1 1 0 1 1 1.414-1.414l.02.019.879-1.76a1 1 0 0 1 1.341-.447Z"/><path d="M5.545 1A2.542 2.542 0 0 0 3 3.538v16.924A2.542 2.542 0 0 0 5.545 23h12.91A2.542 2.542 0 0 0 21 20.462V7.5a1 1 0 0 0-.293-.707l-5.5-5.5A1 1 0 0 0 14.5 1H5.545ZM5 3.538C5 3.245 5.24 3 5.545 3h8.54L19 7.914v12.547c0 .294-.24.539-.546.539H5.545A.542.542 0 0 1 5 20.462V3.538Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-submission-medium" viewBox="0 0 24 24"><g><path d="M5 3.538C5 3.245 5.24 3 5.545 3h9.633L20 7.8v12.662a.535.535 0 0 1-.158.379.549.549 0 0 1-.387.159H6a1 1 0 0 1-1-1v-2.5a1 1 0 1 0-2 0V20a3 3 0 0 0 3 3h13.455c.673 0 1.32-.266 1.798-.742A2.535 2.535 0 0 0 22 20.462V7.385a1 1 0 0 0-.294-.709l-5.41-5.385A1 1 0 0 0 15.591 1H5.545A2.542 2.542 0 0 0 3 3.538V7a1 1 0 0 0 2 0V3.538Z"/><path d="m13.707 13.707-4 4a1 1 0 0 1-1.414 0l-.083-.094a1 1 0 0 1 .083-1.32L10.585 14 2 14a1 1 0 1 1 0-2l8.583.001-2.29-2.294a1 1 0 0 1 1.414-1.414l4.037 4.04.043.05.043.06.059.098.03.063.031.085.03.113.017.122L14 13l-.004.087-.017.118-.013.056-.034.104-.049.105-.048.081-.07.093-.058.063Z"/></g></symbol><symbol id="icon-eds-i-table-1-medium" viewBox="0 0 24 24"><path d="M4.385 22a2.56 2.56 0 0 1-1.14-.279C2.485 21.341 2 20.614 2 19.615V4.385c0-.315.067-.716.279-1.14C2.659 2.485 3.386 2 4.385 2h15.23c.315 0 .716.067 1.14.279.76.38 1.245 1.107 1.245 2.106v15.23c0 .315-.067.716-.279 1.14-.38.76-1.107 1.245-2.106 1.245H4.385ZM4 19.615c0 .213.034.265.14.317a.71.71 0 0 0 .245.068H8v-4H4v3.615ZM20 16H10v4h9.615c.213 0 .265-.034.317-.14a.71.71 0 0 0 .068-.245V16Zm0-2v-4H10v4h10ZM4 14h4v-4H4v4ZM19.615 4H10v4h10V4.385c0-.213-.034-.265-.14-.317A.71.71 0 0 0 19.615 4ZM8 4H4.385l-.082.002c-.146.01-.19.047-.235.138A.71.71 0 0 0 4 4.385V8h4V4Z"/></symbol><symbol id="icon-eds-i-table-2-medium" viewBox="0 0 24 24"><path d="M4.384 22A2.384 2.384 0 0 1 2 19.616V4.384A2.384 2.384 0 0 1 4.384 2h15.232A2.384 2.384 0 0 1 22 4.384v15.232A2.384 2.384 0 0 1 19.616 22H4.384ZM10 15H4v4.616c0 .212.172.384.384.384H10v-5Zm5 0h-3v5h3v-5Zm5 0h-3v5h2.616a.384.384 0 0 0 .384-.384V15ZM10 9H4v4h6V9Zm5 0h-3v4h3V9Zm5 0h-3v4h3V9Zm-.384-5H4.384A.384.384 0 0 0 4 4.384V7h16V4.384A.384.384 0 0 0 19.616 4Z"/></symbol><symbol id="icon-eds-i-tag-medium" viewBox="0 0 24 24"><path d="m12.621 1.998.127.004L20.496 2a1.5 1.5 0 0 1 1.497 1.355L22 3.5l-.005 7.669c.038.456-.133.905-.447 1.206l-9.02 9.018a2.075 2.075 0 0 1-2.932 0l-6.99-6.99a2.075 2.075 0 0 1 .001-2.933L11.61 2.47c.246-.258.573-.418.881-.46l.131-.011Zm.286 2-8.885 8.886a.075.075 0 0 0 0 .106l6.987 6.988c.03.03.077.03.106 0l8.883-8.883L19.999 4l-7.092-.002ZM16 6.5a1.5 1.5 0 0 1 .144 2.993L16 9.5a1.5 1.5 0 0 1 0-3Z"/></symbol><symbol id="icon-eds-i-trash-medium" viewBox="0 0 24 24"><path d="M12 1c2.717 0 4.913 2.232 4.997 5H21a1 1 0 0 1 0 2h-1v12.5c0 1.389-1.152 2.5-2.556 2.5H6.556C5.152 23 4 21.889 4 20.5V8H3a1 1 0 1 1 0-2h4.003l.001-.051C7.114 3.205 9.3 1 12 1Zm6 7H6v12.5c0 .238.19.448.454.492l.102.008h10.888c.315 0 .556-.232.556-.5V8Zm-4 3a1 1 0 0 1 1 1v6.005a1 1 0 0 1-2 0V12a1 1 0 0 1 1-1Zm-4 0a1 1 0 0 1 1 1v6a1 1 0 0 1-2 0v-6a1 1 0 0 1 1-1Zm2-8c-1.595 0-2.914 1.32-2.996 3h5.991v-.02C14.903 4.31 13.589 3 12 3Z"/></symbol><symbol id="icon-eds-i-user-account-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 16c-1.806 0-3.52.994-4.664 2.698A8.947 8.947 0 0 0 12 21a8.958 8.958 0 0 0 4.664-1.301C15.52 17.994 13.806 17 12 17Zm0-14a9 9 0 0 0-6.25 15.476C7.253 16.304 9.54 15 12 15s4.747 1.304 6.25 3.475A9 9 0 0 0 12 3Zm0 3a4 4 0 1 1 0 8 4 4 0 0 1 0-8Zm0 2a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"/></symbol><symbol id="icon-eds-i-user-add-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm9 10a1 1 0 0 1 1 1v3h3a1 1 0 0 1 0 2h-3v3a1 1 0 0 1-2 0v-3h-3a1 1 0 0 1 0-2h3v-3a1 1 0 0 1 1-1Zm-5.545-.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378Z"/></symbol><symbol id="icon-eds-i-user-assign-medium" viewBox="0 0 24 24"><path d="M16.226 13.298a1 1 0 0 1 1.414-.01l.084.093a1 1 0 0 1-.073 1.32L15.39 17H22a1 1 0 0 1 0 2h-6.611l2.262 2.298a1 1 0 0 1-1.425 1.404l-3.939-4a1 1 0 0 1 0-1.404l3.94-4Zm-3.771-.449a1 1 0 1 1-.91 1.781 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 10.5 20a1 1 0 0 1 .993.883L11.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Z"/></symbol><symbol id="icon-eds-i-user-block-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm9 10a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm-5.545-.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM15 18a3 3 0 0 0 4.294 2.707l-4.001-4c-.188.391-.293.83-.293 1.293Zm3-3c-.463 0-.902.105-1.294.293l4.001 4A3 3 0 0 0 18 15Z"/></symbol><symbol id="icon-eds-i-user-check-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm13.647 12.237a1 1 0 0 1 .116 1.41l-5.091 6a1 1 0 0 1-1.375.144l-2.909-2.25a1 1 0 1 1 1.224-1.582l2.153 1.665 4.472-5.271a1 1 0 0 1 1.41-.116Zm-8.139-.977c.22.214.428.44.622.678a1 1 0 1 1-1.548 1.266 6.025 6.025 0 0 0-1.795-1.49.86.86 0 0 1-.163-.048l-.079-.036a5.721 5.721 0 0 0-2.62-.63l-.194.006c-2.76.134-5.022 2.177-5.592 4.864l-.035.175-.035.213c-.03.201-.05.405-.06.61L3.003 20 10 20a1 1 0 0 1 .993.883L11 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876l.005-.223.02-.356.02-.222.03-.248.022-.15c.02-.133.044-.265.071-.397.44-2.178 1.725-4.105 3.595-5.301a7.75 7.75 0 0 1 3.755-1.215l.12-.004a7.908 7.908 0 0 1 5.87 2.252Z"/></symbol><symbol id="icon-eds-i-user-delete-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6ZM4.763 13.227a7.713 7.713 0 0 1 7.692-.378 1 1 0 1 1-.91 1.781 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20H11.5a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897Zm11.421 1.543 2.554 2.553 2.555-2.553a1 1 0 0 1 1.414 1.414l-2.554 2.554 2.554 2.555a1 1 0 0 1-1.414 1.414l-2.555-2.554-2.554 2.554a1 1 0 0 1-1.414-1.414l2.553-2.555-2.553-2.554a1 1 0 0 1 1.414-1.414Z"/></symbol><symbol id="icon-eds-i-user-edit-medium" viewBox="0 0 24 24"><path d="m19.876 10.77 2.831 2.83a1 1 0 0 1 0 1.415l-7.246 7.246a1 1 0 0 1-.572.284l-3.277.446a1 1 0 0 1-1.125-1.13l.461-3.277a1 1 0 0 1 .283-.567l7.23-7.246a1 1 0 0 1 1.415-.001Zm-7.421 2.08a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 7.5 20a1 1 0 0 1 .993.883L8.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378Zm6.715.042-6.29 6.3-.23 1.639 1.633-.222 6.302-6.302-1.415-1.415ZM9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Z"/></symbol><symbol id="icon-eds-i-user-linked-medium" viewBox="0 0 24 24"><path d="M15.65 6c.31 0 .706.066 1.122.274C17.522 6.65 18 7.366 18 8.35v12.3c0 .31-.066.706-.274 1.122-.375.75-1.092 1.228-2.076 1.228H3.35a2.52 2.52 0 0 1-1.122-.274C1.478 22.35 1 21.634 1 20.65V8.35c0-.31.066-.706.274-1.122C1.65 6.478 2.366 6 3.35 6h12.3Zm0 2-12.376.002c-.134.007-.17.04-.21.12A.672.672 0 0 0 3 8.35v12.3c0 .198.028.24.122.287.09.044.2.063.228.063h.887c.788-2.269 2.814-3.5 5.263-3.5 2.45 0 4.475 1.231 5.263 3.5h.887c.198 0 .24-.028.287-.122.044-.09.063-.2.063-.228V8.35c0-.198-.028-.24-.122-.287A.672.672 0 0 0 15.65 8ZM9.5 19.5c-1.36 0-2.447.51-3.06 1.5h6.12c-.613-.99-1.7-1.5-3.06-1.5ZM20.65 1A2.35 2.35 0 0 1 23 3.348V15.65A2.35 2.35 0 0 1 20.65 18H20a1 1 0 0 1 0-2h.65a.35.35 0 0 0 .35-.35V3.348A.35.35 0 0 0 20.65 3H8.35a.35.35 0 0 0-.35.348V4a1 1 0 1 1-2 0v-.652A2.35 2.35 0 0 1 8.35 1h12.3ZM9.5 10a3.5 3.5 0 1 1 0 7 3.5 3.5 0 0 1 0-7Zm0 2a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3Z"/></symbol><symbol id="icon-eds-i-user-multiple-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm6 0a5 5 0 0 1 0 10 1 1 0 0 1-.117-1.993L15 9a3 3 0 0 0 0-6 1 1 0 0 1 0-2ZM9 3a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm8.857 9.545a7.99 7.99 0 0 1 2.651 1.715A8.31 8.31 0 0 1 23 20.134V21a1 1 0 0 1-1 1h-3a1 1 0 0 1 0-2h1.995l-.005-.153a6.307 6.307 0 0 0-1.673-3.945l-.204-.209a5.99 5.99 0 0 0-1.988-1.287 1 1 0 1 1 .732-1.861Zm-3.349 1.715A8.31 8.31 0 0 1 17 20.134V21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.877c.044-4.343 3.387-7.908 7.638-8.115a7.908 7.908 0 0 1 5.87 2.252ZM9.016 14l-.285.006c-3.104.15-5.58 2.718-5.725 5.9L3.004 20h11.991l-.005-.153a6.307 6.307 0 0 0-1.673-3.945l-.204-.209A5.924 5.924 0 0 0 9.3 14.008L9.016 14Z"/></symbol><symbol id="icon-eds-i-user-notify-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm10 18v1a1 1 0 0 1-2 0v-1h-3a1 1 0 0 1 0-2v-2.818C14 13.885 15.777 12 18 12s4 1.885 4 4.182V19a1 1 0 0 1 0 2h-3Zm-6.545-8.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM18 14c-1.091 0-2 .964-2 2.182V19h4v-2.818c0-1.165-.832-2.098-1.859-2.177L18 14Z"/></symbol><symbol id="icon-eds-i-user-remove-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm3.455 9.85a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM22 17a1 1 0 0 1 0 2h-8a1 1 0 0 1 0-2h8Z"/></symbol><symbol id="icon-eds-i-user-single-medium" viewBox="0 0 24 24"><path d="M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm-.406 9.008a8.965 8.965 0 0 1 6.596 2.494A9.161 9.161 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007Zm.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.157 7.157 0 0 0-1.97-4.573l-.214-.213A6.967 6.967 0 0 0 11.984 14Z"/></symbol><symbol id="icon-eds-i-warning-circle-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm0 11.5a1.5 1.5 0 0 1 .144 2.993L12 17.5a1.5 1.5 0 0 1 0-3ZM12 6a1 1 0 0 1 1 1v5a1 1 0 0 1-2 0V7a1 1 0 0 1 1-1Z"/></symbol><symbol id="icon-eds-i-warning-filled-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 13.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 14.5ZM12 6a1 1 0 0 0-1 1v5a1 1 0 0 0 2 0V7a1 1 0 0 0-1-1Z"/></symbol><symbol id="icon-eds-i-work-medium" viewBox="0 0 24 24"><path d="M4 3.53808C4 3.24519 4.23975 3 4.5451 3H14.1778L19 7.80031V8C19 8.55228 19.4477 9 20 9C20.5523 9 21 8.55228 21 8V7.38477C21 7.11876 20.894 6.86372 20.7055 6.67605L15.2962 1.29129C15.1088 1.10473 14.8551 1 14.5907 1H4.5451C3.14377 1 2 2.13206 2 3.53808V20.5007C2 21.882 3.11988 23 4.5 23H8C8.55228 23 9 22.5523 9 22C9 21.4477 8.55228 21 8 21H4.5C4.22327 21 4 20.7762 4 20.5007V3.53808Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M19.8764 10.7698C19.6887 10.5822 19.4341 10.4768 19.1687 10.4769C18.9033 10.4771 18.6489 10.5827 18.4614 10.7706L11.2306 18.0167C11.0776 18.1701 10.9785 18.3691 10.9483 18.5836L10.4867 21.8605C10.443 22.1707 10.5472 22.4835 10.7682 22.7055C10.9892 22.9275 11.3015 23.0331 11.6118 22.9909L14.8888 22.5447C15.1054 22.5152 15.3064 22.4155 15.461 22.261L22.7071 15.0148C22.8947 14.8273 23 14.5729 23 14.3077C23 14.0425 22.8947 13.7881 22.7071 13.6006L19.8764 10.7698ZM12.8821 19.1931L19.17 12.8919L20.5858 14.3077L14.285 20.6085L12.6515 20.8309L12.8821 19.1931Z"/><path d="M11.0812 4.68628C11.5307 5.00729 11.6347 5.63184 11.3137 6.08125L8.81373 9.58125C8.64288 9.82045 8.37543 9.97236 8.08248 9.99661C7.78953 10.0209 7.50075 9.91498 7.29289 9.70712L5.79289 8.20712C5.40237 7.8166 5.40237 7.18343 5.79289 6.79291C6.18342 6.40239 6.81658 6.40239 7.20711 6.79291L7.8724 7.4582L9.68627 4.91878C10.0073 4.46937 10.6318 4.36527 11.0812 4.68628Z"/><path d="M11.3137 12.0813C11.6347 11.6318 11.5307 11.0073 11.0812 10.6863C10.6318 10.3653 10.0073 10.4694 9.68627 10.9188L7.8724 13.4582L7.20711 12.7929C6.81658 12.4024 6.18342 12.4024 5.79289 12.7929C5.40237 13.1834 5.40237 13.8166 5.79289 14.2071L7.29289 15.7071C7.50075 15.915 7.78953 16.0209 8.08248 15.9966C8.37543 15.9724 8.64288 15.8205 8.81373 15.5813L11.3137 12.0813Z"/></symbol><symbol id="icon-ai-stars"><path d="M22.294 13.39c.941.536.941 1.945 0 2.482l-3.613 2.061c-.228.13-.415.325-.54.563l-1.976 3.768a1.33 1.33 0 0 1-2.38 0l-1.977-3.768a1.4 1.4 0 0 0-.539-.563l-3.614-2.061c-.94-.537-.94-1.946 0-2.482l3.614-2.061c.228-.13.415-.325.54-.563l1.976-3.768a1.33 1.33 0 0 1 2.38 0l1.977 3.768c.124.238.311.433.539.563zM10.08 4.861c1.044.508 1.044 2.056 0 2.564l-1.543.751c-.29.14-.521.383-.656.684l-.72 1.61a1.334 1.334 0 0 1-2.459 0l-.72-1.61a1.4 1.4 0 0 0-.656-.684l-1.543-.751c-1.044-.508-1.044-2.056 0-2.564l1.543-.751c.29-.14.521-.383.656-.684l.72-1.61a1.334 1.334 0 0 1 2.459 0l.72 1.61c.135.301.367.543.656.684z"/></symbol><symbol id="icon-chevron-left-medium" viewBox="0 0 24 24"><path d="M15.7194 3.3054C15.3358 2.90809 14.7027 2.89699 14.3054 3.28061L6.54342 10.7757C6.19804 11.09 6 11.5335 6 12C6 12.4665 6.19804 12.91 6.5218 13.204L14.3054 20.7194C14.7027 21.103 15.3358 21.0919 15.7194 20.6946C16.103 20.2973 16.0919 19.6642 15.6946 19.2806L8.155 12L15.6946 4.71939C16.0614 4.36528 16.099 3.79863 15.8009 3.40105L15.7194 3.3054Z"/></symbol><symbol id="icon-chevron-right-medium" viewBox="0 0 24 24"><path d="M8.28061 3.3054C8.66423 2.90809 9.29729 2.89699 9.6946 3.28061L17.4566 10.7757C17.802 11.09 18 11.5335 18 12C18 12.4665 17.802 12.91 17.4782 13.204L9.6946 20.7194C9.29729 21.103 8.66423 21.0919 8.28061 20.6946C7.89699 20.2973 7.90809 19.6642 8.3054 19.2806L15.845 12L8.3054 4.71939C7.93865 4.36528 7.90098 3.79863 8.19908 3.40105L8.28061 3.3054Z"/></symbol><symbol id="icon-eds-alerts" viewBox="0 0 32 32"><path d="M28 12.667c.736 0 1.333.597 1.333 1.333v13.333A3.333 3.333 0 0 1 26 30.667H6a3.333 3.333 0 0 1-3.333-3.334V14a1.333 1.333 0 1 1 2.666 0v1.252L16 21.769l10.667-6.518V14c0-.736.597-1.333 1.333-1.333Zm-1.333 5.71-9.972 6.094c-.427.26-.963.26-1.39 0l-9.972-6.094v8.956c0 .368.299.667.667.667h20a.667.667 0 0 0 .667-.667v-8.956ZM19.333 12a1.333 1.333 0 1 1 0 2.667h-6.666a1.333 1.333 0 1 1 0-2.667h6.666Zm4-10.667a3.333 3.333 0 0 1 3.334 3.334v6.666a1.333 1.333 0 1 1-2.667 0V4.667A.667.667 0 0 0 23.333 4H8.667A.667.667 0 0 0 8 4.667v6.666a1.333 1.333 0 1 1-2.667 0V4.667a3.333 3.333 0 0 1 3.334-3.334h14.666Zm-4 5.334a1.333 1.333 0 0 1 0 2.666h-6.666a1.333 1.333 0 1 1 0-2.666h6.666Z"/></symbol><symbol id="icon-eds-arrow-up" viewBox="0 0 24 24"><path fill-rule="evenodd" d="m13.002 7.408 4.88 4.88a.99.99 0 0 0 1.32.08l.09-.08c.39-.39.39-1.03 0-1.42l-6.58-6.58a1.01 1.01 0 0 0-1.42 0l-6.58 6.58a1 1 0 0 0-.09 1.32l.08.1a1 1 0 0 0 1.42-.01l4.88-4.87v11.59a.99.99 0 0 0 .88.99l.12.01c.55 0 1-.45 1-1V7.408z" class="layer"/></symbol><symbol id="icon-eds-checklist" viewBox="0 0 32 32"><path d="M19.2 1.333a3.468 3.468 0 0 1 3.381 2.699L24.667 4C26.515 4 28 5.52 28 7.38v19.906c0 1.86-1.485 3.38-3.333 3.38H7.333c-1.848 0-3.333-1.52-3.333-3.38V7.38C4 5.52 5.485 4 7.333 4h2.093A3.468 3.468 0 0 1 12.8 1.333h6.4ZM9.426 6.667H7.333c-.36 0-.666.312-.666.713v19.906c0 .401.305.714.666.714h17.334c.36 0 .666-.313.666-.714V7.38c0-.4-.305-.713-.646-.714l-2.121.033A3.468 3.468 0 0 1 19.2 9.333h-6.4a3.468 3.468 0 0 1-3.374-2.666Zm12.715 5.606c.586.446.7 1.283.253 1.868l-7.111 9.334a1.333 1.333 0 0 1-1.792.306l-3.556-2.333a1.333 1.333 0 1 1 1.463-2.23l2.517 1.651 6.358-8.344a1.333 1.333 0 0 1 1.868-.252ZM19.2 4h-6.4a.8.8 0 0 0-.8.8v1.067a.8.8 0 0 0 .8.8h6.4a.8.8 0 0 0 .8-.8V4.8a.8.8 0 0 0-.8-.8Z"/></symbol><symbol id="icon-eds-citation" viewBox="0 0 36 36"><path d="M23.25 1.5a1.5 1.5 0 0 1 1.06.44l8.25 8.25a1.5 1.5 0 0 1 .44 1.06v19.5c0 2.105-1.645 3.75-3.75 3.75H18a1.5 1.5 0 0 1 0-3h11.25c.448 0 .75-.302.75-.75V11.873L22.628 4.5H8.31a.811.811 0 0 0-.8.68l-.011.13V16.5a1.5 1.5 0 0 1-3 0V5.31A3.81 3.81 0 0 1 8.31 1.5h14.94ZM8.223 20.358a.984.984 0 0 1-.192 1.378l-.048.034c-.54.36-.942.676-1.206.951-.59.614-.885 1.395-.885 2.343.115-.028.288-.042.518-.042.662 0 1.26.237 1.791.711.533.474.799 1.074.799 1.799 0 .753-.259 1.352-.777 1.799-.518.446-1.151.669-1.9.669-1.006 0-1.812-.293-2.417-.878C3.302 28.536 3 27.657 3 26.486c0-1.115.165-2.085.496-2.907.331-.823.734-1.513 1.209-2.071.475-.558.971-.997 1.49-1.318a6.01 6.01 0 0 1 .347-.2 1.321 1.321 0 0 1 1.681.368Zm7.5 0a.984.984 0 0 1-.192 1.378l-.048.034c-.54.36-.942.676-1.206.951-.59.614-.885 1.395-.885 2.343.115-.028.288-.042.518-.042.662 0 1.26.237 1.791.711.533.474.799 1.074.799 1.799 0 .753-.259 1.352-.777 1.799-.518.446-1.151.669-1.9.669-1.006 0-1.812-.293-2.417-.878-.604-.586-.906-1.465-.906-2.636 0-1.115.165-2.085.496-2.907.331-.823.734-1.513 1.209-2.071.475-.558.971-.997 1.49-1.318a6.01 6.01 0 0 1 .347-.2 1.321 1.321 0 0 1 1.681.368Z"/></symbol><symbol id="icon-eds-i-access-indicator" viewBox="0 0 16 16"><circle cx="4.5" cy="11.5" r="3.5" style="fill:currentColor"/><path fill-rule="evenodd" d="M4 3v3a1 1 0 0 1-2 0V2.923C2 1.875 2.84 1 3.909 1h5.909a1 1 0 0 1 .713.298l3.181 3.231a1 1 0 0 1 .288.702v7.846c0 .505-.197.993-.554 1.354a1.902 1.902 0 0 1-1.355.569H10a1 1 0 1 1 0-2h2V5.64L9.4 3H4Z" clip-rule="evenodd" style="fill:#222"/></symbol><symbol id="icon-eds-i-accessibility-medium" viewBox="0 0 24 24"><path d="M17 10.5C17.5523 10.5 18 10.9477 18 11.5C18 12.0523 17.5523 12.5 17 12.5H13V13C13 13.4952 13.2735 14.3106 13.7695 15.3027C14.1249 16.0135 14.551 16.7321 14.9483 17.3564L15.332 17.9453L15.3848 18.0332C15.6218 18.4812 15.4855 19.0448 15.0547 19.332C14.6238 19.6193 14.0508 19.5282 13.7285 19.1367L13.668 19.0547L13.2627 18.4326C12.8412 17.7703 12.3781 16.9924 11.9844 16.2061C11.4619 17.2978 10.8292 18.309 10.332 19.0547C10.0257 19.5142 9.40486 19.6384 8.94533 19.332C8.4858 19.0257 8.36163 18.4048 8.66798 17.9453C9.15666 17.2123 9.75032 16.2592 10.2197 15.2617C10.6385 14.372 10.9221 13.5218 10.9863 12.8008L11 12.5H7.00002C6.44774 12.5 6.00002 12.0523 6.00002 11.5C6.00002 10.9477 6.44774 10.5 7.00002 10.5H17Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M12 4C12.7957 4 13.5585 4.3163 14.1211 4.87891C14.6837 5.44152 15 6.20435 15 7C15 7.79565 14.6837 8.55848 14.1211 9.12109C13.5585 9.6837 12.7957 10 12 10C11.2044 10 10.4415 9.6837 9.87892 9.12109C9.31631 8.55848 9.00002 7.79565 9.00002 7C9.00002 6.20435 9.31631 5.44152 9.87892 4.87891C10.4415 4.3163 11.2044 4 12 4ZM12 6C11.7348 6 11.4805 6.10543 11.293 6.29297C11.1054 6.4805 11 6.73478 11 7C11 7.26522 11.1054 7.5195 11.293 7.70703C11.4805 7.89457 11.7348 8 12 8C12.2652 8 12.5195 7.89457 12.707 7.70703C12.8946 7.5195 13 7.26522 13 7C13 6.73478 12.8946 6.48051 12.707 6.29297C12.5195 6.10543 12.2652 6 12 6Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M12 1C18.0751 1 23 5.92487 23 12C23 18.0751 18.0751 23 12 23C5.92488 23 1.00002 18.0751 1.00002 12C1.00002 5.92487 5.92488 1 12 1ZM12 3C7.02945 3 3.00002 7.02944 3.00002 12C3.00002 16.9706 7.02945 21 12 21C16.9706 21 21 16.9706 21 12C21 7.02944 16.9706 3 12 3Z"/></symbol><symbol id="icon-eds-i-book-research-medium"><path fill-rule="evenodd" d="M9.99.952c.922 0 1.822.273 2.589.784l2.42 1.614 2.421-1.614a4.667 4.667 0 0 1 2.283-.774l.306-.01h6.324a3.333 3.333 0 0 1 3.333 3.334v8.666a1.333 1.333 0 0 1-2.666 0V4.286a.667.667 0 0 0-.667-.667h-6.324a2 2 0 0 0-1.11.336l-2.566 1.71v5.954a1.333 1.333 0 1 1-2.667 0V5.666L11.1 3.955a2 2 0 0 0-1.11-.336H3.666A.667.667 0 0 0 3 4.286v17.333c0 .368.298.667.666.667h10a1.333 1.333 0 1 1 0 2.666h-10A3.333 3.333 0 0 1 .333 21.62V4.286A3.333 3.333 0 0 1 3.666.952H9.99Zm12.343 13.334a6 6 0 0 1 5.08 9.193l1.863 1.864a1.333 1.333 0 1 1-1.886 1.886l-1.864-1.863a6 6 0 1 1-3.193-11.08Zm-3.333 6a3.333 3.333 0 1 1 6.666 0 3.333 3.333 0 0 1-6.666 0Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-circle-bluesky" viewBox="0 0 25 24"><path d="M12.5 0c6.627 0 12 5.373 12 12s-5.373 12-12 12-12-5.373-12-12 5.373-12 12-12m7 7.44c0-2.158-1.877-1.48-3.035-.604-1.605 1.214-3.331 3.676-3.965 4.997-.634-1.321-2.36-3.783-3.965-4.997C7.377 5.96 5.5 5.282 5.5 7.439c0 .432.245 3.62.389 4.137.5 1.8 2.32 2.258 3.94 1.98-2.831.486-3.551 2.095-1.996 3.703 2.954 3.054 4.246-.766 4.577-1.745.061-.181.09-.265.09-.19 0-.075.029.009.09.19.33.98 1.623 4.8 4.577 1.745 1.555-1.608.835-3.217-1.996-3.702 1.62.277 3.44-.182 3.94-1.98.144-.518.389-3.706.389-4.138"/></symbol><symbol id="icon-eds-i-circle-facebook" viewBox="0 0 25 24"><path d="M12.5 0C5.872 0 .5 5.372.5 12s5.372 12 12 12 12-5.372 12-12-5.372-12-12-12m3.356 7.417h-1.62s-.858-.023-.93 1v1.836h2.186l-.017 2.454h-2.168l-.003 6.409-2.571-.004v-6.405H8.612V10.23h2.12V8.012s.096-2.574 2.598-2.884h2.526z"/></symbol><symbol id="icon-eds-i-circle-instagram" viewBox="0 0 25 24"><g clip-path="url(#a)"><path d="M12.5 0C5.872 0 .5 5.372.5 12s5.372 12 12 12 12-5.372 12-12-5.372-12-12-12m6.687 15.313a3.377 3.377 0 0 1-3.374 3.373H9.188a3.377 3.377 0 0 1-3.373-3.373V8.687a3.377 3.377 0 0 1 3.373-3.374h6.627a3.377 3.377 0 0 1 3.373 3.374zm-3.374-8.544H9.188A1.92 1.92 0 0 0 7.27 8.687v6.627a1.92 1.92 0 0 0 1.918 1.918h6.627a1.92 1.92 0 0 0 1.918-1.918V8.686a1.92 1.92 0 0 0-1.918-1.918M12.5 15.444A3.45 3.45 0 0 1 9.056 12c0-1.9 1.545-3.444 3.444-3.444S15.944 10.1 15.944 12 14.4 15.444 12.5 15.444m3.444-6.073a.816.816 0 1 1 .002-1.632.816.816 0 0 1-.002 1.632m-3.444.64c-1.096 0-1.99.893-1.99 1.989s.894 1.99 1.99 1.99A1.99 1.99 0 0 0 14.489 12a1.99 1.99 0 0 0-1.989-1.989"/></g></symbol><symbol id="icon-eds-i-circle-linkedin" viewBox="0 0 25 24"><path d="M12.5 0C5.872 0 .5 5.373.5 12s5.372 12 12 12 12-5.373 12-12-5.373-12-12-12m-2.288 16.662h-2.24V9.765h2.24zM9.056 8.844a1.253 1.253 0 1 1 0-2.507 1.253 1.253 0 0 1 0 2.507m9.03 7.81h-2.119v-3.998s-.034-1.321-1.22-1.17c0 0-1.016-.05-1.135 1.237v3.914h-2.151v-6.86h2.032v.914s.576-1.135 2.067-1.101c0 0 2.405-.254 2.525 2.49z"/></symbol><symbol id="icon-eds-i-circle-x" viewBox="0 0 25 24"><path d="m12.59 11.233-2.397-3.427H8.915l2.97 4.247.373.534 2.542 3.636h1.278l-3.115-4.455zM12.5 0C5.872 0 .5 5.372.5 12s5.372 12 12 12 12-5.372 12-12-5.372-12-12-12m1.908 16.82-2.572-3.743-3.221 3.744h-.832l3.683-4.281-3.683-5.36h2.809l2.435 3.544 3.05-3.545h.833l-3.513 4.083 3.82 5.56z"/></symbol><symbol id="icon-eds-i-copy-link" viewBox="0 0 24 24"><path fill-rule="evenodd" clip-rule="evenodd" d="M19.4594 8.57015C19.0689 8.17963 19.0689 7.54646 19.4594 7.15594L20.2927 6.32261C20.2927 6.32261 20.2927 6.32261 20.2927 6.32261C21.0528 5.56252 21.0528 4.33019 20.2928 3.57014C19.5327 2.81007 18.3004 2.81007 17.5404 3.57014L16.7071 4.40347C16.3165 4.794 15.6834 4.794 15.2928 4.40348C14.9023 4.01296 14.9023 3.3798 15.2928 2.98927L16.1262 2.15594C17.6673 0.614803 20.1659 0.614803 21.707 2.15593C23.2481 3.69705 23.248 6.19569 21.707 7.7368L20.8737 8.57014C20.4831 8.96067 19.85 8.96067 19.4594 8.57015Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M18.0944 5.90592C18.4849 6.29643 18.4849 6.9296 18.0944 7.32013L16.4278 8.9868C16.0373 9.37733 15.4041 9.37734 15.0136 8.98682C14.6231 8.59631 14.6231 7.96314 15.0136 7.57261L16.6802 5.90594C17.0707 5.51541 17.7039 5.5154 18.0944 5.90592Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M13.5113 6.32243C13.9018 6.71295 13.9018 7.34611 13.5113 7.73664L12.678 8.56997C12.678 8.56997 12.678 8.56997 12.678 8.56997C11.9179 9.33006 11.9179 10.5624 12.6779 11.3224C13.438 12.0825 14.6703 12.0825 15.4303 11.3224L16.2636 10.4891C16.6542 10.0986 17.2873 10.0986 17.6779 10.4891C18.0684 10.8796 18.0684 11.5128 17.6779 11.9033L16.8445 12.7366C15.3034 14.2778 12.8048 14.2778 11.2637 12.7366C9.72262 11.1955 9.72266 8.69689 11.2637 7.15578L12.097 6.32244C12.4876 5.93191 13.1207 5.93191 13.5113 6.32243Z"/><path d="M8 20V22H19.4619C20.136 22 20.7822 21.7311 21.2582 21.2529C21.7333 20.7757 22 20.1289 22 19.4549V15C22 14.4477 21.5523 14 21 14C20.4477 14 20 14.4477 20 15V19.4549C20 19.6004 19.9426 19.7397 19.8408 19.842C19.7399 19.9433 19.6037 20 19.4619 20H8Z"/><path d="M4 13H2V19.4619C2 20.136 2.26889 20.7822 2.74705 21.2582C3.22434 21.7333 3.87105 22 4.5451 22H9C9.55228 22 10 21.5523 10 21C10 20.4477 9.55228 20 9 20H4.5451C4.39957 20 4.26028 19.9426 4.15804 19.8408C4.05668 19.7399 4 19.6037 4 19.4619V13Z"/><path d="M4 13H2V4.53808C2 3.86398 2.26889 3.21777 2.74705 2.74178C3.22434 2.26666 3.87105 2 4.5451 2H9C9.55228 2 10 2.44772 10 3C10 3.55228 9.55228 4 9 4H4.5451C4.39957 4 4.26028 4.05743 4.15804 4.15921C4.05668 4.26011 4 4.39633 4 4.53808V13Z"/></symbol><symbol id="icon-eds-i-funding-dollar" viewBox="0 0 32 32"><path d="M17.333 7.79549V9.21808C18.3681 9.32469 19.2889 9.82002 19.9444 10.5523C20.2938 10.9427 20.5697 11.4022 20.7488 11.9089C20.9942 12.6031 20.6303 13.3649 19.936 13.6103C19.2418 13.8558 18.48 13.4919 18.2346 12.7976C18.1735 12.6249 18.0788 12.4665 17.9574 12.3308C17.6988 12.0419 17.3272 11.8632 16.9122 11.8632H16.042C16.028 11.8636 16.0139 11.8639 15.9997 11.8639C15.9907 11.8639 15.9817 11.8638 15.9727 11.8636C15.9676 11.8635 15.9624 11.8634 15.9573 11.8632H14.7952C14.1833 11.8632 13.6872 12.3593 13.6872 12.9713C13.6872 13.492 14.0498 13.9424 14.5584 14.0537L17.7816 14.7588C19.6498 15.1675 20.9806 16.8226 20.9806 18.734C20.9806 20.8383 19.3827 22.5712 17.333 22.7819V24.2051C17.333 24.9415 16.7361 25.5384 15.9997 25.5384C15.2633 25.5384 14.6663 24.9415 14.6663 24.2051V22.7817C13.0793 22.618 11.7653 21.5424 11.2524 20.091C11.007 19.3967 11.3709 18.635 12.0651 18.3896C12.7594 18.1442 13.5212 18.5081 13.7666 19.2024C13.9597 19.7486 14.4807 20.1367 15.0889 20.1367H15.9849C15.9898 20.1367 15.9947 20.1366 15.9997 20.1366C16.0046 20.1366 16.0095 20.1367 16.0144 20.1367H16.9122C17.6857 20.1367 18.3139 19.5088 18.3139 18.734C18.3139 18.0748 17.8548 17.5045 17.2118 17.3639L13.9886 16.6588C12.2557 16.2797 11.0205 14.7451 11.0205 12.9713C11.0205 10.9297 12.6413 9.26664 14.6663 9.19869V7.79549C14.6663 7.05911 15.2633 6.46216 15.9997 6.46216C16.7361 6.46216 17.333 7.05911 17.333 7.79549Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M15.9997 1.33325C7.8995 1.33325 1.33301 7.89974 1.33301 15.9999C1.33301 24.1002 7.89951 30.6666 15.9997 30.6666C24.1 30.6666 30.6663 24.1002 30.6663 15.9999C30.6663 7.89975 24.1 1.33325 15.9997 1.33325ZM3.99967 15.9999C3.99967 9.3725 9.37226 3.99992 15.9997 3.99992C22.6272 3.99992 27.9997 9.3725 27.9997 15.9999C27.9997 22.6274 22.6272 27.9999 15.9997 27.9999C9.37225 27.9999 3.99967 22.6274 3.99967 15.9999Z"/></symbol><symbol id="icon-eds-i-github-medium" viewBox="0 0 24 24"><path d="M 11.964844 0 C 5.347656 0 0 5.269531 0 11.792969 C 0 17.003906 3.425781 21.417969 8.179688 22.976562 C 8.773438 23.09375 8.992188 22.722656 8.992188 22.410156 C 8.992188 22.136719 8.972656 21.203125 8.972656 20.226562 C 5.644531 20.929688 4.953125 18.820312 4.953125 18.820312 C 4.417969 17.453125 3.625 17.101562 3.625 17.101562 C 2.535156 16.378906 3.703125 16.378906 3.703125 16.378906 C 4.914062 16.457031 5.546875 17.589844 5.546875 17.589844 C 6.617188 19.386719 8.339844 18.878906 9.03125 18.566406 C 9.132812 17.804688 9.449219 17.277344 9.785156 16.984375 C 7.132812 16.710938 4.339844 15.695312 4.339844 11.167969 C 4.339844 9.878906 4.8125 8.824219 5.566406 8.003906 C 5.445312 7.710938 5.03125 6.5 5.683594 4.878906 C 5.683594 4.878906 6.695312 4.566406 8.972656 6.089844 C 9.949219 5.832031 10.953125 5.703125 11.964844 5.699219 C 12.972656 5.699219 14.003906 5.835938 14.957031 6.089844 C 17.234375 4.566406 18.242188 4.878906 18.242188 4.878906 C 18.898438 6.5 18.480469 7.710938 18.363281 8.003906 C 19.136719 8.824219 19.589844 9.878906 19.589844 11.167969 C 19.589844 15.695312 16.796875 16.691406 14.125 16.984375 C 14.558594 17.355469 14.933594 18.058594 14.933594 19.171875 C 14.933594 20.753906 14.914062 22.019531 14.914062 22.410156 C 14.914062 22.722656 15.132812 23.09375 15.726562 22.976562 C 20.480469 21.414062 23.910156 17.003906 23.910156 11.792969 C 23.929688 5.269531 18.558594 0 11.964844 0 Z M 11.964844 0 "/></symbol><symbol id="icon-eds-i-institution-medium" viewBox="0 0 24 24"><g><path fill-rule="evenodd" clip-rule="evenodd" d="M11.9967 1C11.6364 1 11.279 1.0898 10.961 1.2646C10.9318 1.28061 10.9035 1.29806 10.8761 1.31689L2.79765 6.87C2.46776 7.08001 2.20618 7.38466 2.07836 7.76668C1.94823 8.15561 1.98027 8.55648 2.12665 8.90067C2.42086 9.59246 3.12798 10 3.90107 10H4.99994V16H4.49994C3.11923 16 1.99994 17.1193 1.99994 18.5V19.5C1.99994 20.8807 3.11923 22 4.49994 22H19.4999C20.8807 22 21.9999 20.8807 21.9999 19.5V18.5C21.9999 17.1193 20.8807 16 19.4999 16H18.9999V10H20.0922C20.8653 10 21.5725 9.59252 21.8667 8.90065C22.0131 8.55642 22.0451 8.15553 21.9149 7.7666C21.7871 7.38459 21.5255 7.07997 21.1956 6.86998L13.1172 1.31689C13.0898 1.29806 13.0615 1.28061 13.0324 1.2646C12.7143 1.0898 12.357 1 11.9967 1ZM4.6844 8L11.9472 3.00755C11.9616 3.00295 11.9783 3 11.9967 3C12.015 3 12.0318 3.00295 12.0461 3.00755L19.3089 8H4.6844ZM16.9999 16V10H14.9999V16H16.9999ZM12.9999 16V10H10.9999V16H12.9999ZM8.99994 16V10H6.99994V16H8.99994ZM3.99994 18.5C3.99994 18.2239 4.2238 18 4.49994 18H19.4999C19.7761 18 19.9999 18.2239 19.9999 18.5V19.5C19.9999 19.7761 19.7761 20 19.4999 20H4.49994C4.2238 20 3.99994 19.7761 3.99994 19.5V18.5Z"/></g></symbol><symbol id="icon-eds-i-limited-access" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 3v3a1 1 0 0 1-2 0V2.923C2 1.875 2.84 1 3.909 1h5.909a1 1 0 0 1 .713.298l3.181 3.231a1 1 0 0 1 .288.702V6a1 1 0 1 1-2 0v-.36L9.4 3H4ZM3 8a1 1 0 0 1 1 1v1a1 1 0 1 1-2 0V9a1 1 0 0 1 1-1Zm10 0a1 1 0 0 1 1 1v1a1 1 0 1 1-2 0V9a1 1 0 0 1 1-1Zm-3.5 6a1 1 0 0 1-1 1h-1a1 1 0 1 1 0-2h1a1 1 0 0 1 1 1Zm2.441-1a1 1 0 0 1 2 0c0 .73-.246 1.306-.706 1.664a1.61 1.61 0 0 1-.876.334l-.032.002H11.5a1 1 0 1 1 0-2h.441ZM4 13a1 1 0 0 0-2 0c0 .73.247 1.306.706 1.664a1.609 1.609 0 0 0 .876.334l.032.002H4.5a1 1 0 1 0 0-2H4Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-rss" viewBox="0 0 22 22"><path d="M1.96094 1C1.96094 0.447715 2.40865 0 2.96094 0C5.46109 0 7.93678 0.492038 10.2467 1.44806C12.5565 2.40407 14.6554 3.80534 16.4234 5.57189C18.1913 7.33843 19.5939 9.4357 20.5508 11.744C21.5077 14.0522 22.0001 16.5263 22.0001 19.0247C22.0001 19.577 21.5524 20.0247 21.0001 20.0247C20.4478 20.0247 20.0001 19.577 20.0001 19.0247C20.0001 16.7891 19.5595 14.5753 18.7033 12.5098C17.8471 10.4444 16.5919 8.56762 15.0097 6.98666C13.4275 5.40575 11.5492 4.15167 9.48182 3.29604C7.41447 2.4404 5.19868 2 2.96094 2C2.40865 2 1.96094 1.55228 1.96094 1Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M0 18.649C0 16.7974 1.50196 15.298 3.35294 15.298C5.20392 15.298 6.70588 16.7974 6.70588 18.649C6.70588 20.5003 5.20397 22 3.35294 22C1.50191 22 0 20.5003 0 18.649ZM3.35294 17.298C2.60493 17.298 2 17.9036 2 18.649C2 19.3943 2.60498 20 3.35294 20C4.1009 20 4.70588 19.3943 4.70588 18.649C4.70588 17.9036 4.10095 17.298 3.35294 17.298Z"/><path d="M3.3374 7.46115C2.78512 7.46115 2.3374 7.90887 2.3374 8.46115C2.3374 9.01344 2.78512 9.46115 3.3374 9.46115C4.54515 9.46115 5.74107 9.69885 6.85684 10.1606C7.97262 10.6224 8.98639 11.2993 9.84028 12.1525C10.6942 13.0057 11.3715 14.0185 11.8336 15.1332C12.2956 16.2478 12.5335 17.4424 12.5335 18.649C12.5335 19.2013 12.9812 19.649 13.5335 19.649C14.0858 19.649 14.5335 19.2013 14.5335 18.649C14.5335 17.1796 14.2438 15.7247 13.6811 14.3673C13.1184 13.0099 12.2936 11.7765 11.2539 10.7377C10.2142 9.69885 8.97999 8.87484 7.62168 8.31266C6.26337 7.75049 4.80757 7.46115 3.3374 7.46115Z"/></symbol><symbol id="icon-eds-i-search-category-medium" viewBox="0 0 32 32"><path fill-rule="evenodd" d="M2 5.306A3.306 3.306 0 0 1 5.306 2h5.833a3.306 3.306 0 0 1 3.306 3.306v5.833a3.306 3.306 0 0 1-3.306 3.305H5.306A3.306 3.306 0 0 1 2 11.14V5.306Zm3.306-.584a.583.583 0 0 0-.584.584v5.833c0 .322.261.583.584.583h5.833a.583.583 0 0 0 .583-.583V5.306a.583.583 0 0 0-.583-.584H5.306Zm15.555 8.945a7.194 7.194 0 1 0 4.034 13.153l2.781 2.781a1.361 1.361 0 1 0 1.925-1.925l-2.781-2.781a7.194 7.194 0 0 0-5.958-11.228Zm3.173 10.346a4.472 4.472 0 1 0-.021.021l.01-.01.011-.011Zm-5.117-19.29a.583.583 0 0 0-.584.583v5.833a1.361 1.361 0 0 1-2.722 0V5.306A3.306 3.306 0 0 1 18.917 2h5.833a3.306 3.306 0 0 1 3.306 3.306v5.833c0 .6-.161 1.166-.443 1.654a1.361 1.361 0 1 1-2.357-1.363.575.575 0 0 0 .078-.291V5.306a.583.583 0 0 0-.584-.584h-5.833ZM2 18.916a3.306 3.306 0 0 1 3.306-3.306h5.833a1.361 1.361 0 1 1 0 2.722H5.306a.583.583 0 0 0-.584.584v5.833c0 .322.261.583.584.583h5.833a.574.574 0 0 0 .29-.077 1.361 1.361 0 1 1 1.364 2.356 3.296 3.296 0 0 1-1.654.444H5.306A3.306 3.306 0 0 1 2 24.75v-5.833Z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-search-magic" viewBox="0 0 20 20"><path d="M8.695 1.667a9.1 9.1 0 0 1 1.756.17A3.098 3.098 0 0 0 9.436 3.37a7.333 7.333 0 0 0-.738-.038c-3.841 0-6.956 2.986-6.956 6.668 0 3.681 3.115 6.665 6.956 6.665 3.642 0 6.627-2.681 6.928-6.096a3.19 3.19 0 0 0 1.763-.548 8.091 8.091 0 0 1-1.961 5.25l3.446 3.306a.81.81 0 0 1 0 1.178.897.897 0 0 1-1.23 0l-3.447-3.303a8.892 8.892 0 0 1-5.502 1.88C3.893 18.334 0 14.603 0 10c0-4.603 3.893-8.333 8.695-8.334Z"/><path d="M20 4.166a.663.663 0 0 1-.128.395.709.709 0 0 1-.342.251l-2.341.827-.863 2.244a.693.693 0 0 1-.263.326.74.74 0 0 1-.821 0 .693.693 0 0 1-.264-.326l-.862-2.244-2.341-.827a.715.715 0 0 1-.341-.252.669.669 0 0 1 0-.787.715.715 0 0 1 .34-.252l2.342-.827.862-2.244a.693.693 0 0 1 .264-.327.74.74 0 0 1 .821 0 .7.7 0 0 1 .263.327l.863 2.244 2.34.827a.709.709 0 0 1 .343.251.663.663 0 0 1 .128.394Z"/></symbol><symbol id="icon-eds-i-subjects-medium" viewBox="0 0 24 24"><g id="icon-subjects-copy" stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.3846154,2 C14.7015971,2 15.7692308,3.06762994 15.7692308,4.38461538 L15.7692308,7.15384615 C15.7692308,8.47082629 14.7015955,9.53846154 13.3846154,9.53846154 L13.1038388,9.53925278 C13.2061091,9.85347965 13.3815528,10.1423885 13.6195822,10.3804178 C13.9722182,10.7330539 14.436524,10.9483278 14.9293854,10.9918129 L15.1153846,11 C16.2068332,11 17.2535347,11.433562 18.0254647,12.2054189 C18.6411944,12.8212361 19.0416785,13.6120766 19.1784166,14.4609738 L19.6153846,14.4615385 C20.932386,14.4615385 22,15.5291672 22,16.8461538 L22,19.6153846 C22,20.9323924 20.9323924,22 19.6153846,22 L16.8461538,22 C15.5291672,22 14.4615385,20.932386 14.4615385,19.6153846 L14.4615385,16.8461538 C14.4615385,15.5291737 15.5291737,14.4615385 16.8461538,14.4615385 L17.126925,14.460779 C17.0246537,14.1465537 16.8492179,13.857633 16.6112344,13.6196157 C16.2144418,13.2228606 15.6764136,13 15.1153846,13 C14.0239122,13 12.9771569,12.5664197 12.2053686,11.7946314 C12.1335167,11.7227795 12.0645962,11.6485444 11.9986839,11.5721119 C11.9354038,11.6485444 11.8664833,11.7227795 11.7946314,11.7946314 C11.0228431,12.5664197 9.97608778,13 8.88461538,13 C8.323576,13 7.78552852,13.2228666 7.38881294,13.6195822 C7.15078359,13.8576115 6.97533988,14.1465203 6.8730696,14.4607472 L7.15384615,14.4615385 C8.47082629,14.4615385 9.53846154,15.5291737 9.53846154,16.8461538 L9.53846154,19.6153846 C9.53846154,20.932386 8.47083276,22 7.15384615,22 L4.38461538,22 C3.06762347,22 2,20.9323876 2,19.6153846 L2,16.8461538 C2,15.5291721 3.06762994,14.4615385 4.38461538,14.4615385 L4.8215823,14.4609378 C4.95831893,13.6120029 5.3588057,12.8211623 5.97459937,12.2053686 C6.69125996,11.488708 7.64500941,11.0636656 8.6514968,11.0066017 L8.88461538,11 C9.44565477,11 9.98370225,10.7771334 10.3804178,10.3804178 C10.6184472,10.1423885 10.7938909,9.85347965 10.8961612,9.53925278 L10.6153846,9.53846154 C9.29840448,9.53846154 8.23076923,8.47082629 8.23076923,7.15384615 L8.23076923,4.38461538 C8.23076923,3.06762994 9.29840286,2 10.6153846,2 L13.3846154,2 Z M7.15384615,16.4615385 L4.38461538,16.4615385 C4.17220099,16.4615385 4,16.63374 4,16.8461538 L4,19.6153846 C4,19.8278134 4.17218833,20 4.38461538,20 L7.15384615,20 C7.36626945,20 7.53846154,19.8278103 7.53846154,19.6153846 L7.53846154,16.8461538 C7.53846154,16.6337432 7.36625679,16.4615385 7.15384615,16.4615385 Z M19.6153846,16.4615385 L16.8461538,16.4615385 C16.6337432,16.4615385 16.4615385,16.6337432 16.4615385,16.8461538 L16.4615385,19.6153846 C16.4615385,19.8278103 16.6337306,20 16.8461538,20 L19.6153846,20 C19.8278229,20 20,19.8278229 20,19.6153846 L20,16.8461538 C20,16.6337306 19.8278103,16.4615385 19.6153846,16.4615385 Z M13.3846154,4 L10.6153846,4 C10.4029708,4 10.2307692,4.17220099 10.2307692,4.38461538 L10.2307692,7.15384615 C10.2307692,7.36625679 10.402974,7.53846154 10.6153846,7.53846154 L13.3846154,7.53846154 C13.597026,7.53846154 13.7692308,7.36625679 13.7692308,7.15384615 L13.7692308,4.38461538 C13.7692308,4.17220099 13.5970292,4 13.3846154,4 Z" id="Shape" fill-rule="nonzero"/></g></symbol><symbol id="icon-eds-small-arrow-left" viewBox="0 0 16 17"><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M14 8.092H2m0 0L8 2M2 8.092l6 6.035"/></symbol><symbol id="icon-eds-small-arrow-right" viewBox="0 0 16 16"><g fill-rule="evenodd" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M2 8.092h12M8 2l6 6.092M8 14.127l6-6.035"/></g></symbol><symbol id="icon-globe-with-star" viewBox="0 0 32 32"><path style="fill:none;stroke:#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M27.282 17.026c0 6.797-5.51 12.307-12.307 12.307-6.798 0-12.308-5.51-12.308-12.307 0-6.798 5.51-12.308 12.308-12.308M2.667 17.026h14.201"/><path style="fill:none;stroke:#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M14.975 4.718a21.244 21.244 0 0 0-4.734 12.308c.233 4.5 1.89 8.81 4.734 12.307a21.245 21.245 0 0 0 4.394-9.467M17.045 9.72c-.709-.126-.709-1.16 0-1.286 2.568-.454 4.61-2.443 5.168-5.032l.043-.199c.153-.712 1.15-.716 1.31-.006l.052.232c.578 2.577 2.621 4.549 5.182 5.002.712.126.712 1.166 0 1.292-2.561.453-4.604 2.425-5.182 5.002l-.052.231c-.16.711-1.157.707-1.31-.005l-.043-.199c-.557-2.59-2.6-4.578-5.168-5.032Z"/></symbol><symbol id="icon-orcid-logo" viewBox="0 0 40 40"><path fill-rule="evenodd" d="M12.281 10.453c.875 0 1.578-.719 1.578-1.578 0-.86-.703-1.578-1.578-1.578-.875 0-1.578.703-1.578 1.578 0 .86.703 1.578 1.578 1.578Zm-1.203 18.641h2.406V12.359h-2.406v16.735Z"/><path fill-rule="evenodd" d="M17.016 12.36h6.5c6.187 0 8.906 4.421 8.906 8.374 0 4.297-3.36 8.375-8.875 8.375h-6.531V12.36Zm6.234 14.578h-3.828V14.53h3.703c4.688 0 6.828 2.844 6.828 6.203 0 2.063-1.25 6.203-6.703 6.203Z" clip-rule="evenodd"/></symbol><symbol id="icon-thumbs-down" viewBox="41 0 33 33"><path stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M61 17.611h-1l-2.085 4.436a1 1 0 0 1-.366.332 1.04 1.04 0 0 1-1.197-.159.95.95 0 0 1-.298-.678v-3.32h-3.93c-.61 0-.674-1.066-.599-1.55l.726-4.301a.98.98 0 0 1 .341-.622 1.06 1.06 0 0 1 .685-.249H61M63.5 11.5v6"/></symbol><symbol id="icon-thumbs-up-medium" viewBox="0 0 24 24"><path d="M6.75 9.33333H8.25L11.3778 2.67913C11.5138 2.47128 11.7025 2.29994 11.9263 2.18116C12.1502 2.06239 12.4018 2.00005 12.6576 2C13.056 1.99999 13.4384 2.15092 13.7214 2.41998C14.0044 2.68903 14.1652 3.05442 14.1688 3.43663V8.41667C16.1342 8.41667 18.1116 8.41667 20.0648 8.41667C20.9795 8.41667 21.0744 10.0164 20.9625 10.7422L19.8733 17.194C19.8269 17.5542 19.6449 17.8856 19.3616 18.1261C19.0783 18.3667 18.7132 18.4996 18.3349 18.5H6.75" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M3 18.5V9.5" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></symbol><symbol id="icon-thumbs-up"><path stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 15.389h1l2.085-4.436a1 1 0 0 1 .366-.332 1.04 1.04 0 0 1 1.197.159.95.95 0 0 1 .298.678v3.32h3.93c.61 0 .674 1.066.599 1.55l-.726 4.301a.98.98 0 0 1-.341.622 1.06 1.06 0 0 1-.685.249H13m-2.5 0v-6"/></symbol><symbol id="icon-tick-with-curly-circle" viewBox="0 0 40 40"><path d="M17.923.757a3.227 3.227 0 0 1 4.401.231l2.168 2.25 3.001-.866.33-.076a3.23 3.23 0 0 1 3.598 2.076l.099.325.75 3.03 3.033.753a3.229 3.229 0 0 1 2.4 3.697l-.075.33-.865 3.001 2.249 2.168.231.247a3.227 3.227 0 0 1-.231 4.401l-2.25 2.166.866 3.003.076.33a3.228 3.228 0 0 1-2.401 3.697l-3.033.75-.75 3.033a3.229 3.229 0 0 1-4.027 2.325l-3.001-.867-2.168 2.25a3.227 3.227 0 0 1-4.648 0l-2.17-2.25-2.999.867a3.229 3.229 0 0 1-4.027-2.325l-.752-3.033-3.03-.75a3.229 3.229 0 0 1-2.326-4.027l.865-3.001-2.249-2.168a3.227 3.227 0 0 1 0-4.648l2.25-2.17-.866-2.999A3.229 3.229 0 0 1 4.697 8.48l3.031-.752.752-3.03a3.229 3.229 0 0 1 4.027-2.326l3 .865L17.675.988l.247-.231Zm.027 5.156a3.23 3.23 0 0 1-3.219.864l-2.838-.82-.712 2.87a3.227 3.227 0 0 1-2.355 2.354l-2.868.712.819 2.838a3.23 3.23 0 0 1-.864 3.219L3.786 20l2.127 2.05a3.23 3.23 0 0 1 .864 3.219l-.82 2.837 2.87.713.215.062a3.228 3.228 0 0 1 2.14 2.293l.71 2.867 2.84-.818a3.229 3.229 0 0 1 3.218.864L20 36.214l2.05-2.127.16-.156a3.228 3.228 0 0 1 2.841-.76l.218.052 2.837.818.713-2.867.062-.214a3.227 3.227 0 0 1 2.293-2.141l2.867-.713-.816-2.837c-.331-1.15 0-2.389.861-3.219L36.212 20l-2.126-2.05a3.229 3.229 0 0 1-.861-3.219l.816-2.838-2.867-.712a3.228 3.228 0 0 1-2.355-2.355l-.713-2.868-2.837.819a3.23 3.23 0 0 1-3.219-.864L20 3.786l-2.05 2.127Z" stroke="transparent" stroke-width=".9"/><path d="M25.723 13.406a1.807 1.807 0 0 1 2.699 2.397l-9.658 12.074a1.808 1.808 0 0 1-2.497.316L11.44 24.57a1.806 1.806 0 1 1 2.168-2.892l3.427 2.57 8.565-10.704.124-.138Z" stroke="transparent" stroke-width=".9"/></symbol></svg>
</div>


        

        
        
    <a class="c-skip-link" href="#main">Skip to main content</a>

    
        
    

    <header class="eds-c-header" data-eds-c-header>
    <div class="eds-c-header__container" data-eds-c-header-expander-anchor>
        <div class="eds-c-header__brand">
            
                
                    <a href="https://link.springer.com"
                    	 data-test=springerlink-logo
                        
                            data-track="click_imprint_logo"
                        
                            data-track-context="unified header"
                        
                            data-track-action="click logo link"
                        
                            data-track-category="unified header"
                        
                            data-track-label="link"
                        
					>
                        <img src="/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg" alt="Springer Nature Link">
                    </a>
                
            
        </div>

        
            
                
    
        <a class="c-header__link eds-c-header__link" id="identity-account-widget" data-track="click_login" data-track-context="header" href='https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1007/s12559-024-10325-w'><span class="eds-c-header__widget-fragment-title">Log in</span></a>
    


            
        
    </div>

    
        <nav class="eds-c-header__nav" aria-label="header navigation">
            <div class="eds-c-header__nav-container">
                <div class="eds-c-header__item eds-c-header__item--menu">
                   <a href="#eds-c-header-nav" class="eds-c-header__link" data-eds-c-header-expander>
                        <svg class="eds-c-header__icon" width="24" height="24" aria-hidden="true" focusable="false">
                            <use xlink:href="#icon-eds-i-menu-medium"></use>
                        </svg><span>Menu</span>
                    </a>
                </div>

                <div class="eds-c-header__item eds-c-header__item--inline-links">
                    
                        <a class="eds-c-header__link" href="https://link.springer.com/journals/"
                            
                                data-track="nav_find_a_journal"
                            
                                data-track-context="unified header"
                            
                                data-track-action="click find a journal"
                            
                                data-track-category="unified header"
                            
                                data-track-label="link"
                            
						>
                            Find a journal
                        </a>
                    
                        <a class="eds-c-header__link" href="https://www.springernature.com/gp/authors"
                            
                                data-track="nav_how_to_publish"
                            
                                data-track-context="unified header"
                            
                                data-track-action="click publish with us link"
                            
                                data-track-category="unified header"
                            
                                data-track-label="link"
                            
						>
                            Publish with us
                        </a>
                    
                        <a class="eds-c-header__link" href="https://link.springernature.com/home/"
                            
                                data-track="nav_track_your_research"
                            
                                data-track-context="unified header"
                            
                                data-track-action="click track your research"
                            
                                data-track-category="unified header"
                            
                                data-track-label="link"
                            
						>
                            Track your research
                        </a>
                    
                </div>

                <div class="eds-c-header__link-container">
                    
                        <div class="eds-c-header__item eds-c-header__item--divider">
                            <a href="#eds-c-header-popup-search" class="eds-c-header__link" data-eds-c-header-expander data-eds-c-header-test-search-btn>
                                <svg class="eds-c-header__icon" width="24" height="24" aria-hidden="true" focusable="false">
                                    <use xlink:href="#icon-eds-i-search-medium"></use>
                                </svg><span>Search</span>
                            </a>
                        </div>
                    
                    
                        <div id="ecommerce-header-cart-icon-link" class="eds-c-header__item ecommerce-cart" style="display:inline-block">
 <a class="eds-c-header__link" href="https://order.springer.com/public/cart" style="appearance:none;border:none;background:none;color:inherit;position:relative">
  <svg id="eds-i-cart" class="eds-c-header__icon" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 24 24" aria-hidden="true" focusable="false">
   <path fill="currentColor" fill-rule="nonzero" d="M2 1a1 1 0 0 0 0 2l1.659.001 2.257 12.808a2.599 2.599 0 0 0 2.435 2.185l.167.004 9.976-.001a2.613 2.613 0 0 0 2.61-1.748l.03-.106 1.755-7.82.032-.107a2.546 2.546 0 0 0-.311-1.986l-.108-.157a2.604 2.604 0 0 0-2.197-1.076L6.042 5l-.56-3.17a1 1 0 0 0-.864-.82l-.12-.007L2.001 1ZM20.35 6.996a.63.63 0 0 1 .54.26.55.55 0 0 1 .082.505l-.028.1L19.2 15.63l-.022.05c-.094.177-.282.299-.526.317l-10.145.002a.61.61 0 0 1-.618-.515L6.394 6.999l13.955-.003ZM18 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4ZM8 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"></path>
  </svg>
  <span>Cart</span><span class="cart-info" style="display:none;position:absolute;top:10px;right:45px;background-color:#C65301;color:#fff;width:18px;height:18px;font-size:11px;border-radius:50%;line-height:17.5px;text-align:center"></span>
 </a>
 <script>(function () { var exports = {}; if (window.fetch) {
            
            "use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.headerWidgetClientInit = void 0;
var headerWidgetClientInit = function (getCartInfo) {
    document.body.addEventListener("updatedCart", function () {
        updateCartIcon();
    }, false);
    return updateCartIcon();
    function updateCartIcon() {
        return getCartInfo()
            .then(function (res) { return res.json(); })
            .then(refreshCartState)
            .catch(function (_) { });
    }
    function refreshCartState(json) {
        var indicator = document.querySelector("#ecommerce-header-cart-icon-link .cart-info");
        /* istanbul ignore else */
        if (indicator && json.itemCount) {
            indicator.style.display = 'block';
            indicator.textContent = json.itemCount > 9 ? '9+' : json.itemCount.toString();
            var moreThanOneItem = json.itemCount > 1;
            indicator.setAttribute('title', "there ".concat(moreThanOneItem ? "are" : "is", " ").concat(json.itemCount, " item").concat(moreThanOneItem ? "s" : "", " in your cart"));
        }
        return json;
    }
};
exports.headerWidgetClientInit = headerWidgetClientInit;

            
            headerWidgetClientInit(
              function () {
                return window.fetch("https://cart.springer.com/cart-info", {
                  credentials: "include",
                  headers: { Accept: "application/json" }
                })
              }
            )
        }})()</script>
</div>
                    
                </div>
            </div>
        </nav>
    
</header>



    <article lang="en" id="main" class="app-masthead__colour-30">
        <section class="app-masthead " aria-label="article masthead">
    <div class="app-masthead__container">
        
            <div class="app-article-masthead u-sans-serif js-context-bar-sticky-point-masthead" data-track-component="article" data-test="masthead-component">
                <div class="app-article-masthead__info">
                    
    
        <nav aria-label="breadcrumbs" data-test="breadcrumbs">
            <ol class="c-breadcrumbs c-breadcrumbs--contrast" itemscope itemtype="https://schema.org/BreadcrumbList">
                
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <a href="/" class="c-breadcrumbs__link" itemprop="item" data-track="click_breadcrumb" data-track-context="article page"  data-track-category="article" data-track-action="breadcrumbs" data-track-label="breadcrumb1"><span itemprop="name">Home</span></a><meta itemprop="position" content="1">
                            <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" width="10" height="10" viewBox="0 0 10 10">
                                <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                            </svg>
                    </li>
                
                    <li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <a href="/journal/12559" class="c-breadcrumbs__link" itemprop="item" data-track="click_breadcrumb" data-track-context="article page"  data-track-category="article" data-track-action="breadcrumbs" data-track-label="breadcrumb2"><span itemprop="name">Cognitive Computation</span></a><meta itemprop="position" content="2">
                            <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" width="10" height="10" viewBox="0 0 10 10">
                                <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                            </svg>
                    </li>
                
                    <li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <span itemprop="name">Article</span><meta itemprop="position" content="3">
                    </li>
                
            </ol>
        </nav>
    

                    <h1 class="c-article-title" data-test="article-title" data-article-title="">Explainable AI for Text Classification: Lessons from a Comprehensive Evaluation of Post Hoc Methods</h1>

                    <ul class="c-article-identifiers">
                        
        <li class="c-article-identifiers__item" data-test="article-category">Review</li>
    
        <li class="c-article-identifiers__item">
            <a href="https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research" data-track="click" data-track-action="open access" data-track-label="link" class="u-color-open-access" data-test="open-access">Open access</a>
        </li>
    
    

                        <li class="c-article-identifiers__item">
                            Published: <time datetime="2024-08-06">06 August 2024</time>
                        </li>
                    </ul>
                    <ul class="c-article-identifiers c-article-identifiers--cite-list">
                        <li class="c-article-identifiers__item">
                            <span data-test="journal-volume">Volume 16</span>, pages 3077–3095, (<span data-test="article-publication-year">2024</span>)
                        </li>
                        <li class="c-article-identifiers__item c-article-identifiers__item--cite">
                            <a href="#citeas" data-track="click" data-track-action="cite this article" data-track-category="article body" data-track-label="link">Cite this article</a>
                        </li>
                    </ul>

                    <div class="app-article-masthead__buttons" data-test="download-article-link-wrapper" data-track-context="masthead">
                        <p class="app-article-masthead__access">
                            <svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-check-filled-medium"></use></svg>
                            You have full access to this <a href="https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research" data-track="click" data-track-action="open access" data-track-label="link">open access</a> article</p>
                        

                        
                            
        <div class="c-pdf-container">
            <div class="c-pdf-download u-clear-both u-mb-16">
                <a href="/content/pdf/10.1007/s12559-024-10325-w.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="content_download" data-track-type="article pdf download" data-track-action="download pdf" data-track-label="button" data-track-external download>
                    
                        <span class="c-pdf-download__text">Download PDF</span>
                        <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-eds-i-download-medium"/></svg>
                    
                </a>
            </div>
        </div>
    

                        
                    </div>
                </div>
                <div class="app-article-masthead__brand">
                    
                        
                            <a href="/journal/12559"
                        
                           class="app-article-masthead__journal-link"
                           data-track="click_journal_home"
                           data-track-action="journal homepage"
                           data-track-context="article page"
                           data-track-label="link">
                            <picture>
                                <source type="image/webp" media="(min-width: 768px)" width="120" height="159"
                                        srcset="https://media.springernature.com/w120/springer-static/cover-hires/journal/12559?as=webp,
                                                    https://media.springernature.com/w316/springer-static/cover-hires/journal/12559?as=webp 2x">
                                <img width="72" height="95"
                                     src="https://media.springernature.com/w72/springer-static/cover-hires/journal/12559?as=webp"
                                     srcset="https://media.springernature.com/w144/springer-static/cover-hires/journal/12559?as=webp 2x" alt="">
                            </picture>
                            <span class="app-article-masthead__journal-title">Cognitive Computation</span>
                        </a>
                        
                            <a href="/journal/12559/aims-and-scope" class="app-article-masthead__submission-link"
                               data-track="click_aims_and_scope"
                               data-track-action="aims and scope"
                               data-track-context="article page"
                               data-track-label="link">
                                Aims and scope
                                <svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-arrow-right-medium"></use></svg>
                            </a>
                        
                        
                            <a href="https://submission.springernature.com/new-submission/12559/3" class="app-article-masthead__submission-link"
                               data-track="click_submit_manuscript"
                               data-track-context="article masthead on springerlink article page"
                               data-track-action="submit manuscript"
                               data-track-label="link">
                                Submit manuscript
                                <svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-arrow-right-medium"></use></svg>
                            </a>
                        
                    
                </div>
            </div>
        
    </div>
</section>

        <div class="c-article-main u-container u-mt-24 u-mb-32 l-with-sidebar" id="main-content"
             data-component="article-container">
            <main class="u-serif js-main-column" data-track-component="article body">
                
                
                    <div class="c-context-bar u-hide"
                         data-test="context-bar"
                         data-context-bar
                         aria-hidden="true">
                        <div class="c-context-bar__container u-container">
                            <div class="c-context-bar__title">
                                Explainable AI for Text Classification: Lessons from a Comprehensive Evaluation of Post Hoc Methods
                            </div>
                            
                                <div data-test="inCoD" data-track-context="sticky banner">
                                    
        <div class="c-pdf-container">
            <div class="c-pdf-download u-clear-both u-mb-16">
                <a href="/content/pdf/10.1007/s12559-024-10325-w.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="content_download" data-track-type="article pdf download" data-track-action="download pdf" data-track-label="button" data-track-external download>
                    
                        <span class="c-pdf-download__text">Download PDF</span>
                        <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-eds-i-download-medium"/></svg>
                    
                </a>
            </div>
        </div>
    

                                    
                                </div>
                            
                        </div>
                    </div>
                

                <div class="c-article-header">
                    <header>
                        <ul class="c-article-author-list c-article-author-list--short" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="1_5" data-track-context="researcher popup with no profile" href="#auth-Mirko-Cesarini-Aff1" data-author-popup="auth-Mirko-Cesarini-Aff1" data-author-search="Cesarini, Mirko">Mirko Cesarini</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="2_5" data-track-context="researcher popup with no profile" href="#auth-Lorenzo-Malandri-Aff1" data-author-popup="auth-Lorenzo-Malandri-Aff1" data-author-search="Malandri, Lorenzo" data-corresp-id="c1">Lorenzo Malandri<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="3_5" data-track-context="researcher popup with no profile" href="#auth-Filippo-Pallucchini-Aff1" data-author-popup="auth-Filippo-Pallucchini-Aff1" data-author-search="Pallucchini, Filippo">Filippo Pallucchini</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="4_5" data-track-context="researcher popup with no profile" href="#auth-Andrea-Seveso-Aff1" data-author-popup="auth-Andrea-Seveso-Aff1" data-author-search="Seveso, Andrea">Andrea Seveso</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup> &amp; </li><li class="c-article-author-list__show-more" aria-label="Show all 5 authors for this article" title="Show all 5 authors for this article">…</li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="5_5" data-track-context="researcher popup with no profile" href="#auth-Frank-Xing-Aff2" data-author-popup="auth-Frank-Xing-Aff2" data-author-search="Xing, Frank">Frank Xing</a><sup class="u-js-hide"><a href="#Aff2">2</a></sup> </li></ul><button aria-expanded="false" class="c-article-author-list__button"><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-down-medium"></use></svg><span>Show authors</span></button>
                        
    


                        <div data-test="article-metrics">
                            
        <ul class="app-article-metrics-bar u-list-reset">
            
                <li class="app-article-metrics-bar__item" data-test="access-count">
                    <p class="app-article-metrics-bar__count"><svg class="u-icon app-article-metrics-bar__icon" width="24" height="24" aria-hidden="true" focusable="false">
                        <use xlink:href="#icon-eds-i-accesses-medium"></use>
                    </svg>4085 <span class="app-article-metrics-bar__label">Accesses</span></p>
                </li>
            
            
                <li class="app-article-metrics-bar__item" data-test=citation-count>
                    <p class="app-article-metrics-bar__count"><svg class="u-icon app-article-metrics-bar__icon" width="24" height="24" aria-hidden="true" focusable="false">
                        <use xlink:href="#icon-eds-i-citations-medium"></use>
                    </svg>12 <span class="app-article-metrics-bar__label">Citations</span></p>
                </li>
            
            
                
            
            
            
                
                    <li class="app-article-metrics-bar__item app-article-metrics-bar__item--metrics">
                        <p class="app-article-metrics-bar__details"><a href="/article/10.1007/s12559-024-10325-w/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Explore all metrics <svg class="u-icon app-article-metrics-bar__arrow-icon" width="24" height="24" aria-hidden="true" focusable="false">
                            <use xlink:href="#icon-eds-i-arrow-right-medium"></use>
                        </svg></a></p>
                    </li>
                
            
        </ul>
    
                        </div>
                        
                        
    <div class="u-mt-32">
    

    
    </div>

                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">

                    
                    <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper addresses the notable gap in evaluating eXplainable Artificial Intelligence (XAI) methods for text classification. While existing frameworks focus on assessing XAI in areas such as recommender systems and visual analytics, a comprehensive evaluation is missing. Our study surveys and categorises recent post hoc XAI methods according to their scope of explanation and output format. We then conduct a systematic evaluation, assessing the effectiveness of these methods across varying scopes and levels of output granularity using a combination of objective metrics and user studies. Key findings reveal that feature-based explanations exhibit higher fidelity than rule-based ones. While global explanations are perceived as more satisfying and trustworthy, they are less practical than local explanations. These insights enhance understanding of XAI in text classification and offer valuable guidance for developing effective XAI systems, enabling users to evaluate each explainer’s pros and cons and select the most suitable one for their needs.</p></div></div></section>

                    

                    
    


                    

                    <div data-test="cobranding-download">
                        
                    </div>

                    
                        
        
            <section aria-labelledby="inline-recommendations" data-title="Inline Recommendations" class="c-article-recommendations" data-track-component="inline-recommendations">
                <h3 class="c-article-recommendations-title" id="inline-recommendations">Similar content being viewed by others</h3>
                <div class="c-article-recommendations-list">
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-981-19-8563-8?as&#x3D;webp" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://link.springer.com/10.1007/978-981-19-8563-8_41?fromPaywallRec=false"
                                           data-track="select_recommendations_1"
                                           data-track-context="inline recommendations"
                                           data-track-action="click recommendations inline - 1"
                                           data-track-label="10.1007/978-981-19-8563-8_41">Exploring the Potential of eXplainable AI in Identifying Errors and Biases
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations" data-test="recommendation-info">
                                        <span class="c-article-meta-recommendations__item-type">Chapter</span>
                                        
                                         <span class="c-article-meta-recommendations__date">© 2023</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-20319-0?as&#x3D;webp" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://link.springer.com/10.1007/978-3-031-20319-0_30?fromPaywallRec=false"
                                           data-track="select_recommendations_2"
                                           data-track-context="inline recommendations"
                                           data-track-action="click recommendations inline - 2"
                                           data-track-label="10.1007/978-3-031-20319-0_30">Evaluation Metrics in Explainable Artificial Intelligence (XAI)
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations" data-test="recommendation-info">
                                        <span class="c-article-meta-recommendations__item-type">Chapter</span>
                                        
                                         <span class="c-article-meta-recommendations__date">© 2022</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs44230-023-00038-y/MediaObjects/44230_2023_38_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://link.springer.com/10.1007/s44230-023-00038-y?fromPaywallRec=false"
                                           data-track="select_recommendations_3"
                                           data-track-context="inline recommendations"
                                           data-track-action="click recommendations inline - 3"
                                           data-track-label="10.1007/s44230-023-00038-y">Survey on Explainable AI: From Approaches, Limitations and Applications Aspects
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations" data-test="recommendation-info">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">10 August 2023</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                </div>
            </section>
        
            <script>
                window.dataLayer = window.dataLayer || [];
                window.dataLayer.push({
                    recommendations: {
                        recommender: 'semantic',
                        model: 'specter',
                        policy_id: 'NA',
                        timestamp: 1760786481,
                        embedded_user: 'null'
                    }
                });
            </script>
        
    
                    

                    
                        
    <section class="app-explore-related-subjects" aria-labelledby="content-related-subjects" data-test="subject-content">
        <h3 id="content-related-subjects" class="app-explore-related-subjects__title">Explore related subjects</h3>
        <span class="u-sans-serif u-text-xs u-display-block u-mb-16">Discover the latest articles, books and news in related subjects, suggested using machine learning.</span>
        <ul class="app-explore-related-subjects__list app-explore-related-subjects__list--no-mb" role="list">
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/categorization"  data-track="select_related_subject_1" data-track-context="related subjects from content page" data-track-label="Categorization">Categorization</a>
            </li>
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/computational-intelligence"  data-track="select_related_subject_2" data-track-context="related subjects from content page" data-track-label="Computational Intelligence">Computational Intelligence</a>
            </li>
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/learning-algorithms"  data-track="select_related_subject_3" data-track-context="related subjects from content page" data-track-label="Learning algorithms">Learning algorithms</a>
            </li>
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/machine-translation"  data-track="select_related_subject_4" data-track-context="related subjects from content page" data-track-label="Machine Translation">Machine Translation</a>
            </li>
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/machine-learning"  data-track="select_related_subject_5" data-track-context="related subjects from content page" data-track-label="Machine Learning">Machine Learning</a>
            </li>
        
            <li class="app-explore-related-subjects__items" data-test="related-subject-item">
                <a href="/subjects/artificial-intelligence"  data-track="select_related_subject_6" data-track-context="related subjects from content page" data-track-label="Artificial Intelligence">Artificial Intelligence</a>
            </li>
        
        </ul>
    </section>

                    

                    
                        
    <div class="app-card-service" data-test="article-checklist-banner">
        <div>
            <a class="app-card-service__link" data-track="click_presubmission_checklist" data-track-context="article page top of reading companion" data-track-category="pre-submission-checklist" data-track-action="clicked article page checklist banner test 2 old version" data-track-label="link" href="https://beta.springernature.com/pre-submission?journalId=12559"
            data-test="article-checklist-banner-link">
            <span class="app-card-service__link-text">Use our pre-submission checklist</span>
            <svg class="app-card-service__link-icon" aria-hidden="true" focusable="false"><use xlink:href="#icon-eds-i-arrow-right-small"></use></svg>
            </a>
            <p class="app-card-service__description">Avoid common mistakes on your manuscript.</p>
        </div>
        <div class="app-card-service__icon-container">
            <svg class="app-card-service__icon" aria-hidden="true" focusable="false">
                <use xlink:href="#icon-eds-i-clipboard-check-medium"></use>
            </svg>
        </div>
    </div>

                    

                    
                        
                                <div class="main-content">
                                    <section data-title="Introduction and Motivation"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction and Motivation</h2><div class="c-article-section__content" id="Sec1-content"><p>In several Machine Learning (ML) applications, the ability to explain a model’s predictions and provide the rationale behind the output for any particular data point is just as important as the accuracy of those predictions in various applications [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gunning D, Stefik M, Choi J, Miller T, Stumpf S, Yang G-Z. XAI—explainable artificial intelligence. Sci Robot. 2019;4(37):7120." href="#ref-CR1" id="ref-link-section-d156213074e403">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gozzi N, Malandri L, Mercorio F, Pedrocchi A. XAI for myo-controlled prosthesis: Explaining EMG data for hand gesture classification. Knowl Based Syst. 2022;240:108053." href="#ref-CR2" id="ref-link-section-d156213074e403_1">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Xing F, Malandri L, Zhang Y, Cambria E. Financial sentiment analysis: An investigation into common mistakes and silver bullets. In: Proceedings of the 28th International Conference on Computational Linguistics. 2020. pp. 978–87." href="/article/10.1007/s12559-024-10325-w#ref-CR3" id="ref-link-section-d156213074e406">3</a>]. Achieving peak accuracy on extensive modern datasets frequently entails employing complex models, like ensemble or deep learning models, and interpretation in this scenario is challenging and, in some cases, outright impossible. The trade-off between accuracy and interpretability has spurred the development of diverse methods to facilitate user comprehension of complex model predictions. Yet, how these methods address this trade-off is still the subject of ongoing research [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Hassija V, Chamola V, Mahapatra A, Singal A, Goel D, Huang K, Scardapane S, Spinelli I, Mahmud M, Hussain A. Interpreting black-box models: a review on explainable artificial intelligence. Cogn Comput. 2024;16(1):45–74." href="/article/10.1007/s12559-024-10325-w#ref-CR4" id="ref-link-section-d156213074e409">4</a>]. This study aims to provide a comprehensive overview of existing eXplainable Artificial Intelligence (XAI) methods documented in the literature and their suitability for text classification. Diverse data types are approached in a fundamentally distinct manner in XAI. For instance, tabular classifiers must deal with mixtures of continuous and categorical features, finding the right discretisation of the former, which can result in accurate and interpretable results at the same time [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Malandri L, Mercorio F, Mezzanzanica M, Seveso A. Model-contrastive explanations through symbolic reasoning. Decis Support Syst. 2024;176:114040." href="/article/10.1007/s12559-024-10325-w#ref-CR5" id="ref-link-section-d156213074e412">5</a>]. XAI for images, to give another example, does not usually explain classification outputs at a single feature level (i.e. pixel level) but focuses on higher level features, often presented in the form of heatmaps or saliency maps [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Cambria E, Malandri L, Mercorio F, Mezzanzanica M, Nobani N. A survey on XAI and natural language explanations. Inf Process Manag. 2023;60(1):103111." href="/article/10.1007/s12559-024-10325-w#ref-CR6" id="ref-link-section-d156213074e415">6</a>]. Even the XAI methods for textual data differ regarding input features, underlying models, and output. Furthermore, Textual data need to be computationally feasible for many features, which typically comprise the dictionaries of textual corpora [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Arrieta AB, Díaz-Rodríguez N, Del Ser J, Bennetot A, Tabik S, Barbado A, García S, Gil-López S, Molina D, Benjamins R, et al. Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Inf Fusion. 2020;58:82–115." href="/article/10.1007/s12559-024-10325-w#ref-CR7" id="ref-link-section-d156213074e419">7</a>]. In this article, we narrow down the scope of the paper on XAI for text classification, which has gained great importance in academia and industry in the last years [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Li Q, Peng H, Li J, Xia C, Yang R, Sun L, Yu PS, He L. A survey on text classification: From traditional to deep learning. ACM Trans Intell Syst Technol (TIST). 2022;13(2):1–41." href="/article/10.1007/s12559-024-10325-w#ref-CR8" id="ref-link-section-d156213074e422">8</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Minaee S, Kalchbrenner N, Cambria E, Nikzad N, Chenaghlu M, Gao J. Deep learning-based text classification: a comprehensive review. ACM Comput Surv (CSUR). 2021;54(3):1–40." href="/article/10.1007/s12559-024-10325-w#ref-CR9" id="ref-link-section-d156213074e425">9</a>].</p><p>As the research field of XAI rapidly grows, some previous surveys [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Arrieta AB, Díaz-Rodríguez N, Del Ser J, Bennetot A, Tabik S, Barbado A, García S, Gil-López S, Molina D, Benjamins R, et al. Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Inf Fusion. 2020;58:82–115." href="/article/10.1007/s12559-024-10325-w#ref-CR7" id="ref-link-section-d156213074e431">7</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56–67." href="/article/10.1007/s12559-024-10325-w#ref-CR10" id="ref-link-section-d156213074e434">10</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Burkart N, Huber MF. A survey on the explainability of supervised machine learning. J Artif Intell Res. 2021;70:245–317." href="/article/10.1007/s12559-024-10325-w#ref-CR11" id="ref-link-section-d156213074e437">11</a>] have attempted to identify the most suitable XAI methods for specific user needs. Still, as we will discuss in the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s12559-024-10325-w#Sec4">Related Works</a>” section, none has succeeded in offering a truly exhaustive perspective, leaving users with limited guidance based on the available literature. To address this problem, we describe how to use several XAI methods in real-case scenarios, evaluating each algorithm’s performance and the insights it provides.</p><p>The rationale behind preferring one XAI method over another varies depending on the specific requirements; for example, explainer <i>A</i>’s transparency might be a deciding factor, while explainer <i>B</i>’s ability to cover a broader range of data could be advantageous in others.</p><p>As previously introduced, many XAI methods are available in the literature. In the next section, we will present the decision-making process to identify the XAI methods included in this study and provide an overview of the selected techniques. Our XAI methods comparison is based on a real-world dataset, incorporating user evaluations and using existing metrics. The rationale for the approach used in this paper is that, despite that there are theoretical assurances regarding the selected XAI methods, certain properties may be compromised in specific application domains or datasets. Thus, a real-world application is essential for a comprehensive overview and to assist users in identifying and deploying the most suitable XAI method for their particular use case.</p><p>The big challenge is evaluating XAI methods since scholars from different disciplines focus on different objectives, which poses challenges for identifying appropriate design and evaluation methodology [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Mohseni S, Zarei N, Ragan ED. A multidisciplinary survey and framework for design and evaluation of explainable ai systems. ACM Trans Interact Intell Syst. 2021;11(3–4):1–45. &#xA;                  https://doi.org/10.1145/3387166&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR12" id="ref-link-section-d156213074e459">12</a>].</p><p>While numerous well-established works, such as Sokol and Flach [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56–67." href="/article/10.1007/s12559-024-10325-w#ref-CR10" id="ref-link-section-d156213074e465">10</a>], theoretically introduced several metrics, unfortunately, it is still unclear how to practically utilise them for comparing explanation methods [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Zhou J, Gandomi AH, Chen F, Holzinger A. Evaluating the quality of machine learning explanations: A survey on methods and metrics. Electronics. 2021;10(5):593." href="/article/10.1007/s12559-024-10325-w#ref-CR13" id="ref-link-section-d156213074e468">13</a>]. Therefore, we have re-investigated the existing metrics, we chose the most suitable ones for the proposed benchmark and we developed a method for measuring them.</p><p>The contributions of this work can be summarised in three points: </p><ol class="u-list-style-none">
                <li>
                  <span class="u-custom-list-number">(i)</span>
                  
                    <p>Gathering all these XAI methods into an Evaluation tasks will empower users of explainability systems to comprehensively assess the pros and cons of each explainer, facilitating informed decisions to select the most suitable one for their specific application.</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">(ii)</span>
                  
                    <p>Additionally, the proposed benchmark can serve as a valuable tool for both the development and deployment phases of explainable approaches, providing a structured checklist to ensure a thorough evaluation and to support a successful integration into various systems.</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">(iii)</span>
                  
                    <p>All explainers have been deployed in notebooks and are accessible through a GitHub<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> repository, promoting transparency, reproducibility, and easy adoption by the community.</p>
                  
                </li>
              </ol><p>All acronyms employed in this manuscript can be found in the <a data-track="click" data-track-label="link" data-track-action="appendix anchor" href="/article/10.1007/s12559-024-10325-w#App1">Appendix</a> in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s12559-024-10325-w#Tab5">5</a>.</p></div></div></section><section data-title="Machine Learning Explanations"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Machine Learning Explanations</h2><div class="c-article-section__content" id="Sec2-content"><p>We will now outline the XAI methods for classification and elucidate the process by which we selected the ones deemed suitable for our analysis. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s12559-024-10325-w#Fig1">1</a> shows a concise and straightforward diagram that serves as a roadmap throughout the paper’s literature, facilitating the understanding of the key features that comprise any XAI method, as presented in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56–67." href="/article/10.1007/s12559-024-10325-w#ref-CR10" id="ref-link-section-d156213074e538">10</a>]. The feature descriptions in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s12559-024-10325-w#Fig1">1</a> are based on the framework proposed in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56–67." href="/article/10.1007/s12559-024-10325-w#ref-CR10" id="ref-link-section-d156213074e544">10</a>]. The primary distinction between XAI methods lies in the contrast between <i>ante hoc</i> and <i>post hoc</i> methodologies. <i>Ante hoc</i> approaches employ the same model for prediction and explanation, from elucidating a linear regression through its feature weights to explaining sentiment analysis results with neural network attention weights [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Du K, Xing F, Cambria E. Incorporating multiple knowledge sources for targeted aspect-based financial sentiment analysis. ACM Trans Manag Inf Syst. 2023;14(3):23." href="/article/10.1007/s12559-024-10325-w#ref-CR14" id="ref-link-section-d156213074e557">14</a>]. It’s crucial to acknowledge that certain techniques within this approach may be accompanied by caveats and assumptions regarding the training data or process, which must also be fulfilled for the explanation task. Still, the latter may not always be feasible. In <i>post hoc</i> approaches, predictions are generated using one model, while explanations are generated using a separate one. Post hoc approaches are this article’s main focus and allow for tailoring explanations to specific information needs while keeping the prediction model untouched. Both methodologies can be further categorised into two distinct groups: <i>model-agnostic</i>, which can operate independently of any model family, and <i>model-specific</i>, which solely applies to a particular model, such as decision trees. Finally, focusing on the generalisability property, each of the previously identified (sub)groups could be divided into three stages as follows: <i>local</i>, which pertains to a single data point or prediction; <i>cohort</i>, which involves analysing a subgroup within a dataset or a subspace within the model’s decision space; and <i>global</i>, which offers a comprehensive explanation of the model. We conducted a comprehensive literature review to identify the most commonly used XAI methods, and we framed the works in the categories just introduced, as reported in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s12559-024-10325-w#Tab1">1</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Fig. 1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s12559-024-10325-w/figures/1" rel="nofollow"><picture><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs12559-024-10325-w/MediaObjects/12559_2024_10325_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="285"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Concise taxonomy of XAI methods for classification</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s12559-024-10325-w/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>As advised by [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Keele S, et al. Guidelines for performing systematic literature reviews in software engineering. Technical Report, ver. 2.3 ebse technical report. ebse. 2007." href="/article/10.1007/s12559-024-10325-w#ref-CR15" id="ref-link-section-d156213074e602">15</a>], we comprehensively searched electronic databases. The databases utilised for this search were as follows:</p><ul class="u-list-style-bullet">
                <li>
                  <p>ACL (<a href="https://aclanthology.org/">https://aclanthology.org/</a>)</p>
                </li>
                <li>
                  <p>Springer (<a href="http://www.springerlink.com">www.springerlink.com</a>)</p>
                </li>
                <li>
                  <p>ACM Digital Library (<a href="http://www.acm.org/dl">www.acm.org/dl</a>)</p>
                </li>
                <li>
                  <p>ScienceDirect (<a href="http://www.sciencedirect.com">www.sciencedirect.com</a>)</p>
                </li>
                <li>
                  <p>Wiley Interscience (<a href="http://www.Interscience.wiley.com">www.Interscience.wiley.com</a>)</p>
                </li>
                <li>
                  <p>Google Scholar (<a href="http://www.scholar.google.co.in">www.scholar.google.co.in</a>)</p>
                </li>
                <li>
                  <p>IEEE eXplore (<a href="https://ieeexplore.ieee.org/Xplore/home.jsp">www.ieeexplore.ieee.org</a>)</p>
                </li>
                <li>
                  <p>Taylor Francis Online (<a href="https://www.tandfonline.com/">https://www.tandfonline.com/</a>)</p>
                </li>
                <li>
                  <p>PubMed (<a href="https://pubmed.ncbi.nlm.nih.gov/">https://pubmed.ncbi.nlm.nih.gov/</a>)</p>
                </li>
                <li>
                  <p>SemEval (<a href="https://semeval.github.io/">https://semeval.github.io/</a>)</p>
                </li>
              </ul><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Mapping selected papers to our roadmap</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s12559-024-10325-w/tables/1" aria-label="Full size table 1"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>We conducted an extensive literature review to identify the most pertinent XAI methods. Our search encompassed research studies from diverse sources, including conferences, journals, and arXiv. Specifically, we focused on papers published in conferences ranked as A, and journals ranked as Q1 or at least Q2. However, despite not being published in a conference or journal, we made an exception for a notable work that garnered over 500 citations since 2017 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Guidotti R, Monreale A, Ruggieri S, Pedreschi D, Turini F, Giannotti F. Local rule-based explanations of black box decision systems. &#xA;                  arXiv:1805.10820&#xA;                  &#xA;                 [Preprint]. 2018. Available from: &#xA;                  http://arxiv.org/abs/1805.10820&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR16" id="ref-link-section-d156213074e830">16</a>]. A summary of the most representative methods under consideration is reported here. We have taken into consideration 29 methods: (a) We will begin with XAI methods employing a <i>post hoc</i> approach, offering <i>global</i> explanations, and maintaining <i>model agnosticism</i>. Among these, TREPAN. Craven and Shavlik [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Craven M, Shavlik J. Extracting tree-structured representations of trained networks. Adv Neural Inf Process Syst. 1995;8." href="/article/10.1007/s12559-024-10325-w#ref-CR17" id="ref-link-section-d156213074e842">17</a>] stands out as one of the earliest explainers we examined. This algorithm induces a decision tree that approximates the outcome of a classifier. SAGE [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Covert I, Lundberg SM, Lee S-I. Understanding global feature contributions with additive importance measures. Adv Neural Inf Process Syst. 2020;33:17212–23." href="/article/10.1007/s12559-024-10325-w#ref-CR18" id="ref-link-section-d156213074e846">18</a>] uses shapley values to quantify the predictive power of individual input features at a global level while considering feature interactions. TREPAN constructs its tree using a hill-climbing search process and a gain ratio criterion to identify the best M-of-N splits for each node. ProfWeight [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Dhurandhar A, Shanmugam K, Luss R, Olsen PA. Improving simple models with confidence profiles. Adv Neural Inf Process Syst. 2018;31." href="/article/10.1007/s12559-024-10325-w#ref-CR19" id="ref-link-section-d156213074e849">19</a>] utilises linear probes to generate confidence scores via flattened intermediate representations, while GLRM [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Wei D, Dash S, Gao T, Gunluk O. Generalized linear rule models. In: International Conference on Machine Learning. PMLR; 2019. pp. 6687–96." href="/article/10.1007/s12559-024-10325-w#ref-CR20" id="ref-link-section-d156213074e852">20</a>] employs rule-based features for regression and probabilistic classification. These rules aid in model interpretation by capturing nonlinear dependencies and interactions. GLRM utilises column generation techniques to optimise over an exponentially large space of rules without the need to pre-generate a large subset of candidates or boost rules greedily one by one. (b) Concerning <i>model-specific</i> approaches, we find the work of Sushil et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Sushil M, Šuster S, Daelemans W. Rule induction for global explanation of trained models. In: Analyzing and Interpreting Neural Networks for NLP (BlackBoxNLP), Workshop at EMNLP. 2018. pp. 82–97." href="/article/10.1007/s12559-024-10325-w#ref-CR21" id="ref-link-section-d156213074e858">21</a>] to be particularly relevant for our purposes. Their research identifies if-then-else rules between various input features and the class labels that a trained network captures. (c) When examining <i>post hoc</i> methods with a <i>local</i> scope and <i>model-agnostic</i> nature, it’s crucial to consider arguably the two most renowned XAI methods: LIME [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Ribeiro MT, Singh S, Guestrin C. “Why should I trust you?” Explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2016. pp. 1135–44." href="/article/10.1007/s12559-024-10325-w#ref-CR22" id="ref-link-section-d156213074e871">22</a>] and SHAP [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Lundberg SM, Lee S-I. A unified approach to interpreting model predictions. Adv Neural Inf Process Syst. 2017;30." href="/article/10.1007/s12559-024-10325-w#ref-CR23" id="ref-link-section-d156213074e874">23</a>]. LIME elucidates the predictions of any classifier in an interpretable and accurate manner by constructing explanations locally around the prediction. On the other hand, SHAP assigns an importance value to each feature, based on the concept of Shapley values from the cooperative game theory, indicating its contribution to the model’s prediction. Other methodologies within this category include the one proposed by van der Waa et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="van der Waa J, Robeer M, van Diggelen J, Brinkhuis M. Neerincx M. Contrastive explanations with local foil trees. In: Proceedings of the ICML Workshop on Human Interpretability in Machine Learning (WHI 2018), Stockholm, Sweden. 2018. p. 37." href="/article/10.1007/s12559-024-10325-w#ref-CR24" id="ref-link-section-d156213074e877">24</a>], which utilises locally trained one-versus-all decision trees to identify the disjoint set of rules responsible for classifying data points as the foil rather than the fact. Another notable approach is the one introduced by Elenberg et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Elenberg E, Dimakis AG, Feldman M, Karbasi A. Streaming weak submodularity: Interpreting neural networks on the fly. Adv Neural Inf Process Syst. 2017;30." href="/article/10.1007/s12559-024-10325-w#ref-CR25" id="ref-link-section-d156213074e880">25</a>], called STREAK, wherein the authors frame the interpretability of black box classifiers as a combinatorial maximisation problem and present an efficient streaming algorithm to solve it, subject to cardinality constraints. In addition to the previously mentioned explainers, various other approaches to explainability have been explored. These include techniques such as the one proposed by Lei et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Lei T, Barzilay R, Jaakkola T. Rationalizing neural predictions. In: Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. 2016. pp. 107–17." href="/article/10.1007/s12559-024-10325-w#ref-CR26" id="ref-link-section-d156213074e884">26</a>], which extracts concise and coherent pieces of input text as justifications; TCAV [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Kim B, Wattenberg M, Gilmer J, Cai C, Wexler J, Viegas F, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In: International Conference on Machine Learning. PMLR; 2018. pp. 2668–77." href="/article/10.1007/s12559-024-10325-w#ref-CR27" id="ref-link-section-d156213074e887">27</a>], utilising directional derivatives to measure the importance of user-defined concepts; CheckList [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Ribeiro MT, Wu T, Guestrin C, Singh S. Beyond accuracy: Behavioral testing of NLP models with checklist. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020. pp. 4902–12." href="/article/10.1007/s12559-024-10325-w#ref-CR28" id="ref-link-section-d156213074e890">28</a>], which evaluates explainers’ capabilities through distinct test types; QII [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Datta A, Sen S, Zick Y. Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. In: 2016 IEEE Symposium on Security and Privacy (SP). IEEE; 2016. pp. 598–617." href="/article/10.1007/s12559-024-10325-w#ref-CR29" id="ref-link-section-d156213074e893">29</a>], breaking input correlations for causal reasoning and marginal influence computation; TED [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Hind M, Wei D, Campbell M, Codella NC, Dhurandhar A, Mojsilović A, Natesan Ramamurthy K, Varshney KR. Ted: Teaching AI to explain its decisions. In: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 2019. pp. 123–9." href="/article/10.1007/s12559-024-10325-w#ref-CR30" id="ref-link-section-d156213074e896">30</a>], providing explanations coherent with consumer mental models; Staniak and Biecek [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Staniak M, Biecek P. Explanations of model predictions with live and breakdown packages. R J. 2018;10(2)." href="/article/10.1007/s12559-024-10325-w#ref-CR31" id="ref-link-section-d156213074e899">31</a>] alternative implementation of LIME; LORE [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Guidotti R, Monreale A, Ruggieri S, Pedreschi D, Turini F, Giannotti F. Local rule-based explanations of black box decision systems. &#xA;                  arXiv:1805.10820&#xA;                  &#xA;                 [Preprint]. 2018. Available from: &#xA;                  http://arxiv.org/abs/1805.10820&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR16" id="ref-link-section-d156213074e903">16</a>], which employs a genetic algorithm to train local interpretable predictors for meaningful explanations; and lastly, we encountered CASME [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Zolna K, Geras KJ, Cho K. Classifier-agnostic saliency map extraction. Comput Vis Image Underst. 2020;196:102969." href="/article/10.1007/s12559-024-10325-w#ref-CR32" id="ref-link-section-d156213074e906">32</a>], an approach that involves the simultaneous training of a classifier and a saliency mapping utilising stochastic gradient descent. (d) In the category of <i>ante hoc</i>, <i>model-specific</i>, and <i>global</i> XAI methods, we came across BRCG [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Dash S, Gunluk O, Wei D. Boolean decision rules via column generation. Adv Neural Inf Process Syst. 2018;31." href="/article/10.1007/s12559-024-10325-w#ref-CR33" id="ref-link-section-d156213074e918">33</a>], a study focused on learning Boolean rules. These rules are presented in either disjunctive normal form (DNF, OR-of-ANDs, equivalent to decision rule sets) or conjunctive normal form (CNF, AND-of-ORs), serving as an interpretable method for classification. (e) We encountered eight notable works in the category encompassing <i>post hoc</i>, <i>model-specific</i>, and <i>local</i> approaches. The most famous one is Grad-CAM [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra D. Grad-cam: Visual explanations from deep networks via gradient-based localization. In: Proceedings of the IEEE International Conference on Computer Vision. 2017. pp. 618–26." href="/article/10.1007/s12559-024-10325-w#ref-CR34" id="ref-link-section-d156213074e931">34</a>], which leverages the gradients associated with a specific target concept, propagating through the final convolutional layer to generate a rough localisation map. This map accentuates significant areas within the image that contribute to predicting the concept. ACD [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Singh C, Murdoch WJ, Yu B. Hierarchical interpretations for neural network predictions. In: International Conference on Learning Representations. 2018." href="/article/10.1007/s12559-024-10325-w#ref-CR35" id="ref-link-section-d156213074e934">35</a>] utilises hierarchical clustering optimised to discern clusters of features learned by a Deep Neural Network as predictive. The CEM [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Dhurandhar A, Chen P-Y, Luss R, Tu C-C, Ting P, Shanmugam K, Das P. Explanations based on the missing: Towards contrastive explanations with pertinent negatives. Adv Neural Inf Process Syst. 2018;31." href="/article/10.1007/s12559-024-10325-w#ref-CR36" id="ref-link-section-d156213074e937">36</a>] algorithm identifies minimally and sufficiently present elements required to justify classification and those minimally and necessarily absent. DeepLIFT [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Shrikumar A, Greenside P, Kundaje A. Learning important features through propagating activation differences. In: International Conference on Machine Learning. PMLR; 2017. pp. 3145–53." href="/article/10.1007/s12559-024-10325-w#ref-CR37" id="ref-link-section-d156213074e941">37</a>] decomposes a neural network’s output prediction for a specific input by backpropagating the contributions of all network neurons to each input feature. LRP [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Lapuschkin S, Binder A, Montavon G, Müller K-R, Samek W. The LRP toolbox for artificial neural networks. J Mach Learn Res. 2016;17(114):1–5." href="/article/10.1007/s12559-024-10325-w#ref-CR38" id="ref-link-section-d156213074e944">38</a>] explains a classifier’s prediction for a given data point by attributing relevance scores to important input components using the model’s learned topology. MLAM [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Hu L, Jian S, Cao L, Chen Q. Interpretable recommendation via attraction modeling: Learning multilevel attractiveness over multimodal movie contents. In: IJCAI International Joint Conference on Artificial Intelligence. 2018." href="/article/10.1007/s12559-024-10325-w#ref-CR39" id="ref-link-section-d156213074e947">39</a>] focuses on identifying and interpreting attractive points in available content, explaining the user’s choices. RISE [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Petsiuk V, Das A. Saenko K. Rise: Randomized input sampling for explanation of black-box models. In: Proceedings of the British Machine Vision Conference (BMVC). 2018." href="/article/10.1007/s12559-024-10325-w#ref-CR40" id="ref-link-section-d156213074e950">40</a>] estimates importance empirically by probing the model with randomly masked versions of the input image, obtaining corresponding outputs. Finally, the work by Wang et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Wang T, Rudin C, Doshi-Velez F, Liu Y, Klampfl E, MacNeille P. A bayesian framework for learning rule sets for interpretable classification. J Mach Learn Res. 2017;18(70):1–37." href="/article/10.1007/s12559-024-10325-w#ref-CR41" id="ref-link-section-d156213074e953">41</a>] introduces an approximate inference method utilising association rule mining and a randomised search algorithm. In the final category (f), which includes <i>post hoc</i>, <i>model-agnostic</i>, and <i>cohort</i> explanation methods, we discovered only Anchors [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Ribeiro MT, Singh S, Guestrin C. Anchors: High-precision model-agnostic explanations. In: Proceedings of the AAAI Conference on Artificial Intelligence (vol. 32). 2018." href="/article/10.1007/s12559-024-10325-w#ref-CR42" id="ref-link-section-d156213074e966">42</a>]. Anchors is a systematic method designed to elucidate the behaviour of complex models by establishing high-precision rules known as anchors. These anchors represent local, “sufficient” conditions for prediction.</p><p>ProtoryNet [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Hong D, Wang T, Baek S. Protorynet - interpretable text classification via prototype trajectories. J Mach Learn Res. 2023;24(264):1–39." href="/article/10.1007/s12559-024-10325-w#ref-CR43" id="ref-link-section-d156213074e972">43</a>] is an approach for interpretable text classification based on prototypical learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Nauta M, Seifert C. The co-12 recipe for evaluating interpretable part-prototype image classifiers. In: Longo L, editor. Explainable Artificial Intelligence. Cham: Springer; 2023. p. 397–420." href="/article/10.1007/s12559-024-10325-w#ref-CR44" id="ref-link-section-d156213074e975">44</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Datta P, Kibler D. Learning prototypical concept descriptions. In: Machine Learning Proceedings 1995. 1995. pp. 158–66." href="/article/10.1007/s12559-024-10325-w#ref-CR45" id="ref-link-section-d156213074e978">45</a>]. In computer vision, part-prototype XAI methods are deep neural networks explainable by design (since they identify key parts of the image and use them to perform both classification and explanation). ProtoryNet [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Hong D, Wang T, Baek S. Protorynet - interpretable text classification via prototype trajectories. J Mach Learn Res. 2023;24(264):1–39." href="/article/10.1007/s12559-024-10325-w#ref-CR43" id="ref-link-section-d156213074e981">43</a>] is a notable work carrying on the prototype approach in text classification using neural networks. Unfortunately, ProtoryNet doesn’t fit the aim of this paper since it is a model-specific and ante hoc approach.</p><p>We focus on supervised models, as a significant portion of the literature is dedicated to them [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56–67." href="/article/10.1007/s12559-024-10325-w#ref-CR10" id="ref-link-section-d156213074e988">10</a>]. Explanations for these methods provide a rationale behind the output for any particular data point, serving as justifications for the provided predictions [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56–67." href="/article/10.1007/s12559-024-10325-w#ref-CR10" id="ref-link-section-d156213074e991">10</a>].</p><p>In this work, we focused on <i>model-agnostic</i> models. As previously mentioned, they can work with any model family, as they focus on revealing certain properties of the black box model by requiring only input values and predictions [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Burkart N, Huber MF. A survey on the explainability of supervised machine learning. J Artif Intell Res. 2021;70:245–317." href="/article/10.1007/s12559-024-10325-w#ref-CR11" id="ref-link-section-d156213074e1000">11</a>]. It is worth recalling that a <i>post hoc</i> approach is required to make an explainability technique model-agnostic. Concerning the third characteristic considered for models, the examined XAI methods encompass both <i>local</i>, <i>cohort</i> and <i>global</i> aspects. The carefully selected features in our analysis enable us to conduct a comprehensive benchmark study, providing valuable insights for user decision-making across a wide array of applications.</p><h3 class="c-article__sub-heading" id="Sec3">Selected Tools</h3><p>The evaluation focuses on model-agnostic tools, meaning they should not depend on internal model components like weights or structural information, ensuring applicability to any black box model. This choice is dictated by the fact that these explainers are more generally applicable and make it easier to compare several classification models. Again, for comparability, we discard papers that require handcrafted inputs, such as checklists [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Ribeiro MT, Wu T, Guestrin C, Singh S. Beyond accuracy: Behavioral testing of NLP models with checklist. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020. pp. 4902–12." href="/article/10.1007/s12559-024-10325-w#ref-CR28" id="ref-link-section-d156213074e1023">28</a>] or input explanations to be validated [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Hind M, Wei D, Campbell M, Codella NC, Dhurandhar A, Mojsilović A, Natesan Ramamurthy K, Varshney KR. Ted: Teaching AI to explain its decisions. In: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 2019. pp. 123–9." href="/article/10.1007/s12559-024-10325-w#ref-CR30" id="ref-link-section-d156213074e1026">30</a>]. Moreover, we consider only tools that have public, updated, and working Python code. Finally, we added transparent machine learning models to the above list that can be used as surrogates, i.e. decision trees (DT), logistic regression (LR), and naive Bayes (NB). Given these criteria, the selection falls on the following methods: LIME<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Ribeiro MT, Singh S, Guestrin C. “Why should I trust you?” Explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2016. pp. 1135–44." href="/article/10.1007/s12559-024-10325-w#ref-CR22" id="ref-link-section-d156213074e1040">22</a>], SHAP<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Lundberg SM, Lee S-I. A unified approach to interpreting model predictions. Adv Neural Inf Process Syst. 2017;30." href="/article/10.1007/s12559-024-10325-w#ref-CR23" id="ref-link-section-d156213074e1055">23</a>], SAGE<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Covert I, Lundberg SM, Lee S-I. Understanding global feature contributions with additive importance measures. Adv Neural Inf Process Syst. 2020;33:17212–23." href="/article/10.1007/s12559-024-10325-w#ref-CR18" id="ref-link-section-d156213074e1069">18</a>], BRCG<sup><a href="#Fn5"><span class="u-visually-hidden">Footnote </span>5</a></sup> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Dash S, Gunluk O, Wei D. Boolean decision rules via column generation. Adv Neural Inf Process Syst. 2018;31." href="/article/10.1007/s12559-024-10325-w#ref-CR33" id="ref-link-section-d156213074e1083">33</a>], Anchors<sup><a href="#Fn6"><span class="u-visually-hidden">Footnote </span>6</a></sup> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Ribeiro MT, Singh S, Guestrin C. Anchors: High-precision model-agnostic explanations. In: Proceedings of the AAAI Conference on Artificial Intelligence (vol. 32). 2018." href="/article/10.1007/s12559-024-10325-w#ref-CR42" id="ref-link-section-d156213074e1098">42</a>], QII<sup><a href="#Fn7"><span class="u-visually-hidden">Footnote </span>7</a></sup> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Datta A, Sen S, Zick Y. Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. In: 2016 IEEE Symposium on Security and Privacy (SP). IEEE; 2016. pp. 598–617." href="/article/10.1007/s12559-024-10325-w#ref-CR29" id="ref-link-section-d156213074e1112">29</a>], DT classifier,<sup><a href="#Fn8"><span class="u-visually-hidden">Footnote </span>8</a></sup> LR,<sup><a href="#Fn9"><span class="u-visually-hidden">Footnote </span>9</a></sup> and NB.<sup><a href="#Fn10"><span class="u-visually-hidden">Footnote </span>10</a></sup> Moreover, we add a rule-based random explainer, which generates random rules, and a random feature importance generator, built similarly as a baseline.</p><h3 class="c-article__sub-heading" id="Sec4">Related Works</h3><p>This work delves into the direction of evaluating XAI methods and explanations to facilitate <i>human evaluation</i>, according to the open challenge outlined in the XAI Manifesto [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Longo L, Brcic M, Cabitza F, Choi J, Confalonieri R, Ser JD, Guidotti R, Hayashi Y, Herrera F, Holzinger A, Jiang R, Khosravi H, Lecue F, Malgieri G, Páez A, Samek W, Schneider J, Speith T, Stumpf S. Explainable artificial intelligence (XAI) 2.0: A manifesto of open challenges and interdisciplinary research directions. Inf Fusion. 2024;106:102301. &#xA;                  https://doi.org/10.1016/j.inffus.2024.102301&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR48" id="ref-link-section-d156213074e1159">48</a>]. In preceding literature, several researchers have proposed a comparison of XAI methods. Some works [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Vilone G, Rizzo L, Longo L. A comparative analysis of rule-based, model-agnostic methods for explainable artificial intelligence. 2020." href="/article/10.1007/s12559-024-10325-w#ref-CR49" id="ref-link-section-d156213074e1162">49</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Vilone G, Longo L. A quantitative evaluation of global, rule-based explanations of post-hoc, model agnostic methods. Front Artif Intell. 2021;4:717899." href="/article/10.1007/s12559-024-10325-w#ref-CR50" id="ref-link-section-d156213074e1165">50</a>] focus on global, post hoc, rule-based explainers, comparing the rules generated by different decision trees. Other works evaluate a larger plethora of methods. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Belaid MK, Bornemann R, Rabus M, Krestel R, Hüllermeier E. Compare-XAI: Toward unifying functional testing methods for post-hoc XAI algorithms into a multi-dimensional benchmark. In: World Conference on Explainable Artificial Intelligence. Springer; 2023. pp. 88–109." href="/article/10.1007/s12559-024-10325-w#ref-CR51" id="ref-link-section-d156213074e1168">51</a>], the authors propose a scoring system that uses various functional tests from existing research, categorising the tests into four groups: fidelity, fragility, stability, and stress tests. They display results for 13 XAI methods using 11 functional tests. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Rasouli P, Yu IC. Explan: Explaining black-box classifiers using adaptive neighborhood generation. In: 2020 International Joint Conference on Neural Networks (IJCNN). IEEE; 2020. pp. 1–9." href="/article/10.1007/s12559-024-10325-w#ref-CR52" id="ref-link-section-d156213074e1172">52</a>], the authors propose EXPLAN, an algorithm that produces interpretable logical rules, ideal for qualitative analysis of the model’s behaviour, comparing it with LIME, LORE, and Anchor. However, EXPLAN is limited to local and cohort methods. A lot of works [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Hassija V, Chamola V, Mahapatra A, Singal A, Goel D, Huang K, Scardapane S, Spinelli I, Mahmud M, Hussain A. Interpreting black-box models: a review on explainable artificial intelligence. Cogn Comput. 2024;16(1):45–74." href="/article/10.1007/s12559-024-10325-w#ref-CR4" id="ref-link-section-d156213074e1175">4</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Dwivedi R, Dave D, Naik H, Singhal S, Omer R, Patel P, Qian B, Wen Z, Shah T, Morgan G, Ranjan R. Explainable AI (XAI): Core ideas, techniques, and solutions. ACM Comput Surv. 2023;55(9). &#xA;                  https://doi.org/10.1145/3561048&#xA;                  &#xA;                ." href="#ref-CR53" id="ref-link-section-d156213074e1178">53</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Schwalbe G, Finzel B. A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts. Data Min Knowl Disc. 2023;1:1–59." href="#ref-CR54" id="ref-link-section-d156213074e1178_1">54</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Saeed W, Omlin C. Explainable AI (XAI): A systematic meta-survey of current challenges and future opportunities. Knowl Based Syst. 2023;263:110273. &#xA;                  https://doi.org/10.1016/j.knosys.2023.110273&#xA;                  &#xA;                ." href="#ref-CR55" id="ref-link-section-d156213074e1178_2">55</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Yang W, Wei Y, Wei H, Chen Y, Huang G, Li X, Li R, Yao N, Wang X, Gu X, et al. Survey on explainable AI: From approaches, limitations and applications aspects. Hum Centric Intell Syst. 2023;3(3):161–88." href="#ref-CR56" id="ref-link-section-d156213074e1178_3">56</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Rong Y, Leemann T, Nguyen T-T, Fiedler L, Qian P, Unhelkar V, Seidel T, Kasneci G, Kasneci E. Towards human-centered explainable AI: A survey of user studies for model explanations. IEEE Trans Pattern Anal Mach Intell. 2024;46(4):2104–22. &#xA;                  https://doi.org/10.1109/TPAMI.2023.3331846&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR57" id="ref-link-section-d156213074e1181">57</a>] survey and discuss several XAI techniques to understand their capabilities and limitations and to categorise the investigated methods. Unfortunately, many taxonomies for XAI methods of varying levels of detail and depth can be found in the literature. While they often have a different focus, they also exhibit many points of overlap [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Schwalbe G, Finzel B. A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts. Data Min Knowl Disc. 2023;1:1–59." href="/article/10.1007/s12559-024-10325-w#ref-CR54" id="ref-link-section-d156213074e1184">54</a>]. The works above perform a rigorously structured and theoretically grounded analysis, but none compares the XAI methods to a common dataset.</p><p>Finally, in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Fauvel K, Masson V, Fromont E. A performance-explainability framework to benchmark machine learning methods: Application to multivariate time series classifiers. In: Proceedings of the IJCAI-PRICAI 2020 Workshop on Explainable AI. 2021. pp. 1–8." href="/article/10.1007/s12559-024-10325-w#ref-CR58" id="ref-link-section-d156213074e1190">58</a>], the authors propose a framework to benchmark XAI methods for time series. In all these methods, the comparison lacks differentiation between local and global methods and between rule-based and feature-based explanations, making it challenging. Moreover, none of the previous approaches compares XAI methods both through metrics and user studies, which is crucial for incorporating the user perspective into XAI method evaluation. Therefore, as far as we know, this is the first paper that (1) builds a comprehensive evaluation of different types of XAI methods, comparing local, global, and cohort methods and rule-based and feature-based explanations, also highlighting their differences (2) brings together metrics and user evaluation for a joint comparison and (3) makes the comparison reproducible by providing a repository with the implementation of the benchmarked methods and the metrics.</p></div></div></section><section data-title="Evaluation of ML Explanations"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Evaluation of ML Explanations</h2><div class="c-article-section__content" id="Sec5-content"><p>The recent proliferation of XAI methods requires to rigorously evaluate their efficacy and interpretability. Previous researches tend to agree that the main distinction is between objective versus human-centred metrics [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Zhou J, Gandomi AH, Chen F, Holzinger A. Evaluating the quality of machine learning explanations: A survey on methods and metrics. Electronics. 2021;10(5):593." href="/article/10.1007/s12559-024-10325-w#ref-CR13" id="ref-link-section-d156213074e1202">13</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Vilone G, Longo L. Notions of explainability and evaluation approaches for explainable artificial intelligence. Inf Fusion. 2021;76:89–106." href="/article/10.1007/s12559-024-10325-w#ref-CR59" id="ref-link-section-d156213074e1205">59</a>]. In contrast, the former is more functionality-oriented and objective, while the latter is more human-centred and subjective. To summarise, </p><ol class="u-list-style-none">
                <li>
                  <span class="u-custom-list-number">1.</span>
                  
                    <p><b>Objective Evaluation (OE)</b> contains objective metrics and automated approaches to evaluate XAI methods.</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">2.</span>
                  
                    <p><b>Human-Centred Evaluations (HCE)</b> encompass methods utilising a human-in-the-loop approach, where end-users are engaged, and their feedback or informed judgment is used.</p>
                  
                </li>
              </ol><p>Within this main partition, previous studies have yielded a plethora of metrics and evaluation frameworks tailored to assess the efficacy and quality of XAI explanations. However, the vast majority focus either on the objective evaluation or on the human-centred one. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56–67." href="/article/10.1007/s12559-024-10325-w#ref-CR10" id="ref-link-section-d156213074e1237">10</a>], On the other hand, the authors propose a framework that groups 34 metrics into 5 dimensions: (1) functional requirements, ensuring the method’s core capabilities are met; (2) operational requirements, detailing practical implementation needs; (3) usability criteria, evaluating user experience and effectiveness; (4) security and privacy considerations, identifying potential vulnerabilities; and (5) validation methods, confirming the method’s reliability through testing. These dimensions provide a comprehensive framework for evaluating and comparing explainability methods, ensuring thorough understanding and standardisation in the field. We selected this paper as a reference because the proposed framework offers a comprehensive yet synthetic comparison of capabilities and limitations of XAI methods that (1) covers both the objective and human-centred evaluation and (2), in addition to the evaluation, paves the way for framing the methods chosen through their characteristics.</p><p>Among the 5 dimensions proposed by [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56–67." href="/article/10.1007/s12559-024-10325-w#ref-CR10" id="ref-link-section-d156213074e1243">10</a>], four refer to objective evaluations and one to human-centred ones. Below, we describe the five dimensions, specifying if they belong to OE or HCE: </p><ol class="u-list-style-none">
                <li>
                  <span class="u-custom-list-number">1.</span>
                  
                    <p><b>Functional Requirements — OE</b> include the algorithmic requirements, e.g. the problem type (regression, classification, or clustering), the explanation scope (global, local, or cohort), the explainer’s computational complexity, etc.</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">2.</span>
                  
                    <p><b>Operational Requirements — OE</b> focus on the user and explainer interaction, e.g. the explanatory medium (summarisation, visualisation, etc.), the trade-off between performances and explainability, and the type of interaction with the system (static or dynamic).</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">3.</span>
                  
                    <p><b>Usability Requirements — OE</b> are objective metrics centred on the user’s perspective. They focus on making the explanation more natural and easily comprehensible. Some examples are the soundness, completeness, and interactiveness of the explanation.</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">4.</span>
                  
                    <p><b>Safety Requirements — OE</b> cover the impact of XAI systems on the robustness, security, and privacy aspects of the underlying predictive models.</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">5.</span>
                  
                    <p><b>Validation Requirements — HCE</b> encompass user studies and synthetic experiments. Because XAI aims to make algorithmic decisions more comprehensible to humans, their final efficacy needs to be evaluated by users.</p>
                  
                </li>
              </ol><p>The remainder of this section will assess the chosen XAI methods through objective and human-centred evaluations. Specifically, the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s12559-024-10325-w#Sec6">Objective Evaluation</a>” section is dedicated to objective evaluation. The metrics utilised will be examined in the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s12559-024-10325-w#Sec8">Objective Metrics</a>” section, while their implementation and experimental results will be shown in the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s12559-024-10325-w#Sec9">Objective Evaluation</a>” section. In the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s12559-024-10325-w#Sec10">Human Evaluation</a>” section, we will introduce the human-centred evaluation, defining its measures and practices in the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s12559-024-10325-w#Sec11">Evaluation Design and Experiments</a>” section and presenting its results in the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s12559-024-10325-w#Sec12">Human Evaluation Results</a>” section.</p><h3 class="c-article__sub-heading" id="Sec6">Objective Evaluation</h3><p>This section will delve into the four dimensions from [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56–67." href="/article/10.1007/s12559-024-10325-w#ref-CR10" id="ref-link-section-d156213074e1340">10</a>] that refer to OE. The authors present 34 criteria, of which 9 belong to <i>functional requirements</i> (denoted with codes from F1 to F9), 10 to <i>operational ones</i> (O1–O10), 11 to <i>usability</i> (U1–U11) and 4 to <i>safety requirements</i> (S1–S4).</p><p>Of these 34 criteria, some are characteristics or desiderata of the explainer, while others are evaluation metrics. For instance, F1, the problem supervision level, is a characteristic of the XAI method, expressing whether it works with unsupervised, supervised, or semisupervised ML algorithms. On the contrary, U1 is a metric that measures the Soundness of the XAI methods with respect to the prediction of the underlying ML model. In the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s12559-024-10325-w#Sec7">Characteristics of the Selected Methods</a>” section, we present all the characteristics of the selected XAI methods, while in the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s12559-024-10325-w#Sec8">Objective Metrics</a>” section, we present the metrics that are used to evaluate the selected XAI methods, presenting the results of their measurement in the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s12559-024-10325-w#Sec9">Objective Evaluation</a>” section. Both characteristics and metrics are taken from [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56–67." href="/article/10.1007/s12559-024-10325-w#ref-CR10" id="ref-link-section-d156213074e1367">10</a>] and abbreviated as in the paper, e.g. the Functional Requirement 1 — Problem Supervision Level, which is abbreviated as F1.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec7">Characteristics of the Selected Methods</h4><p>In this article, we have selected several methods for comparison based on specific characteristics. This section discusses the selected characteristics according to [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56–67." href="/article/10.1007/s12559-024-10325-w#ref-CR10" id="ref-link-section-d156213074e1377">10</a>] formalisation.</p>
                    <h3 class="c-article__sub-heading" id="FPar1">Functional Requirements</h3>
                    <p>In XAI, the vast majority of the literature is about <b>supervised learning</b> (F1 — Problem Supervision Level), in the context of <b>classification</b> (F2 — Problem Type) where explanations serve as a justification of <b>predictions</b> (F3 — Explanation Target) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56–67." href="/article/10.1007/s12559-024-10325-w#ref-CR10" id="ref-link-section-d156213074e1396">10</a>], and this will also be the approach of this research. Regarding the Explanations Scope (F4), we will consider explanations at all levels: <b>local, global, and cohort</b>. For the sake of comparability, and because of their greater adoption, we will test only <b>model agnostic</b> (F6 — Applicable Model Class), <b>post hoc</b> (F7 — Relation to the predictive system) explainers. Regarding the Compatible Feature Types (F8), in this article, we focus on <b>textual data</b>.</p>
                  
                    <h3 class="c-article__sub-heading" id="FPar2">Operational Requirements</h3>
                    <p>The family of explanations (O1) that we target is the <b>associations between antecedent and consequent</b>, while counterfactual and contrastive explanations are evaluated according to different methodologies, paradigms, and measures [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Keane MT, Kenny EM, Delaney E, Smyth B. If only we had better counterfactual explanations: Five key deficits to rectify in the evaluation of counterfactual XAI techniques. In: IJCAI. 2021. pp. 4467–74." href="/article/10.1007/s12559-024-10325-w#ref-CR60" id="ref-link-section-d156213074e1423">60</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Waa J, Nieuwburg E, Cremers A, Neerincx M. Evaluating XAI: a comparison of rule-based and example-based explanations. Artif Intell. 2021;291:103404." href="/article/10.1007/s12559-024-10325-w#ref-CR61" id="ref-link-section-d156213074e1426">61</a>]. Regarding the explanatory medium (O2) and the system interaction (O3), all the explainers tested present <b>statistic summarisation</b> and <b>static interaction</b>, respectively. Researchers have found that in current XAI methods, the presentation layer is usually distinctly delineated and less curated than the core algorithm [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Cambria E, Malandri L, Mercorio F, Mezzanzanica M, Nobani N. A survey on XAI and natural language explanations. Inf Process Manag. 2023;60(1):103111." href="/article/10.1007/s12559-024-10325-w#ref-CR6" id="ref-link-section-d156213074e1436">6</a>]. Some works use relevant word highlighting as a visualisation technique (e.g. in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Ribeiro MT, Singh S, Guestrin C. “Why should I trust you?” Explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2016. pp. 1135–44." href="/article/10.1007/s12559-024-10325-w#ref-CR22" id="ref-link-section-d156213074e1439">22</a>]), and the present work goes in that direction. The explanation domain (O4) in this work focuses on text classification. Considering the transparency of the data model (O5), we opt to concentrate on post hoc, model-agnostic XAI methods, enabling the utilisation of <b>any opaque underlying model</b>. Concerning the explanation audience (O6) and the purpose of the explanation (O7), the user study includes a <b>broad audience, both experts in XAI and non-experts</b>. The study encompasses <b>various functions, explained in the</b> “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s12559-024-10325-w#Sec11">Evaluation Design and Experiments</a>” section, such as, but not limited to, understandability, trust, and satisfaction. All the explanations provided by the considered methods are of <b>causal nature</b> (O8). The rest of the paper discusses the requirements of trust vs performance (O9) and provenance (O10).</p>
                  
                    <h3 class="c-article__sub-heading" id="FPar3">Usability Requirements</h3>
                    <p>This category includes five metrics, discussed in the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s12559-024-10325-w#Sec6">Objective Evaluation</a>” and “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s12559-024-10325-w#Sec9">Objective Evaluation</a>” sections, respectively: U1, U2, U3, U9, and U11. Moreover, since neither system provides interactive nor actionable outputs, as specified in Par.  <i>Operational Requirements</i>, neither U4 nor U5 is discussed. Chronology (U6), coherence (U7), novelty (U8), and personalisation (U10) assume previous knowledge and expectations regarding the output of the system and its interaction with the user, which in turn implicates a continuous use of the system over time, which is not this case.</p>
                  
                    <h3 class="c-article__sub-heading" id="FPar4">Safety Requirements</h3>
                    <p>Of the explanation requirements, S3 will be discussed and measured in the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s12559-024-10325-w#Sec6">Objective Evaluation</a>” and “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s12559-024-10325-w#Sec9">Objective Evaluation</a>” sections, respectively. The other safety requirements (S1, S2, and S4) are very specific to the application domain where XAI is used. Therefore, they are not part of the scope of this paper.</p>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec8">Objective Metrics</h4><p>Below, we present the metrics used for the objective evaluation. The metrics used differ depending on the type of explanation, e.g. local vs. global explanations and rules vs. feature-returning explanations. However, all metrics adopted are viable for the classification task.</p>
                    <h3 class="c-article__sub-heading" id="FPar5">Computational Complexity (F5) and Caveats (F9)</h3>
                    <p>The choice of an XAI method should consider its time, memory, and computational complexity constraints. In text classification, the number of features is typically very high. Not all the XAI methods presented scale well beyond certain amount of features regarding computational times and memory requirements. In the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s12559-024-10325-w#Sec9">Objective Evaluation</a>” section, we will present the results of four different runs, in which we consider respectively the 10, 100, 1000 and 10,000 most common features of the corpus. In this way, we can identify the explainers’ computational limits. From the perspective of memory, we consider only algorithms that do not exceed the 64GB of RAM requirement, which is the RAM size on which we are conducting experiments, while from the perspective of computational times, an algorithm will be considered intractable if it takes longer than one day for global explainers and more than 3 h for a single explanation of local ones.</p>
                  
                    <h3 class="c-article__sub-heading" id="FPar6">Explanation Fidelity Measures: Soundness (U1) and Completeness (U2)</h3>
                    <p>Those two dimensions measure how well the explainer agrees with the underlying model developed by the classifier. <b>Soundness</b> is usually measured for global surrogate models through <i>fidelity</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Burkart N, Huber MF. A survey on the explainability of supervised machine learning. J Artif Intell Res. 2021;70:245–317." href="/article/10.1007/s12559-024-10325-w#ref-CR11" id="ref-link-section-d156213074e1523">11</a>], i.e. the concordance <i>S</i> of the predictions of the XAI method <i>w</i>; taken from a set of possible white box models <i>I</i> approximated on the training data <span class="mathjax-tex">\(X = \{x_{i}, \dots , x_{n}\}\)</span>; with the predictions of the underlying black box one <i>b</i>, as in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s12559-024-10325-w#Equ1">1</a>).</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \arg \max _{w \in I}{\frac{1}{|X|}\sum _{x \in X}{S(w(x),b(x))}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>On the other hand, <b>completeness</b> assesses the extent to which an explanation can generalise. It can be evaluated by verifying the accuracy of an explanation across comparable data points (individuals) within various groups across a dataset [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56–67." href="/article/10.1007/s12559-024-10325-w#ref-CR10" id="ref-link-section-d156213074e1712">10</a>]. For rule-based explainers, it can be measured with their <i>correctness</i>, i.e. the number of correctly predicted instances explained by the output rules <i>r</i> over total instances <i>X</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Vilone G, Longo L. A quantitative evaluation of global, rule-based explanations of post-hoc, model agnostic methods. Front Artif Intell. 2021;4:717899." href="/article/10.1007/s12559-024-10325-w#ref-CR50" id="ref-link-section-d156213074e1724">50</a>], following Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s12559-024-10325-w#Equ2">2</a>).</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \dfrac{r}{X} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>For feature-based ones, we refer to the measure of <i>faithfulness</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Yeh C-K, Hsieh C-Y, Suggala A, Inouye DI, Ravikumar PK. On the (in) fidelity and sensitivity of explanations. Adv Neural Inf Process Syst. 2019;32." href="/article/10.1007/s12559-024-10325-w#ref-CR62" id="ref-link-section-d156213074e1773">62</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Bhatt U, Weller A, Moura JM. Evaluating and aggregating feature-based model explanations. &#xA;                  arXiv:2005.00631&#xA;                  &#xA;                 [Preprint]. 2020. Available from: &#xA;                  http://arxiv.org/abs/2005.00631&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR63" id="ref-link-section-d156213074e1776">63</a>] shown in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s12559-024-10325-w#Equ3">3</a>). For an explainer to be faithful, the important features of the model should correspond to the important ones of the explainer. It is measured by perturbing the explainer’s features. For an explainer to be faithful, given a subset size |<i>S</i>|, the change in the predictor <i>b</i>’s output between the perturbed explanation and the unchanged one should be proportional to the sum of attribution scores. The proportionality is computed using Pearson correlation.</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \text {corr}_{S \subseteq \scriptstyle \begin{pmatrix} \scriptstyle |d|\\ \scriptstyle |S|\end{pmatrix}} \left( \sum _{i \in S} w(x)_i, b(x) - b(x_{x_s = x_i})\right) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                  
                    <h3 class="c-article__sub-heading" id="FPar7">Contextfulness (U3)</h3>
                    <p>It is important to frame the single explanation in a context for cohort explanations. The context can be used in several ways, e.g. to check for safe generalisation. This measure applies only to rule-based explanations. Each instance is classified by a rule to which a class is associated. To measure the contexfulness, we select the widely known rule <b>coverage</b> metric [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Vilone G, Rizzo L, Longo L. A comparative analysis of rule-based, model-agnostic methods for explainable artificial intelligence. 2020." href="/article/10.1007/s12559-024-10325-w#ref-CR49" id="ref-link-section-d156213074e1985">49</a>] shown in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s12559-024-10325-w#Equ4">4</a>), computing the ratio of covered input instances <i>c</i> over total input instances <i>X</i>.</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \frac{c}{X} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div>
                  
                    <h3 class="c-article__sub-heading" id="FPar8">Parsimony (U11)</h3>
                    <p>Explanations should be selective and concise to prevent users from being overwhelmed with unnecessary details. In other words, parsimonious methods should aim to address the most significant (explanation) gaps using the fewest arguments possible. For rule-based explainers, the <b>selectiveness</b> of rules is measured through their features fraction overlap [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Vilone G, Longo L. Notions of explainability and evaluation approaches for explainable artificial intelligence. Inf Fusion. 2021;76:89–106." href="/article/10.1007/s12559-024-10325-w#ref-CR59" id="ref-link-section-d156213074e2045">59</a>], the degree of overlap between every pair of rules <span class="mathjax-tex">\(r_{i}, r_{k}\)</span> in a ruleset <i>R</i>, according to Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s12559-024-10325-w#Equ5">5</a>). <b>Conciseness</b> is measured by the ruleset’s cardinality |<i>R</i>| and the rules’ average length.</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \frac{2}{R(R-1)} \sum _{i,j: i \le j} \frac{\text {overlap}(r_i, r_j)}{X} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>For feature-based explainers, a complex explanation is one in which all the features have equal attribution, while the simplest explanation would be concentrated on one feature [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56–67." href="/article/10.1007/s12559-024-10325-w#ref-CR10" id="ref-link-section-d156213074e2206">10</a>]. Consequently, in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Bhatt U, Weller A, Moura JM. Evaluating and aggregating feature-based model explanations. &#xA;                  arXiv:2005.00631&#xA;                  &#xA;                 [Preprint]. 2020. Available from: &#xA;                  http://arxiv.org/abs/2005.00631&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR63" id="ref-link-section-d156213074e2209">63</a>], the authors measure the complexity of an explanation as the entropy of its features attributions. Using their metric, in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s12559-024-10325-w#Equ6">6</a>) we measure the parsimony of feature base explainers as 1 minus the complexity, where <span class="mathjax-tex">\(P_w(i)\)</span> is the fractional contribution of feature <span class="mathjax-tex">\(x_{i}\)</span> to the total magnitude of the attribution.</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} 1 - \sum _{i=1}^d P_w(i) \ln (P_w(i)) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div>
                  
                    <h3 class="c-article__sub-heading" id="FPar9">Explanation Invariance (S3)</h3>
                    <p>The ideal explainer should represent the underlying model and its changes in behaviour without introducing variability of their own. For this reason, explanations must be:</p>
                    <p><b>Consistent</b>, i.e. given a fixed ML model, explanations of similar data points should be similar. If we define the sensitivity [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Yeh C-K, Hsieh C-Y, Suggala A, Inouye DI, Ravikumar PK. On the (in) fidelity and sensitivity of explanations. Adv Neural Inf Process Syst. 2019;32." href="/article/10.1007/s12559-024-10325-w#ref-CR62" id="ref-link-section-d156213074e2386">62</a>] as the variation of the features/rules of the explanation function concerning a change in the input, we can measure the consistency as 1-sensitivity. Given the black box model <i>b</i>, the explanation function <i>w</i>, the distance metric <i>D</i>, an instance <i>x</i> and its perturbation <i>z</i>, we follow Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s12559-024-10325-w#Equ7">7</a>) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Bhatt U, Weller A, Moura JM. Evaluating and aggregating feature-based model explanations. &#xA;                  arXiv:2005.00631&#xA;                  &#xA;                 [Preprint]. 2020. Available from: &#xA;                  http://arxiv.org/abs/2005.00631&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR63" id="ref-link-section-d156213074e2408">63</a>].</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} 1 - \max D(w(b, x), w(b, z)) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>In the case of textual data, variations are usually of small magnitude and typically do not alter the sentence’s meaning, such as introducing typos and substituting some words with synonyms. The changes were made using the nlpaug library [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Ma E. NLP Augmentation. 2019. &#xA;                  https://github.com/makcedward/nlpaug&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR64" id="ref-link-section-d156213074e2491">64</a>]. <b>Stable</b>, i.e. different runs of the same XAI method, should provide the same output. Like consistency, stability will be measured using a binary variable equal to 1 if the rules generated after a rerun with a different randomisation seed are the same.</p>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec9">Objective Evaluation</h4>
                    <h3 class="c-article__sub-heading" id="FPar10">Dataset and Preprocessing</h3>
                    <p>The experiments were conducted on the International Movie Reviews dataset (IMDB) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="Maas A, Daly RE, Pham PT, Huang D, Ng AY, Potts C. Learning word vectors for sentiment analysis. In: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. 2011. pp. 142–50." href="/article/10.1007/s12559-024-10325-w#ref-CR65" id="ref-link-section-d156213074e2511">65</a>]. The dataset contains 50,000 movie reviews collected from IMDB with relative ratings and consists of an even number of positive and negative reviews. The IMDB dataset is widely known and frequently used in research, [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Kumar H, Harish B, Darshan H. Sentiment analysis on imdb movie reviews using hybrid feature extraction method. Int J Interact Multimed Artif Intell. 2019;5(5)." href="/article/10.1007/s12559-024-10325-w#ref-CR66" id="ref-link-section-d156213074e2514">66</a>]. It is a large dataset containing several reviews on diverse movies and is well suited for text classification.</p>
                    <p>Following previous literature, the authors considered a negative review with a score <span class="mathjax-tex">\(\le \)</span> 4 out of 10 and a positive review with a score <span class="mathjax-tex">\(\ge \)</span> 7 out of 10. Therefore, only highly polarised reviews are considered. For comparability, the preprocessing has been kept as simple as possible. The dataset undergoes standard preprocessing transformations for feature normalisation and noise reduction: converting all the words to lowercase and stopwords [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Bird S, Klein E, Loper E. Natural Language Processing with Python: Analyzing text with the natural language toolkit. 2009." href="/article/10.1007/s12559-024-10325-w#ref-CR67" id="ref-link-section-d156213074e2556">67</a>] and punctuation removal.</p>
                  
                    <h3 class="c-article__sub-heading" id="FPar11">Training of the Underlying Classification Model</h3>
                    <p>As in the case of preprocessing, the process was made as simple and repeatable as possible. Three classic yet powerful ML models were chosen: Random Forest Classifier (RF), Gradient Boosting Classifier (GB), and Support Vector Classifier (SVC) with a linear kernel. Several factors drove the selection.</p><ul class="u-list-style-bullet">
                      <li>
                        <p>The paper results can be easily reproduced not using special hardware (e.g. GPU)</p>
                      </li>
                      <li>
                        <p>Although Deep Learning models perform better on text classification w.r.t. Non-Deep Learning models [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Li Q, Peng H, Li J, Xia C, Yang R, Sun L, Yu PS, He L. A survey on text classification: From traditional to deep learning. ACM Trans Intell Syst Technol. 2022;13(2). &#xA;                  https://doi.org/10.1145/3495162&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR68" id="ref-link-section-d156213074e2579">68</a>], the difference is not so big and other factors might have a more significant impact, e.g. according to [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Siino M, Tinnirello I, La Cascia M. Is text preprocessing still worth the time? A comparative survey on the influence of popular preprocessing methods on transformers and traditional classifiers. Inf Syst. 2024;121:102342. &#xA;                  https://doi.org/10.1016/j.is.2023.102342&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR69" id="ref-link-section-d156213074e2582">69</a>] text preprocessing (e.g. slang and abbreviation replacement, repeated punctuation removal, ...) and simple classification methods can achieve state-of-the-art results, sometimes outperforming complex and recent pre-trained architectures (i.e. Transformer-based models)</p>
                      </li>
                      <li>
                        <p>According to [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Minaee S, Kalchbrenner N, Cambria E, Nikzad N, Chenaghlu M, Gao J. Deep learning-based text classification: A comprehensive review. ACM Comput Surv. 2021;54(3). &#xA;                  https://doi.org/10.1145/3439726&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR70" id="ref-link-section-d156213074e2591">70</a>], developing a text classifier is a trial-and-error process. Therefore, Non-Deep Learning models can be an effective solution in the early stages of the process thanks to the reduced training time and the low computational effort required</p>
                      </li>
                    </ul><p>The chosen models (i.e. RF, GB, and SVC) are popular algorithms in text classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Li Q, Peng H, Li J, Xia C, Yang R, Sun L, Yu PS, He L. A survey on text classification: From traditional to deep learning. ACM Trans Intell Syst Technol. 2022;13(2). &#xA;                  https://doi.org/10.1145/3495162&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR68" id="ref-link-section-d156213074e2597">68</a>]. The models used are implemented using scikit-learn [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V, Vanderplas J, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay E. Scikit-learn: Machine learning in Python. J Mach Learn Res. 2011;12:2825–30." href="/article/10.1007/s12559-024-10325-w#ref-CR71" id="ref-link-section-d156213074e2600">71</a>] with default parameters, splitting the IMDB dataset into 80% training instances and 20% (10,000) testing instances. The dataset is transformed via Bag Of Word (BOW), and only the 10,000 most common features are kept to have a less sparse matrix devoid of spurious features. The classification results are reported in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s12559-024-10325-w#Tab2">2</a>. Given its highest Accuracy and F1 values, we will test the XAI methods using the predictions and model weights of the SVC classifier.</p>
                  <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Performance of classifiers</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s12559-024-10325-w/tables/2" aria-label="Full size table 2"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div>
                    <h3 class="c-article__sub-heading" id="FPar12">Objective Evaluation Results</h3>
                    <p>In Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s12559-024-10325-w#Tab3">3</a>, we present the results of the experiments for the rule-based explainers while the feature-based ones are presented in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s12559-024-10325-w#Tab4">4</a>. The execution time is reported in average seconds. Following [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Bhatt U, Weller A, Moura JM. Evaluating and aggregating feature-based model explanations. &#xA;                  arXiv:2005.00631&#xA;                  &#xA;                 [Preprint]. 2020. Available from: &#xA;                  http://arxiv.org/abs/2005.00631&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR63" id="ref-link-section-d156213074e2765">63</a>], we computed all the measures as the average on a sample of 50 instances for the cohort and the local explainers. In the experimentation phase, each dimension was evaluated across varying BOW feature counts: 10, 100, 1000, and 10000. However, BRCG and QII measurements were unattainable for 10,000 features due to excessively long computational times, and SAGE for 1000 and 10,000 features. Similarly, the required RAM exceeded 64GB for Shap, rendering the evaluation unfeasible. Another caveat concerns the fidelity of SAGE, which cannot be computed since it does not implement predictive functions but returns features’ importance based on how much predictive power they contribute.</p>
                  <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Objective evaluation results for global and cohort rule-based explainers</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s12559-024-10325-w/tables/3" aria-label="Full size table 3"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Objective evaluation results for global and local feature-based explainers</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s12559-024-10325-w/tables/4" aria-label="Full size table 4"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>The two tables show high consistency and sensitivity for rule-and feature-based explainers, except (as expected) for the random-generated ones. This confirms two facts: (1) explainers do not create variability in the results, and (2) random explainers constitute an effective control method. This is confirmed by the accuracy values, which consistently linger around 0.5 for rule- and feature-based methods.</p><p>The DT surrogate exhibits the highest soundness among the global rule-based explainers. The number of rules generated by DT is a parameter chosen by design, and we used the default one of 20. The BRCG instead uses a smaller number of rules. Moreover, BRCG uses shorter rules with lower fidelity values but greater coverage. For instance, with 1000 features, the DT has a fidelity of 0.76 and coverage of 48% of the dataset with an average rule length of 8.33, while the BRCG has a fidelity of 0.62 on 76% of the dataset with an average rule length of 1.5 words. The execution time of the DT is lower, with the BRCG explainer being even intractable for 10,000 features. Anchors also exhibit short execution times, typically 1 to 5 s, even when dealing with 10,000 features. Being a cohort explainer, we measure the average rule coverage of a sample of 50 instances instead of the coverage on the whole dataset. Increasing the number of features, the coverage decreases while the average rule correctness on 50 rules increases. A potential interpretation of this could be that, by using more features, Anchors generate more precise rules with less common words, which consequently have lower coverage over the data.</p><p>In Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s12559-024-10325-w#Tab4">4</a> we can observe the results for feature-based explainers. In this case, the fidelity of the surrogate logistic regression is close to one, whatever the number of features, and is higher than that of the Naive Bayes classifier. However, the entropy of the weight distributions of the explanation features in Naive Bayes is consistently less than 1%, rendering it simpler and more interpretable compared to logistic regression, albeit with lower fidelity. Local explainers exhibit high faithfulness only when employing many features, which is intuitive given their local nature. Indeed, despite their frequency, it is challenging for a small subset of features to manifest within the selected evaluation phrases consistently. As discussed above, due to time and memory issues, Shap becomes intractable after 1000 features. Usually, text classification involves more than this number of features.</p><h3 class="c-article__sub-heading" id="Sec10">Human Evaluation</h3><p>The FOUSV (Functional, Operational, Usability, Safety, Validation) evaluation framework used above and summarised by Sokol and Flach [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56–67." href="/article/10.1007/s12559-024-10325-w#ref-CR10" id="ref-link-section-d156213074e5303">10</a>] is based on requirements that can be quantitatively formulated, therefore, this framework does not fit user-centred metrics. Another recently proposed user-centred human evaluation framework [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Mohseni S, Zarei N, Ragan ED. A multidisciplinary survey and framework for design and evaluation of explainable ai systems. ACM Trans Interact Intell Syst. 2021;11(3–4):1–45. &#xA;                  https://doi.org/10.1145/3387166&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR12" id="ref-link-section-d156213074e5306">12</a>] considers the “user types” and “design goals” (i.e. AI Novices, Data Experts, or AI Experts will use the explanations with different purposes, hence having different requirements). According to this 5Ms framework, there are also five dimensions to measure for human evaluation:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>M1: Mental Model</p>
                  </li>
                  <li>
                    <p>M2: Usefulness and Satisfaction</p>
                  </li>
                  <li>
                    <p>M3: User Trust and Reliance</p>
                  </li>
                  <li>
                    <p>M4: Human-AI Task Performance</p>
                  </li>
                  <li>
                    <p>M5: Computational Measures</p>
                  </li>
                </ul><p>The mental model (M1) evaluates how helpful the explanations are in conceptualising and understanding the mechanism of the target ML model. Therefore, they are very context-specific and difficult to compare across methods. The M2 and M3 dimensions are usually self-reported, and previous studies have used interviews, questionnaires, and case studies as subjective measures [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Mohseni S, Zarei N, Ragan ED. A multidisciplinary survey and framework for design and evaluation of explainable ai systems. ACM Trans Interact Intell Syst. 2021;11(3–4):1–45. &#xA;                  https://doi.org/10.1145/3387166&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR12" id="ref-link-section-d156213074e5342">12</a>]. M4 has overlaps with functionality metrics. M5 is less commonly implemented according to a survey covering 42 recent papers [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Mohseni S, Zarei N, Ragan ED. A multidisciplinary survey and framework for design and evaluation of explainable ai systems. ACM Trans Interact Intell Syst. 2021;11(3–4):1–45. &#xA;                  https://doi.org/10.1145/3387166&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR12" id="ref-link-section-d156213074e5345">12</a>] and overlaps a lot with objective metrics. Therefore, we concluded that only M2 and M3 are relevant in our study. In the context of text classification, the human evaluation will measure (1) Usefulness, (2) Satisfaction, and (3) Trustworthiness. We dropped “Reliance” because those text classification systems usually achieve high accuracy, hence they are considered very reliable already.</p><p>A reliable metric for Usefulness is the response time. In the translation application example, one complaint is that the explanations for recommended translations extend the time needed for the translator to complete the task. Given that processing the explanations always takes time, useful explanations only minimally increase or reduce the response time to the same task. So, the Usefulness question is designed to ask the user to classify a piece of text into given categories, with/without a hint (system-generated explanations).</p><p>To be consistent with previous studies, Satisfaction and Trustworthiness are measured by self-report:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>Rate from scale 1(worst)-5(best), how satisfied the user is with a given explanation?</p>
                  </li>
                  <li>
                    <p>Compare explanations for the given classification task and choose the more trustworthy one that leads to the classification outcome.</p>
                  </li>
                </ul><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec11">Evaluation Design and Experiments</h4><p>To cover the different XAI methods with a resource constraint, we sample four representative methods, i.e. LIME for local features [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Ribeiro MT, Singh S, Guestrin C. “Why should I trust you?” Explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2016. pp. 1135–44." href="/article/10.1007/s12559-024-10325-w#ref-CR22" id="ref-link-section-d156213074e5375">22</a>], Logistic Regression for global features,<sup><a href="#Fn11"><span class="u-visually-hidden">Footnote </span>11</a></sup> Decision Tree for global rules [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="Breiman L. Classification and regression trees. 2017." href="/article/10.1007/s12559-024-10325-w#ref-CR72" id="ref-link-section-d156213074e5389">72</a>], and Anchors [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Ribeiro MT, Singh S, Guestrin C. Anchors: High-precision model-agnostic explanations. In: Proceedings of the AAAI Conference on Artificial Intelligence (vol. 32). 2018." href="/article/10.1007/s12559-024-10325-w#ref-CR42" id="ref-link-section-d156213074e5392">42</a>] for cohort rules. Each user is presented with 8 questions randomly selected from the explanation no-explanation pairs and 2 additional questions for satisfaction and trustworthiness. This ensures the interdependence of response times and ratings for the same text samples. An example of the survey questions can be found in the <a data-track="click" data-track-label="link" data-track-action="appendix anchor" href="/article/10.1007/s12559-024-10325-w#App1">Appendix</a>.</p><p>We recruited 42 users in two batches from Prolific<sup><a href="#Fn12"><span class="u-visually-hidden">Footnote </span>12</a></sup> using the LimeSurvey<sup><a href="#Fn13"><span class="u-visually-hidden">Footnote </span>13</a></sup> online tool. The participants (paid approx. £ 9 per hour and took 13 min to finish the survey in average) are filtered using two criteria: fluent in English, and have completed secondary education or above.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s12559-024-10325-w#Fig2">2</a> provides more information on the participants’ demographics. It can be observed that the sample is diverse across gender, ethnicity, and country. Because the result distributions, e.g. in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s12559-024-10325-w#Fig3">3</a> are continuous, we believe our findings have good generalisability across these demographic features. Further, we noticed that the participants are skewed to young people under 40: that is probably also the demographics of online gig workers. The geographic distribution also mainly covers Eurafrica and overlooks Asia, probably due to the language criterion. Though we believe these potential biases are unlikely to have significant impacts on user perception of explanations, it may be more confident to limit our user study findings to relatively young English-speaking people.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Fig. 2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s12559-024-10325-w/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs12559-024-10325-w/MediaObjects/12559_2024_10325_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs12559-024-10325-w/MediaObjects/12559_2024_10325_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="421"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Demographic information of the user study participants</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s12559-024-10325-w/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="Fig. 3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s12559-024-10325-w/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs12559-024-10325-w/MediaObjects/12559_2024_10325_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs12559-024-10325-w/MediaObjects/12559_2024_10325_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="521"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Average response time (seconds) with/without the explanations</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s12559-024-10325-w/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec12">Human Evaluation Results</h4><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s12559-024-10325-w#Fig3">3</a> shows the violin plot [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="Hintze JL, Nelson RD. Violin plots: a box plot-density trace synergism. Am Stat. 1998;52(2):181–4. &#xA;                  https://doi.org/10.1080/00031305.1998.10480559&#xA;                  &#xA;                ." href="/article/10.1007/s12559-024-10325-w#ref-CR73" id="ref-link-section-d156213074e5478">73</a>] of users’ response time with/without different explanations. An interesting observation across all explanations is that their presence makes the response time distributions more concentrated. The feature-based explanations (LIME and LR) generally increase the quantiles of response times. Local features seem to be more useful with the 3-quantile decreased. A median decrease is also observed for LIME before averaging. The quantile decreases for rule-based explanations are more pronounced. We conclude that Anchors produce the most useful explanations among the evaluated methods. The general trends are that rule-based explanations are more useful than feature-based ones, and localising explanations increases their usefulness.</p><p>Regarding satisfaction ratings, users are mostly satisfied with the LR (s = 3.04) and DT (s = 2.96) explanations. At the same time, they are less satisfied with the explanations for the Anchors (s = 2.62) and LIME (s = 2.55), although the explanations for the Anchors were the most useful. Users seem to be more satisfied with global explanations than local ones. This may be because global explanations are usually well-aligned with common sense, while local ones, despite their discriminative power, feel unnatural with less frequent features/rules.</p><p>The trustworthiness choices reveal that out of 39 valid responses, the LR explanation is the most trustworthy (n = 27) and is significantly better than DT (n = 9), LIME (n = 2) and Anchors (n = 1) explanations. Further investigation into the user responses suggests the importance of “semantic correctness” of the explanations for trustworthiness. Even those who chose DT mentioned that the words are “really related to the movie” or the narrative “sounds like a review”. On this factor, global explanations usually do a better job of discovering semantically relevant words.</p><p>The human evaluation shows that the results can be quite diverse for different XAI methods, even for subjective metrics like Usefulness, Satisfaction, and Trustworthiness. The rule-format and localised explanations are useful for assisting human text classification tasks, while the global explanations are more satisfying and trustworthy.</p></div></div></section><section data-title="Discussion"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Discussion</h2><div class="c-article-section__content" id="Sec13-content"><p>The objective evaluation of XAI methods highlights nuanced characteristics across different global explainers, as depicted in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s12559-024-10325-w#Tab3">3</a>. At a global level, some methods consistently demonstrate high fidelity, irrespective of the number of features. On the other hand, their simplicity and interpretability are underscored by the consistently low entropy of its weight distributions. On the other hand, local explainers exhibit heightened faithfulness primarily with a substantial feature set, aligning with their inherent nature of capturing localised patterns. Notably, the computational constraints become apparent with several methods, rendering it intractable with many features, a common occurrence in text classification scenarios where feature counts tend to be high.</p><p>Transitioning to the human evaluation, the user study sheds light on complementary insights. Anchors emerge as the most useful explanation method, with rule-based approaches generally deemed more valuable than feature-based ones, particularly in enhancing response time distributions. Users express higher satisfaction with global explanations, potentially attributed to their alignment with common understanding. In contrast, despite their discriminative power, local explanations are perceived as less natural due to incorporating less frequent features or rules. The trustworthiness ratings underscore the significance of semantic correctness in explanations, with global explanations often excelling in uncovering semantically relevant words.</p><p>In synthesis, while the objective evaluation provides crucial insights into the technical capabilities of XAI methods, the human evaluation underscores the subjective nuances in user perception, emphasising the importance of considering technical efficacy and user satisfaction in selecting the appropriate explainers. The absence of a singular silver bullet in XAI necessitates a nuanced approach, where the choice of explainer should be tailored to the specific use case and user requirements.</p></div></div></section><section data-title="Conclusions and Future Contributions"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Conclusions and Future Contributions</h2><div class="c-article-section__content" id="Sec14-content"><p>In this study, we conducted a comprehensive survey of XAI methods. Then, we evaluated different levels of granularity, i.e. their scope and output. In the evaluation, we employed both objective metrics and user evaluations and discussed the results in depth. Our findings underscore the complexity of choosing an appropriate XAI method, as there is no one-size-fits-all solution. Instead, the selection process should be approached on a case-by-case basis, considering the specific context and requirements of the task at hand. By thoroughly analysing the strengths and limitations of different explainers, this paper is a valuable resource for users navigating the landscape of XAI methods, aiding their decision-making process and fostering a deeper understanding of the trade-offs involved. Future research will explore papers focusing on XAI for embeddings, which also hold a particular significance in text classification.</p></div></div></section>
                                </div>
                        
                    

                    
                        
                    

                    
                        
                    

                    
                    <section data-title="Data Availability"><div class="c-article-section" id="data-availability-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="data-availability">Data Availability</h2><div class="c-article-section__content" id="data-availability-content">
              
              <p>The datasets generated and analysed during the current study and the code used to perform the evaluation will be available in a public GitHub (<a href="https://github.com/Crisp-Unimib/XAI_Benchmark">https://github.com/Crisp-Unimib/XAI_Benchmark</a>) repository, and the URL will be provided upon acceptance.</p>
            </div></div></section><section data-title="Notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1" data-counter="1."><div class="c-article-footnote--listed__content"><p><a href="https://github.com/Crisp-Unimib/XAI_Benchmark">https://github.com/Crisp-Unimib/XAI_Benchmark</a></p></div></li><li class="c-article-footnote--listed__item" id="Fn2" data-counter="2."><div class="c-article-footnote--listed__content"><p><a href="https://github.com/marcotcr/lime">https://github.com/marcotcr/lime</a></p></div></li><li class="c-article-footnote--listed__item" id="Fn3" data-counter="3."><div class="c-article-footnote--listed__content"><p><a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a></p></div></li><li class="c-article-footnote--listed__item" id="Fn4" data-counter="4."><div class="c-article-footnote--listed__content"><p><a href="https://github.com/iancovert/sage">https://github.com/iancovert/sage</a></p></div></li><li class="c-article-footnote--listed__item" id="Fn5" data-counter="5."><div class="c-article-footnote--listed__content"><p><a href="https://github.com/IBM/AIX360">https://github.com/IBM/AIX360</a></p></div></li><li class="c-article-footnote--listed__item" id="Fn6" data-counter="6."><div class="c-article-footnote--listed__content"><p><a href="https://github.com/marcotcr/anchor">https://github.com/marcotcr/anchor</a></p></div></li><li class="c-article-footnote--listed__item" id="Fn7" data-counter="7."><div class="c-article-footnote--listed__content"><p><a href="https://github.com/hovinh/QII">https://github.com/hovinh/QII</a></p></div></li><li class="c-article-footnote--listed__item" id="Fn8" data-counter="8."><div class="c-article-footnote--listed__content"><p><a href="https://scikit-learn.org/stable/modules/tree.html">https://scikit-learn.org/stable/modules/tree.html</a></p></div></li><li class="c-article-footnote--listed__item" id="Fn9" data-counter="9."><div class="c-article-footnote--listed__content"><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a></p></div></li><li class="c-article-footnote--listed__item" id="Fn10" data-counter="10."><div class="c-article-footnote--listed__content"><p><a href="https://scikit-learn.org/stable/modules/naive_bayes.html">https://scikit-learn.org/stable/modules/naive_bayes.html</a></p></div></li><li class="c-article-footnote--listed__item" id="Fn11" data-counter="11."><div class="c-article-footnote--listed__content"><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a></p></div></li><li class="c-article-footnote--listed__item" id="Fn12" data-counter="12."><div class="c-article-footnote--listed__content"><p><a href="https://app.prolific.com/">https://app.prolific.com/</a></p></div></li><li class="c-article-footnote--listed__item" id="Fn13" data-counter="13."><div class="c-article-footnote--listed__content"><p><a href="https://www.limesurvey.org/">https://www.limesurvey.org/</a></p></div></li></ol></div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references" data-track-component="outbound reference" data-track-context="references section"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1."><p class="c-article-references__text" id="ref-CR1">Gunning D, Stefik M, Choi J, Miller T, Stumpf S, Yang G-Z. XAI—explainable artificial intelligence. Sci Robot. 2019;4(37):7120.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1126/scirobotics.aay7120" data-track-item_id="10.1126/scirobotics.aay7120" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1126%2Fscirobotics.aay7120" aria-label="Article reference 1" data-doi="10.1126/scirobotics.aay7120">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=XAI%E2%80%94explainable%20artificial%20intelligence&amp;journal=Sci%20Robot&amp;doi=10.1126%2Fscirobotics.aay7120&amp;volume=4&amp;issue=37&amp;publication_year=2019&amp;author=Gunning%2CD&amp;author=Stefik%2CM&amp;author=Choi%2CJ&amp;author=Miller%2CT&amp;author=Stumpf%2CS&amp;author=Yang%2CG-Z">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2."><p class="c-article-references__text" id="ref-CR2">Gozzi N, Malandri L, Mercorio F, Pedrocchi A. XAI for myo-controlled prosthesis: Explaining EMG data for hand gesture classification. Knowl Based Syst. 2022;240:108053.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.knosys.2021.108053" data-track-item_id="10.1016/j.knosys.2021.108053" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.knosys.2021.108053" aria-label="Article reference 2" data-doi="10.1016/j.knosys.2021.108053">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=XAI%20for%20myo-controlled%20prosthesis%3A%20Explaining%20EMG%20data%20for%20hand%20gesture%20classification&amp;journal=Knowl%20Based%20Syst&amp;doi=10.1016%2Fj.knosys.2021.108053&amp;volume=240&amp;publication_year=2022&amp;author=Gozzi%2CN&amp;author=Malandri%2CL&amp;author=Mercorio%2CF&amp;author=Pedrocchi%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3."><p class="c-article-references__text" id="ref-CR3">Xing F, Malandri L, Zhang Y, Cambria E. Financial sentiment analysis: An investigation into common mistakes and silver bullets. In: Proceedings of the 28th International Conference on Computational Linguistics. 2020. pp. 978–87.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4."><p class="c-article-references__text" id="ref-CR4">Hassija V, Chamola V, Mahapatra A, Singal A, Goel D, Huang K, Scardapane S, Spinelli I, Mahmud M, Hussain A. Interpreting black-box models: a review on explainable artificial intelligence. Cogn Comput. 2024;16(1):45–74.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s12559-023-10179-8" data-track-item_id="10.1007/s12559-023-10179-8" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s12559-023-10179-8" aria-label="Article reference 4" data-doi="10.1007/s12559-023-10179-8">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Interpreting%20black-box%20models%3A%20a%20review%20on%20explainable%20artificial%20intelligence&amp;journal=Cogn%20Comput&amp;doi=10.1007%2Fs12559-023-10179-8&amp;volume=16&amp;issue=1&amp;pages=45-74&amp;publication_year=2024&amp;author=Hassija%2CV&amp;author=Chamola%2CV&amp;author=Mahapatra%2CA&amp;author=Singal%2CA&amp;author=Goel%2CD&amp;author=Huang%2CK&amp;author=Scardapane%2CS&amp;author=Spinelli%2CI&amp;author=Mahmud%2CM&amp;author=Hussain%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5."><p class="c-article-references__text" id="ref-CR5">Malandri L, Mercorio F, Mezzanzanica M, Seveso A. Model-contrastive explanations through symbolic reasoning. Decis Support Syst. 2024;176:114040.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.dss.2023.114040" data-track-item_id="10.1016/j.dss.2023.114040" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.dss.2023.114040" aria-label="Article reference 5" data-doi="10.1016/j.dss.2023.114040">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=Model-contrastive%20explanations%20through%20symbolic%20reasoning&amp;journal=Decis%20Support%20Syst&amp;doi=10.1016%2Fj.dss.2023.114040&amp;volume=176&amp;publication_year=2024&amp;author=Malandri%2CL&amp;author=Mercorio%2CF&amp;author=Mezzanzanica%2CM&amp;author=Seveso%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6."><p class="c-article-references__text" id="ref-CR6">Cambria E, Malandri L, Mercorio F, Mezzanzanica M, Nobani N. A survey on XAI and natural language explanations. Inf Process Manag. 2023;60(1):103111.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.ipm.2022.103111" data-track-item_id="10.1016/j.ipm.2022.103111" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.ipm.2022.103111" aria-label="Article reference 6" data-doi="10.1016/j.ipm.2022.103111">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20on%20XAI%20and%20natural%20language%20explanations&amp;journal=Inf%20Process%20Manag&amp;doi=10.1016%2Fj.ipm.2022.103111&amp;volume=60&amp;issue=1&amp;publication_year=2023&amp;author=Cambria%2CE&amp;author=Malandri%2CL&amp;author=Mercorio%2CF&amp;author=Mezzanzanica%2CM&amp;author=Nobani%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7."><p class="c-article-references__text" id="ref-CR7">Arrieta AB, Díaz-Rodríguez N, Del Ser J, Bennetot A, Tabik S, Barbado A, García S, Gil-López S, Molina D, Benjamins R, et al. Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Inf Fusion. 2020;58:82–115.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.inffus.2019.12.012" data-track-item_id="10.1016/j.inffus.2019.12.012" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.inffus.2019.12.012" aria-label="Article reference 7" data-doi="10.1016/j.inffus.2019.12.012">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=Explainable%20artificial%20intelligence%20%28XAI%29%3A%20Concepts%2C%20taxonomies%2C%20opportunities%20and%20challenges%20toward%20responsible%20AI&amp;journal=Inf%20Fusion&amp;doi=10.1016%2Fj.inffus.2019.12.012&amp;volume=58&amp;pages=82-115&amp;publication_year=2020&amp;author=Arrieta%2CAB&amp;author=D%C3%ADaz-Rodr%C3%ADguez%2CN&amp;author=Ser%2CJ&amp;author=Bennetot%2CA&amp;author=Tabik%2CS&amp;author=Barbado%2CA&amp;author=Garc%C3%ADa%2CS&amp;author=Gil-L%C3%B3pez%2CS&amp;author=Molina%2CD&amp;author=Benjamins%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8."><p class="c-article-references__text" id="ref-CR8">Li Q, Peng H, Li J, Xia C, Yang R, Sun L, Yu PS, He L. A survey on text classification: From traditional to deep learning. ACM Trans Intell Syst Technol (TIST). 2022;13(2):1–41.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20on%20text%20classification%3A%20From%20traditional%20to%20deep%20learning&amp;journal=ACM%20Trans%20Intell%20Syst%20Technol%20%28TIST%29&amp;volume=13&amp;issue=2&amp;pages=1-41&amp;publication_year=2022&amp;author=Li%2CQ&amp;author=Peng%2CH&amp;author=Li%2CJ&amp;author=Xia%2CC&amp;author=Yang%2CR&amp;author=Sun%2CL&amp;author=Yu%2CPS&amp;author=He%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9."><p class="c-article-references__text" id="ref-CR9">Minaee S, Kalchbrenner N, Cambria E, Nikzad N, Chenaghlu M, Gao J. Deep learning-based text classification: a comprehensive review. ACM Comput Surv (CSUR). 2021;54(3):1–40.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1145/3439726" data-track-item_id="10.1145/3439726" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1145%2F3439726" aria-label="Article reference 9" data-doi="10.1145/3439726">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning-based%20text%20classification%3A%20a%20comprehensive%20review&amp;journal=ACM%20Comput%20Surv%20%28CSUR%29&amp;doi=10.1145%2F3439726&amp;volume=54&amp;issue=3&amp;pages=1-40&amp;publication_year=2021&amp;author=Minaee%2CS&amp;author=Kalchbrenner%2CN&amp;author=Cambria%2CE&amp;author=Nikzad%2CN&amp;author=Chenaghlu%2CM&amp;author=Gao%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10."><p class="c-article-references__text" id="ref-CR10">Sokol K, Flach P. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 56–67.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11."><p class="c-article-references__text" id="ref-CR11">Burkart N, Huber MF. A survey on the explainability of supervised machine learning. J Artif Intell Res. 2021;70:245–317.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1613/jair.1.12228" data-track-item_id="10.1613/jair.1.12228" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1613%2Fjair.1.12228" aria-label="Article reference 11" data-doi="10.1613/jair.1.12228">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=4224661" aria-label="MathSciNet reference 11">MathSciNet</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20on%20the%20explainability%20of%20supervised%20machine%20learning&amp;journal=J%20Artif%20Intell%20Res&amp;doi=10.1613%2Fjair.1.12228&amp;volume=70&amp;pages=245-317&amp;publication_year=2021&amp;author=Burkart%2CN&amp;author=Huber%2CMF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12."><p class="c-article-references__text" id="ref-CR12">Mohseni S, Zarei N, Ragan ED. A multidisciplinary survey and framework for design and evaluation of explainable ai systems. ACM Trans Interact Intell Syst. 2021;11(3–4):1–45. <a href="https://doi.org/10.1145/3387166" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1145/3387166">https://doi.org/10.1145/3387166</a>.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1145/3387166" data-track-item_id="10.1145/3387166" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1145%2F3387166" aria-label="Article reference 12" data-doi="10.1145/3387166">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20multidisciplinary%20survey%20and%20framework%20for%20design%20and%20evaluation%20of%20explainable%20ai%20systems&amp;journal=ACM%20Trans%20Interact%20Intell%20Syst&amp;doi=10.1145%2F3387166&amp;volume=11&amp;issue=3%E2%80%934&amp;pages=1-45&amp;publication_year=2021&amp;author=Mohseni%2CS&amp;author=Zarei%2CN&amp;author=Ragan%2CED">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13."><p class="c-article-references__text" id="ref-CR13">Zhou J, Gandomi AH, Chen F, Holzinger A. Evaluating the quality of machine learning explanations: A survey on methods and metrics. Electronics. 2021;10(5):593.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.3390/electronics10050593" data-track-item_id="10.3390/electronics10050593" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.3390%2Felectronics10050593" aria-label="Article reference 13" data-doi="10.3390/electronics10050593">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Evaluating%20the%20quality%20of%20machine%20learning%20explanations%3A%20A%20survey%20on%20methods%20and%20metrics&amp;journal=Electronics&amp;doi=10.3390%2Felectronics10050593&amp;volume=10&amp;issue=5&amp;publication_year=2021&amp;author=Zhou%2CJ&amp;author=Gandomi%2CAH&amp;author=Chen%2CF&amp;author=Holzinger%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14."><p class="c-article-references__text" id="ref-CR14">Du K, Xing F, Cambria E. Incorporating multiple knowledge sources for targeted aspect-based financial sentiment analysis. ACM Trans Manag Inf Syst. 2023;14(3):23.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1145/3580480" data-track-item_id="10.1145/3580480" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1145%2F3580480" aria-label="Article reference 14" data-doi="10.1145/3580480">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=Incorporating%20multiple%20knowledge%20sources%20for%20targeted%20aspect-based%20financial%20sentiment%20analysis&amp;journal=ACM%20Trans%20Manag%20Inf%20Syst&amp;doi=10.1145%2F3580480&amp;volume=14&amp;issue=3&amp;publication_year=2023&amp;author=Du%2CK&amp;author=Xing%2CF&amp;author=Cambria%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15."><p class="c-article-references__text" id="ref-CR15">Keele S, et al. Guidelines for performing systematic literature reviews in software engineering. Technical Report, ver. 2.3 ebse technical report. ebse. 2007.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16."><p class="c-article-references__text" id="ref-CR16">Guidotti R, Monreale A, Ruggieri S, Pedreschi D, Turini F, Giannotti F. Local rule-based explanations of black box decision systems. <a href="http://arxiv.org/abs/1805.10820" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1805.10820">arXiv:1805.10820</a> [Preprint]. 2018. Available from: <a href="http://arxiv.org/abs/1805.10820" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1805.10820">http://arxiv.org/abs/1805.10820</a>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17."><p class="c-article-references__text" id="ref-CR17">Craven M, Shavlik J. Extracting tree-structured representations of trained networks. Adv Neural Inf Process Syst. 1995;8.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18."><p class="c-article-references__text" id="ref-CR18">Covert I, Lundberg SM, Lee S-I. Understanding global feature contributions with additive importance measures. Adv Neural Inf Process Syst. 2020;33:17212–23.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Understanding%20global%20feature%20contributions%20with%20additive%20importance%20measures&amp;journal=Adv%20Neural%20Inf%20Process%20Syst.&amp;volume=33&amp;pages=17212-23&amp;publication_year=2020&amp;author=Covert%2CI&amp;author=Lundberg%2CSM&amp;author=Lee%2CS-I">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19."><p class="c-article-references__text" id="ref-CR19">Dhurandhar A, Shanmugam K, Luss R, Olsen PA. Improving simple models with confidence profiles. Adv Neural Inf Process Syst. 2018;31.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20."><p class="c-article-references__text" id="ref-CR20">Wei D, Dash S, Gao T, Gunluk O. Generalized linear rule models. In: International Conference on Machine Learning. PMLR; 2019. pp. 6687–96.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21."><p class="c-article-references__text" id="ref-CR21">Sushil M, Šuster S, Daelemans W. Rule induction for global explanation of trained models. In: Analyzing and Interpreting Neural Networks for NLP (BlackBoxNLP), Workshop at EMNLP. 2018. pp. 82–97.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22."><p class="c-article-references__text" id="ref-CR22">Ribeiro MT, Singh S, Guestrin C. “Why should I trust you?” Explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2016. pp. 1135–44.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23."><p class="c-article-references__text" id="ref-CR23">Lundberg SM, Lee S-I. A unified approach to interpreting model predictions. Adv Neural Inf Process Syst. 2017;30.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24."><p class="c-article-references__text" id="ref-CR24">van der Waa J, Robeer M, van Diggelen J, Brinkhuis M. Neerincx M. Contrastive explanations with local foil trees. In: Proceedings of the ICML Workshop on Human Interpretability in Machine Learning (WHI 2018), Stockholm, Sweden. 2018. p. 37.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25."><p class="c-article-references__text" id="ref-CR25">Elenberg E, Dimakis AG, Feldman M, Karbasi A. Streaming weak submodularity: Interpreting neural networks on the fly. Adv Neural Inf Process Syst. 2017;30.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26."><p class="c-article-references__text" id="ref-CR26">Lei T, Barzilay R, Jaakkola T. Rationalizing neural predictions. In: Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. 2016. pp. 107–17.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27."><p class="c-article-references__text" id="ref-CR27">Kim B, Wattenberg M, Gilmer J, Cai C, Wexler J, Viegas F, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In: International Conference on Machine Learning. PMLR; 2018. pp. 2668–77.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28."><p class="c-article-references__text" id="ref-CR28">Ribeiro MT, Wu T, Guestrin C, Singh S. Beyond accuracy: Behavioral testing of NLP models with checklist. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020. pp. 4902–12.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29."><p class="c-article-references__text" id="ref-CR29">Datta A, Sen S, Zick Y. Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. In: 2016 IEEE Symposium on Security and Privacy (SP). IEEE; 2016. pp. 598–617.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30."><p class="c-article-references__text" id="ref-CR30">Hind M, Wei D, Campbell M, Codella NC, Dhurandhar A, Mojsilović A, Natesan Ramamurthy K, Varshney KR. Ted: Teaching AI to explain its decisions. In: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 2019. pp. 123–9.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31."><p class="c-article-references__text" id="ref-CR31">Staniak M, Biecek P. Explanations of model predictions with live and breakdown packages. R J. 2018;10(2).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32."><p class="c-article-references__text" id="ref-CR32">Zolna K, Geras KJ, Cho K. Classifier-agnostic saliency map extraction. Comput Vis Image Underst. 2020;196:102969.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.cviu.2020.102969" data-track-item_id="10.1016/j.cviu.2020.102969" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cviu.2020.102969" aria-label="Article reference 32" data-doi="10.1016/j.cviu.2020.102969">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=Classifier-agnostic%20saliency%20map%20extraction&amp;journal=Comput%20Vis%20Image%20Underst&amp;doi=10.1016%2Fj.cviu.2020.102969&amp;volume=196&amp;publication_year=2020&amp;author=Zolna%2CK&amp;author=Geras%2CKJ&amp;author=Cho%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33."><p class="c-article-references__text" id="ref-CR33">Dash S, Gunluk O, Wei D. Boolean decision rules via column generation. Adv Neural Inf Process Syst. 2018;31.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34."><p class="c-article-references__text" id="ref-CR34">Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra D. Grad-cam: Visual explanations from deep networks via gradient-based localization. In: Proceedings of the IEEE International Conference on Computer Vision. 2017. pp. 618–26.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35."><p class="c-article-references__text" id="ref-CR35">Singh C, Murdoch WJ, Yu B. Hierarchical interpretations for neural network predictions. In: International Conference on Learning Representations. 2018.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36."><p class="c-article-references__text" id="ref-CR36">Dhurandhar A, Chen P-Y, Luss R, Tu C-C, Ting P, Shanmugam K, Das P. Explanations based on the missing: Towards contrastive explanations with pertinent negatives. Adv Neural Inf Process Syst. 2018;31.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37."><p class="c-article-references__text" id="ref-CR37">Shrikumar A, Greenside P, Kundaje A. Learning important features through propagating activation differences. In: International Conference on Machine Learning. PMLR; 2017. pp. 3145–53.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38."><p class="c-article-references__text" id="ref-CR38">Lapuschkin S, Binder A, Montavon G, Müller K-R, Samek W. The LRP toolbox for artificial neural networks. J Mach Learn Res. 2016;17(114):1–5.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=3543520" aria-label="MathSciNet reference 38">MathSciNet</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20LRP%20toolbox%20for%20artificial%20neural%20networks&amp;journal=J%20Mach%20Learn%20Res&amp;volume=17&amp;issue=114&amp;pages=1-5&amp;publication_year=2016&amp;author=Lapuschkin%2CS&amp;author=Binder%2CA&amp;author=Montavon%2CG&amp;author=M%C3%BCller%2CK-R&amp;author=Samek%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39."><p class="c-article-references__text" id="ref-CR39">Hu L, Jian S, Cao L, Chen Q. Interpretable recommendation via attraction modeling: Learning multilevel attractiveness over multimodal movie contents. In: IJCAI International Joint Conference on Artificial Intelligence. 2018.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40."><p class="c-article-references__text" id="ref-CR40">Petsiuk V, Das A. Saenko K. Rise: Randomized input sampling for explanation of black-box models. In: Proceedings of the British Machine Vision Conference (BMVC). 2018.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41."><p class="c-article-references__text" id="ref-CR41">Wang T, Rudin C, Doshi-Velez F, Liu Y, Klampfl E, MacNeille P. A bayesian framework for learning rule sets for interpretable classification. J Mach Learn Res. 2017;18(70):1–37.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=3714233" aria-label="MathSciNet reference 41">MathSciNet</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20bayesian%20framework%20for%20learning%20rule%20sets%20for%20interpretable%20classification&amp;journal=J%20Mach%20Learn%20Res&amp;volume=18&amp;issue=70&amp;pages=1-37&amp;publication_year=2017&amp;author=Wang%2CT&amp;author=Rudin%2CC&amp;author=Doshi-Velez%2CF&amp;author=Liu%2CY&amp;author=Klampfl%2CE&amp;author=MacNeille%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42."><p class="c-article-references__text" id="ref-CR42">Ribeiro MT, Singh S, Guestrin C. Anchors: High-precision model-agnostic explanations. In: Proceedings of the AAAI Conference on Artificial Intelligence (vol. 32). 2018.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43."><p class="c-article-references__text" id="ref-CR43">Hong D, Wang T, Baek S. Protorynet - interpretable text classification via prototype trajectories. J Mach Learn Res. 2023;24(264):1–39.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44."><p class="c-article-references__text" id="ref-CR44">Nauta M, Seifert C. The co-12 recipe for evaluating interpretable part-prototype image classifiers. In: Longo L, editor. Explainable Artificial Intelligence. Cham: Springer; 2023. p. 397–420.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/978-3-031-44064-9_21" data-track-item_id="10.1007/978-3-031-44064-9_21" data-track-value="chapter reference" data-track-action="chapter reference" href="https://link.springer.com/doi/10.1007/978-3-031-44064-9_21" aria-label="Chapter reference 44" data-doi="10.1007/978-3-031-44064-9_21">Chapter</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20co-12%20recipe%20for%20evaluating%20interpretable%20part-prototype%20image%20classifiers&amp;doi=10.1007%2F978-3-031-44064-9_21&amp;pages=397-420&amp;publication_year=2023&amp;author=Nauta%2CM&amp;author=Seifert%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45."><p class="c-article-references__text" id="ref-CR45">Datta P, Kibler D. Learning prototypical concept descriptions. In: Machine Learning Proceedings 1995. 1995. pp. 158–66.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46."><p class="c-article-references__text" id="ref-CR46">Wang F, Rudin C. Falling rule lists. In: Artificial Intelligence and Statistics. PMLR; 2015. pp. 1013–22.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47."><p class="c-article-references__text" id="ref-CR47">Mothilal RK, Sharma A, Tan C. Explaining machine learning classifiers through diverse counterfactual explanations. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020. pp. 607–17.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48."><p class="c-article-references__text" id="ref-CR48">Longo L, Brcic M, Cabitza F, Choi J, Confalonieri R, Ser JD, Guidotti R, Hayashi Y, Herrera F, Holzinger A, Jiang R, Khosravi H, Lecue F, Malgieri G, Páez A, Samek W, Schneider J, Speith T, Stumpf S. Explainable artificial intelligence (XAI) 2.0: A manifesto of open challenges and interdisciplinary research directions. Inf Fusion. 2024;106:102301. <a href="https://doi.org/10.1016/j.inffus.2024.102301" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1016/j.inffus.2024.102301">https://doi.org/10.1016/j.inffus.2024.102301</a>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49."><p class="c-article-references__text" id="ref-CR49">Vilone G, Rizzo L, Longo L. A comparative analysis of rule-based, model-agnostic methods for explainable artificial intelligence. 2020.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50."><p class="c-article-references__text" id="ref-CR50">Vilone G, Longo L. A quantitative evaluation of global, rule-based explanations of post-hoc, model agnostic methods. Front Artif Intell. 2021;4:717899.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.3389/frai.2021.717899" data-track-item_id="10.3389/frai.2021.717899" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.3389%2Ffrai.2021.717899" aria-label="Article reference 50" data-doi="10.3389/frai.2021.717899">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 50" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20quantitative%20evaluation%20of%20global%2C%20rule-based%20explanations%20of%20post-hoc%2C%20model%20agnostic%20methods&amp;journal=Front%20Artif%20Intell&amp;doi=10.3389%2Ffrai.2021.717899&amp;volume=4&amp;publication_year=2021&amp;author=Vilone%2CG&amp;author=Longo%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51."><p class="c-article-references__text" id="ref-CR51">Belaid MK, Bornemann R, Rabus M, Krestel R, Hüllermeier E. Compare-XAI: Toward unifying functional testing methods for post-hoc XAI algorithms into a multi-dimensional benchmark. In: World Conference on Explainable Artificial Intelligence. Springer; 2023. pp. 88–109.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52."><p class="c-article-references__text" id="ref-CR52">Rasouli P, Yu IC. Explan: Explaining black-box classifiers using adaptive neighborhood generation. In: 2020 International Joint Conference on Neural Networks (IJCNN). IEEE; 2020. pp. 1–9.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53."><p class="c-article-references__text" id="ref-CR53">Dwivedi R, Dave D, Naik H, Singhal S, Omer R, Patel P, Qian B, Wen Z, Shah T, Morgan G, Ranjan R. Explainable AI (XAI): Core ideas, techniques, and solutions. ACM Comput Surv. 2023;55(9). <a href="https://doi.org/10.1145/3561048" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1145/3561048">https://doi.org/10.1145/3561048</a>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54."><p class="c-article-references__text" id="ref-CR54">Schwalbe G, Finzel B. A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts. Data Min Knowl Disc. 2023;1:1–59.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 54" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20comprehensive%20taxonomy%20for%20explainable%20artificial%20intelligence%3A%20a%20systematic%20survey%20of%20surveys%20on%20methods%20and%20concepts&amp;journal=Data%20Min%20Knowl%20Disc&amp;volume=1&amp;pages=1-59&amp;publication_year=2023&amp;author=Schwalbe%2CG&amp;author=Finzel%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="55."><p class="c-article-references__text" id="ref-CR55">Saeed W, Omlin C. Explainable AI (XAI): A systematic meta-survey of current challenges and future opportunities. Knowl Based Syst. 2023;263:110273. <a href="https://doi.org/10.1016/j.knosys.2023.110273" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1016/j.knosys.2023.110273">https://doi.org/10.1016/j.knosys.2023.110273</a>.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.knosys.2023.110273" data-track-item_id="10.1016/j.knosys.2023.110273" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.knosys.2023.110273" aria-label="Article reference 55" data-doi="10.1016/j.knosys.2023.110273">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 55" href="http://scholar.google.com/scholar_lookup?&amp;title=Explainable%20AI%20%28XAI%29%3A%20A%20systematic%20meta-survey%20of%20current%20challenges%20and%20future%20opportunities&amp;journal=Knowl%20Based%20Syst&amp;doi=10.1016%2Fj.knosys.2023.110273&amp;volume=263&amp;publication_year=2023&amp;author=Saeed%2CW&amp;author=Omlin%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="56."><p class="c-article-references__text" id="ref-CR56">Yang W, Wei Y, Wei H, Chen Y, Huang G, Li X, Li R, Yao N, Wang X, Gu X, et al. Survey on explainable AI: From approaches, limitations and applications aspects. Hum Centric Intell Syst. 2023;3(3):161–88.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s44230-023-00038-y" data-track-item_id="10.1007/s44230-023-00038-y" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s44230-023-00038-y" aria-label="Article reference 56" data-doi="10.1007/s44230-023-00038-y">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 56" href="http://scholar.google.com/scholar_lookup?&amp;title=Survey%20on%20explainable%20AI%3A%20From%20approaches%2C%20limitations%20and%20applications%20aspects&amp;journal=Hum%20Centric%20Intell%20Syst&amp;doi=10.1007%2Fs44230-023-00038-y&amp;volume=3&amp;issue=3&amp;pages=161-188&amp;publication_year=2023&amp;author=Yang%2CW&amp;author=Wei%2CY&amp;author=Wei%2CH&amp;author=Chen%2CY&amp;author=Huang%2CG&amp;author=Li%2CX&amp;author=Li%2CR&amp;author=Yao%2CN&amp;author=Wang%2CX&amp;author=Gu%2CX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="57."><p class="c-article-references__text" id="ref-CR57">Rong Y, Leemann T, Nguyen T-T, Fiedler L, Qian P, Unhelkar V, Seidel T, Kasneci G, Kasneci E. Towards human-centered explainable AI: A survey of user studies for model explanations. IEEE Trans Pattern Anal Mach Intell. 2024;46(4):2104–22. <a href="https://doi.org/10.1109/TPAMI.2023.3331846" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1109/TPAMI.2023.3331846">https://doi.org/10.1109/TPAMI.2023.3331846</a>.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TPAMI.2023.3331846" data-track-item_id="10.1109/TPAMI.2023.3331846" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTPAMI.2023.3331846" aria-label="Article reference 57" data-doi="10.1109/TPAMI.2023.3331846">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 57" href="http://scholar.google.com/scholar_lookup?&amp;title=Towards%20human-centered%20explainable%20AI%3A%20A%20survey%20of%20user%20studies%20for%20model%20explanations&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;doi=10.1109%2FTPAMI.2023.3331846&amp;volume=46&amp;issue=4&amp;pages=2104-2122&amp;publication_year=2024&amp;author=Rong%2CY&amp;author=Leemann%2CT&amp;author=Nguyen%2CT-T&amp;author=Fiedler%2CL&amp;author=Qian%2CP&amp;author=Unhelkar%2CV&amp;author=Seidel%2CT&amp;author=Kasneci%2CG&amp;author=Kasneci%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="58."><p class="c-article-references__text" id="ref-CR58">Fauvel K, Masson V, Fromont E. A performance-explainability framework to benchmark machine learning methods: Application to multivariate time series classifiers. In: Proceedings of the IJCAI-PRICAI 2020 Workshop on Explainable AI. 2021. pp. 1–8.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="59."><p class="c-article-references__text" id="ref-CR59">Vilone G, Longo L. Notions of explainability and evaluation approaches for explainable artificial intelligence. Inf Fusion. 2021;76:89–106.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.inffus.2021.05.009" data-track-item_id="10.1016/j.inffus.2021.05.009" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.inffus.2021.05.009" aria-label="Article reference 59" data-doi="10.1016/j.inffus.2021.05.009">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 59" href="http://scholar.google.com/scholar_lookup?&amp;title=Notions%20of%20explainability%20and%20evaluation%20approaches%20for%20explainable%20artificial%20intelligence&amp;journal=Inf%20Fusion&amp;doi=10.1016%2Fj.inffus.2021.05.009&amp;volume=76&amp;pages=89-106&amp;publication_year=2021&amp;author=Vilone%2CG&amp;author=Longo%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="60."><p class="c-article-references__text" id="ref-CR60">Keane MT, Kenny EM, Delaney E, Smyth B. If only we had better counterfactual explanations: Five key deficits to rectify in the evaluation of counterfactual XAI techniques. In: IJCAI. 2021. pp. 4467–74.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="61."><p class="c-article-references__text" id="ref-CR61">Waa J, Nieuwburg E, Cremers A, Neerincx M. Evaluating XAI: a comparison of rule-based and example-based explanations. Artif Intell. 2021;291:103404.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.artint.2020.103404" data-track-item_id="10.1016/j.artint.2020.103404" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.artint.2020.103404" aria-label="Article reference 61" data-doi="10.1016/j.artint.2020.103404">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=4183296" aria-label="MathSciNet reference 61">MathSciNet</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 61" href="http://scholar.google.com/scholar_lookup?&amp;title=Evaluating%20XAI%3A%20a%20comparison%20of%20rule-based%20and%20example-based%20explanations&amp;journal=Artif%20Intell&amp;doi=10.1016%2Fj.artint.2020.103404&amp;volume=291&amp;publication_year=2021&amp;author=Waa%2CJ&amp;author=Nieuwburg%2CE&amp;author=Cremers%2CA&amp;author=Neerincx%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="62."><p class="c-article-references__text" id="ref-CR62">Yeh C-K, Hsieh C-Y, Suggala A, Inouye DI, Ravikumar PK. On the (in) fidelity and sensitivity of explanations. Adv Neural Inf Process Syst. 2019;32.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="63."><p class="c-article-references__text" id="ref-CR63">Bhatt U, Weller A, Moura JM. Evaluating and aggregating feature-based model explanations. <a href="http://arxiv.org/abs/2005.00631" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2005.00631">arXiv:2005.00631</a> [Preprint]. 2020. Available from: <a href="http://arxiv.org/abs/2005.00631" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2005.00631">http://arxiv.org/abs/2005.00631</a>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="64."><p class="c-article-references__text" id="ref-CR64">Ma E. NLP Augmentation. 2019. <a href="https://github.com/makcedward/nlpaug" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://github.com/makcedward/nlpaug">https://github.com/makcedward/nlpaug</a>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="65."><p class="c-article-references__text" id="ref-CR65">Maas A, Daly RE, Pham PT, Huang D, Ng AY, Potts C. Learning word vectors for sentiment analysis. In: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. 2011. pp. 142–50.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="66."><p class="c-article-references__text" id="ref-CR66">Kumar H, Harish B, Darshan H. Sentiment analysis on imdb movie reviews using hybrid feature extraction method. Int J Interact Multimed Artif Intell. 2019;5(5).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="67."><p class="c-article-references__text" id="ref-CR67">Bird S, Klein E, Loper E. Natural Language Processing with Python: Analyzing text with the natural language toolkit. 2009.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="68."><p class="c-article-references__text" id="ref-CR68">Li Q, Peng H, Li J, Xia C, Yang R, Sun L, Yu PS, He L. A survey on text classification: From traditional to deep learning. ACM Trans Intell Syst Technol. 2022;13(2). <a href="https://doi.org/10.1145/3495162" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1145/3495162">https://doi.org/10.1145/3495162</a>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="69."><p class="c-article-references__text" id="ref-CR69">Siino M, Tinnirello I, La Cascia M. Is text preprocessing still worth the time? A comparative survey on the influence of popular preprocessing methods on transformers and traditional classifiers. Inf Syst. 2024;121:102342. <a href="https://doi.org/10.1016/j.is.2023.102342" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1016/j.is.2023.102342">https://doi.org/10.1016/j.is.2023.102342</a>.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.is.2023.102342" data-track-item_id="10.1016/j.is.2023.102342" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.is.2023.102342" aria-label="Article reference 69" data-doi="10.1016/j.is.2023.102342">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 69" href="http://scholar.google.com/scholar_lookup?&amp;title=Is%20text%20preprocessing%20still%20worth%20the%20time%3F%20A%20comparative%20survey%20on%20the%20influence%20of%20popular%20preprocessing%20methods%20on%20transformers%20and%20traditional%20classifiers&amp;journal=Inf%20Syst&amp;doi=10.1016%2Fj.is.2023.102342&amp;volume=121&amp;publication_year=2024&amp;author=Siino%2CM&amp;author=Tinnirello%2CI&amp;author=Cascia%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="70."><p class="c-article-references__text" id="ref-CR70">Minaee S, Kalchbrenner N, Cambria E, Nikzad N, Chenaghlu M, Gao J. Deep learning-based text classification: A comprehensive review. ACM Comput Surv. 2021;54(3). <a href="https://doi.org/10.1145/3439726" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1145/3439726">https://doi.org/10.1145/3439726</a>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="71."><p class="c-article-references__text" id="ref-CR71">Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V, Vanderplas J, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay E. Scikit-learn: Machine learning in Python. J Mach Learn Res. 2011;12:2825–30.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="72."><p class="c-article-references__text" id="ref-CR72">Breiman L. Classification and regression trees. 2017.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="73."><p class="c-article-references__text" id="ref-CR73">Hintze JL, Nelson RD. Violin plots: a box plot-density trace synergism. Am Stat. 1998;52(2):181–4. <a href="https://doi.org/10.1080/00031305.1998.10480559" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1080/00031305.1998.10480559">https://doi.org/10.1080/00031305.1998.10480559</a>.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1080/00031305.1998.10480559" data-track-item_id="10.1080/00031305.1998.10480559" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1080%2F00031305.1998.10480559" aria-label="Article reference 73" data-doi="10.1080/00031305.1998.10480559">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 73" href="http://scholar.google.com/scholar_lookup?&amp;title=Violin%20plots%3A%20a%20box%20plot-density%20trace%20synergism&amp;journal=Am%20Stat.&amp;doi=10.1080%2F00031305.1998.10480559&amp;volume=52&amp;issue=2&amp;pages=181-4&amp;publication_year=1998&amp;author=Hintze%2CJL&amp;author=Nelson%2CRD">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1007/s12559-024-10325-w?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Funding"><div class="c-article-section" id="Fun-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Fun">Funding</h2><div class="c-article-section__content" id="Fun-content"><p>Open access funding provided by Universitá degli Studi di Milano - Bicocca within the CRUI-CARE Agreement. No funding was obtained for this study.</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Statistics and Quantitative Methods, University of Milan-Bicocca, Milan, Italy</p><p class="c-article-author-affiliation__authors-list">Mirko Cesarini, Lorenzo Malandri, Filippo Pallucchini &amp; Andrea Seveso</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">School of Computing, National University of Singapore, Singapore, Singapore</p><p class="c-article-author-affiliation__authors-list">Frank Xing</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Mirko-Cesarini-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Mirko Cesarini</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Mirko%20Cesarini" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Mirko%20Cesarini" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Mirko%20Cesarini%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Lorenzo-Malandri-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Lorenzo Malandri</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Lorenzo%20Malandri" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Lorenzo%20Malandri" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Lorenzo%20Malandri%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Filippo-Pallucchini-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Filippo Pallucchini</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Filippo%20Pallucchini" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Filippo%20Pallucchini" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Filippo%20Pallucchini%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Andrea-Seveso-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Andrea Seveso</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Andrea%20Seveso" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Andrea%20Seveso" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Andrea%20Seveso%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Frank-Xing-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Frank Xing</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Frank%20Xing" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Frank%20Xing" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Frank%20Xing%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="contributions">Contributions</h3><p>Lorenzo Malandri performed the conceptualisation, supervised the project; Mirko Cesarini performed the conceptualisation, supervised the project; Frank Xing defined the methodology, performed the formal analysis; Andrea Seveso defined the methodology, performed the formal analysis, performed the experimental part; Filippo Pallucchini defined the methodology, performed the formal analysis, performed the experimental part, validated the results. All authors were engaged in writing and reviewing the manuscript.</p><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" aria-label="email Lorenzo Malandri" href="mailto:lorenzo.malandri@unimib.it">Lorenzo Malandri</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
              
                <h3 class="c-article__sub-heading" id="FPar13">Research Involving Human Participants</h3>
                <p>The user study conducted in this paper adhered to ethical guidelines by utilising the Prolific platform, which ensures voluntary participation, informed consent, anonymity, confidentiality, and transparent payment information. All participants were informed about the nature of the study, their rights, and the handling of their data, ensuring their welfare and privacy throughout the research process.</p>
              
              
                <h3 class="c-article__sub-heading" id="FPar14">Conflict of interest</h3>
                <p>The authors declare no Conflict of interest.</p>
              
            </div></div></section><section data-title="Additional information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading">Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section aria-labelledby="appendices"><div class="c-article-section" id="appendices-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="appendices">Appendix</h2><div class="c-article-section__content" id="appendices-content"><h3 class="c-article__sub-heading u-visually-hidden" id="App1">Appendix</h3><p>In Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s12559-024-10325-w#Tab5">5</a>, we present all acronyms used throughout the paper.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5 Full list of acronyms employed in this manuscript</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s12559-024-10325-w/tables/5" aria-label="Full size table 5"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Below is the survey template presented to participants; the response time is recorded for all the questions, especially Q1–Q8.</p>
                  <h3 class="c-article__sub-heading" id="FPar15">Q1:</h3>
                  <p>Read the following description,</p>
                  <p>[text sample 1]</p>
                  <p>[no LIME explanation]</p>
                  <p>Do you think of the comment as a positive or negative one?</p>
                
                  <h3 class="c-article__sub-heading" id="FPar16">Q2:</h3>
                  <p>Read the following description,</p>
                  <p>[text sample 2]</p>
                  <p>[with LIME explanations]</p>
                  <p>Do you think of the comment as a positive or negative one?</p>
                
                  <h3 class="c-article__sub-heading" id="FPar17">Q3:</h3>
                  <p>Read the following description,</p>
                  <p>[text sample 3]</p>
                  <p>[no LR explanation]</p>
                  <p>Do you think of the comment as a positive or negative one?</p>
                
                  <h3 class="c-article__sub-heading" id="FPar18">Q4:</h3>
                  <p>Read the following description,</p>
                  <p>[text sample 4]</p>
                  <p>[consider LR explanation words]</p>
                  <p>Do you think of the comment as a positive or negative one?</p>
                
                  <h3 class="c-article__sub-heading" id="FPar19">Q5:</h3>
                  <p>Read the following description,</p>
                  <p>[text sample 5]</p>
                  <p>[no DT explanation]</p>
                  <p>Do you think of the comment as a positive or negative one?</p>
                
                  <h3 class="c-article__sub-heading" id="FPar20">Q6:</h3>
                  <p>Read the following description,</p>
                  <p>[text sample 6]</p>
                  <p>[consider DT rules: if..., choose positive/negative]</p>
                  <p>Do you think of the comment as a positive or negative one?</p>
                
                  <h3 class="c-article__sub-heading" id="FPar21">Q7:</h3>
                  <p>Read the following description,</p>
                  <p>[text sample 7]</p>
                  <p>[no Anchors explanation]</p>
                  <p>Do you think of the comment as a positive or negative one?</p>
                
                  <h3 class="c-article__sub-heading" id="FPar22">Q8:</h3>
                  <p>Read the following description,</p>
                  <p>[text sample 8]</p>
                  <p>[consider Anchors explanation words]</p>
                  <p>Do you think of the comment as a positive or negative one?</p>
                
                  <h3 class="c-article__sub-heading" id="FPar23">Q9:</h3>
                  <p>You are told the following description is positive/negative,</p>
                  <p>[text sample 9]</p>
                  <p>because: /shuffled</p>
                  <p>[LIME explanation]</p>
                  <p>[LR explanation]</p>
                  <p>[DT explanation]</p>
                  <p>[Anchors explanation]</p>
                  <p>Rate the above four explanations from a scale of 1–5, how are you satisfied with them.</p>
                
                  <h3 class="c-article__sub-heading" id="FPar24">Q10:</h3>
                  <p>You are told the following description is positive/negative,</p>
                  <p>[text sample 10]</p>
                  <p>Choose the explanation that you trust most, and briefly explain why do you trust it?</p>
                  <p>/shuffled</p>
                  <p>[LIME explanation]</p>
                  <p>[LR explanation]</p>
                  <p>[DT explanation]</p>
                  <p>[Anchors explanation]</p>
                </div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content">
                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Explainable%20AI%20for%20Text%20Classification%3A%20Lessons%20from%20a%20Comprehensive%20Evaluation%20of%20Post%20Hoc%20Methods&amp;author=Mirko%20Cesarini%20et%20al&amp;contentID=10.1007%2Fs12559-024-10325-w&amp;copyright=The%20Author%28s%29&amp;publication=1866-9956&amp;publicationDate=2024-08-06&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s12559-024-10325-w" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s12559-024-10325-w" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Cesarini, M., Malandri, L., Pallucchini, F. <i>et al.</i> Explainable AI for Text Classification: Lessons from a Comprehensive Evaluation of Post Hoc Methods.
                    <i>Cogn Comput</i> <b>16</b>, 3077–3095 (2024). https://doi.org/10.1007/s12559-024-10325-w</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1007/s12559-024-10325-w?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2024-04-01">01 April 2024</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2024-07-03">03 July 2024</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2024-08-06">06 August 2024</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2024-11">November 2024</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1007/s12559-024-10325-w</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy shareable link to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span><a href="/search?query=Explainable%20AI&amp;facet-discipline=&#34;Computer%20Science&#34;" data-track="click" data-track-action="view keyword" data-track-label="link">Explainable AI</a></span></li><li class="c-article-subject-list__subject"><span><a href="/search?query=XAI%20evaluation&amp;facet-discipline=&#34;Computer%20Science&#34;" data-track="click" data-track-action="view keyword" data-track-label="link">XAI evaluation</a></span></li><li class="c-article-subject-list__subject"><span><a href="/search?query=Text%20classification&amp;facet-discipline=&#34;Computer%20Science&#34;" data-track="click" data-track-action="view keyword" data-track-label="link">Text classification</a></span></li><li class="c-article-subject-list__subject"><span><a href="/search?query=Interpretability&amp;facet-discipline=&#34;Computer%20Science&#34;" data-track="click" data-track-action="view keyword" data-track-label="link">Interpretability</a></span></li><li class="c-article-subject-list__subject"><span><a href="/search?query=Human-computer%20interaction&amp;facet-discipline=&#34;Computer%20Science&#34;" data-track="click" data-track-action="view keyword" data-track-label="link">Human-computer interaction</a></span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>

                    
    <div id="researcher-profile-container">
        <h3>Profiles</h3>
        <ol>
            
                <li data-test="researcher-profile-data"  data-profile-index="3">
                    <span data-test="researcher-profile-name">Andrea Seveso</span>
                    <a class="js-cta-popup-link c-article-authors-search__cta-link" href="/researchers/46459829SN" data-track="click_view_profile" data-test="researcher-profile-link">
                        <span class="eds-c-button eds-c-button--primary">
                            <svg class="c-article-authors-search__cta-icon" aria-hidden="true" focusable="false" width="24" height="24">
                                <use xlink:href="#icon-eds-i-user-single-medium"></use>
                            </svg><span>View author profile</span>
                        </span>
                    </a>
                </li>
            
                <li data-test="researcher-profile-data"  data-profile-index="4">
                    <span data-test="researcher-profile-name">Frank Xing</span>
                    <a class="js-cta-popup-link c-article-authors-search__cta-link" href="/researchers/36535937SN" data-track="click_view_profile" data-test="researcher-profile-link">
                        <span class="eds-c-button eds-c-button--primary">
                            <svg class="c-article-authors-search__cta-icon" aria-hidden="true" focusable="false" width="24" height="24">
                                <use xlink:href="#icon-eds-i-user-single-medium"></use>
                            </svg><span>View author profile</span>
                        </span>
                    </a>
                </li>
            
        </ol>
    </div>


                    
                </div>
            </main>

            <div class="c-article-sidebar u-text-sm u-hide-print l-with-sidebar__sidebar" id="sidebar"
                 data-container-type="reading-companion" data-track-component="reading companion">
                <aside aria-label="reading companion">
                    

                    
                        
    <div class="app-card-service" data-test="article-checklist-banner">
        <div>
            <a class="app-card-service__link" data-track="click_presubmission_checklist" data-track-context="article page top of reading companion" data-track-category="pre-submission-checklist" data-track-action="clicked article page checklist banner test 2 old version" data-track-label="link" href="https://beta.springernature.com/pre-submission?journalId=12559"
            data-test="article-checklist-banner-link">
            <span class="app-card-service__link-text">Use our pre-submission checklist</span>
            <svg class="app-card-service__link-icon" aria-hidden="true" focusable="false"><use xlink:href="#icon-eds-i-arrow-right-small"></use></svg>
            </a>
            <p class="app-card-service__description">Avoid common mistakes on your manuscript.</p>
        </div>
        <div class="app-card-service__icon-container">
            <svg class="app-card-service__icon" aria-hidden="true" focusable="false">
                <use xlink:href="#icon-eds-i-clipboard-check-medium"></use>
            </svg>
        </div>
    </div>

                    

                    
                        <div data-test="collections">
                            
    
        <div class="c-article-associated-content__container">
            <h2 class="c-article-associated-content__title u-h3 u-mb-24 u-visually-hidden">Associated Content</h2>
            
                <div class="c-article-associated-content__collection collection u-mb-24">
                    
                    <p class="c-article-associated-content__collection-label u-sans-serif u-text-bold u-mb-8">Part of a collection:</p>
                    
                    <h3 class="c-article-associated-content__collection-title u-mt-0 u-h3 u-mb-8" itemprop="name headline">
                        <a href="/collections/aiheegecba"
                           data-track="click" data-track-action="view collection" data-track-label="link">Granular Computing for Explainable Artificial Intelligence</a>
                    </h3>
                </div>
            
        </div>

        <script>
        window.dataLayer = window.dataLayer || [];
        window.dataLayer[0] = window.dataLayer[0] || {};
        window.dataLayer[0].content = window.dataLayer[0].content || {};
        window.dataLayer[0].content.collections = 'aiheegecba';
        </script>
    

                        </div>
                    

                    <div data-test="editorial-summary">
                        
                    </div>

                    <div class="c-reading-companion">
                        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky"
                             data-test="reading-companion-sticky">
                            
                            
                                
                                    
                                
                            
                            <div
                                class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active"
                                id="tabpanel-sections">
                                <div class="u-lazy-ad-wrapper u-mt-16 u-hide"
                                     data-component-mpu><div class="c-ad c-ad--300x250">
    <div class="c-ad__inner">
        <p class="c-ad__label">Advertisement</p>
        <div id="div-gpt-ad-MPU1"
             class="div-gpt-ad grade-c-hide"
             data-pa11y-ignore
             data-gpt
             data-gpt-unitpath="/270604982/springerlink/12559/article"
             data-gpt-sizes="300x250" data-test="MPU1-ad"
             data-gpt-targeting="pos=MPU1;articleid=s12559-024-10325-w;">
        </div>
    </div>
</div>

<script>
    window.SN = window.SN || {};
    window.SN.libs = window.SN.libs || {};
    window.SN.libs.ads = window.SN.libs.ads || {};
    window.SN.libs.ads.slotConfig = window.SN.libs.ads.slotConfig || {};
    window.SN.libs.ads.slotConfig['MPU1'] = {
        'pos': 'MPU1',
        'type': 'MPU1',
    };
    window.SN.libs.ads.slotConfig['unitPath'] = '/270604982/springerlink/12559/article';
</script>

</div>
                            </div>
                            <div
                                class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width"
                                id="tabpanel-figures"></div>
                            <div
                                class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width"
                                id="tabpanel-references"></div>
                        </div>
                    </div>
                </aside>
            </div>
        </div>
    </article>
    <div class="app-elements" data-test="footer">
    <nav aria-label="expander navigation">



    
        <div class="eds-c-header__expander eds-c-header__expander--search" id="eds-c-header-popup-search">
            <h2 class="eds-c-header__heading">Search</h2>
            <div class="u-container">
                <search class="eds-c-header__search" role="search" aria-label="Search from the header">
                    <form method="GET" action="//link.springer.com/search"
                        
                            data-test="header-search"
                        
                            data-track="search"
                        
                            data-track-context="search from header"
                        
                            data-track-action="submit search form"
                        
                            data-track-category="unified header"
                        
                            data-track-label="form"
                        
					>
                        <label for="eds-c-header-search" class="eds-c-header__search-label">Search by keyword or author</label>
                        <div class="eds-c-header__search-container">
                            <input id="eds-c-header-search" class="eds-c-header__search-input" autocomplete="off" name="query" type="search" value="" required>
                            <button class="eds-c-header__search-button" type="submit">
                                <svg class="eds-c-header__icon" aria-hidden="true" focusable="false">
                                    <use xlink:href="#icon-eds-i-search-medium"></use>
                                </svg>
                                <span class="u-visually-hidden">Search</span>
                            </button>
                        </div>
                    </form>
                </search>
            </div>
        </div>
    


<div class="eds-c-header__expander eds-c-header__expander--menu" id="eds-c-header-nav">
    
        <h2 class="eds-c-header__heading">Navigation</h2>
        <ul class="eds-c-header__list">
            
                <li class="eds-c-header__list-item">
                   <a class="eds-c-header__link" href="https://link.springer.com/journals/"
                        
                            data-track="nav_find_a_journal"
                        
                            data-track-context="unified header"
                        
                            data-track-action="click find a journal"
                        
                            data-track-category="unified header"
                        
                            data-track-label="link"
                        
					>
                        Find a journal
                    </a>
                </li>
            
                <li class="eds-c-header__list-item">
                   <a class="eds-c-header__link" href="https://www.springernature.com/gp/authors"
                        
                            data-track="nav_how_to_publish"
                        
                            data-track-context="unified header"
                        
                            data-track-action="click publish with us link"
                        
                            data-track-category="unified header"
                        
                            data-track-label="link"
                        
					>
                        Publish with us
                    </a>
                </li>
            
                <li class="eds-c-header__list-item">
                   <a class="eds-c-header__link" href="https://link.springernature.com/home/"
                        
                            data-track="nav_track_your_research"
                        
                            data-track-context="unified header"
                        
                            data-track-action="click track your research"
                        
                            data-track-category="unified header"
                        
                            data-track-label="link"
                        
					>
                        Track your research
                    </a>
                </li>
            
        </ul>
    
</div>
</nav>
    <footer >
	<div class="eds-c-footer"
		
	>
		
			
				<div class="eds-c-footer__container">
		<div class="eds-c-footer__grid eds-c-footer__group--separator">
			
			<div class="eds-c-footer__group">
				<h3 class="eds-c-footer__heading">Discover content</h3>
				<ul class="eds-c-footer__list">
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/journals/a/1" data-track="nav_journals_a_z" data-track-action="journals a-z" data-track-context="unified footer" data-track-label="link">Journals A-Z</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/books/a/1" data-track="nav_books_a_z" data-track-action="books a-z" data-track-context="unified footer" data-track-label="link">Books A-Z</a></li>
					
				</ul>
			</div>
			
			<div class="eds-c-footer__group">
				<h3 class="eds-c-footer__heading">Publish with us</h3>
				<ul class="eds-c-footer__list">
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/journals" data-track="nav_journal_finder" data-track-action="journal finder" data-track-context="unified footer" data-track-label="link">Journal finder</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/authors" data-track="nav_publish_your_research" data-track-action="publish your research" data-track-context="unified footer" data-track-label="link">Publish your research</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://authorservices.springernature.com/go/sn/?utm_source&#x3D;SNLinkfooter&amp;utm_medium&#x3D;Web&amp;utm_campaign&#x3D;SNReferral" data-track="nav_language_editing" data-track-action="language editing" data-track-context="unified footer" data-track-label="link">Language editing</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research" data-track="nav_open_access_publishing" data-track-action="open access publishing" data-track-context="unified footer" data-track-label="link">Open access publishing</a></li>
					
				</ul>
			</div>
			
			<div class="eds-c-footer__group">
				<h3 class="eds-c-footer__heading">Products and services</h3>
				<ul class="eds-c-footer__list">
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/products" data-track="nav_our_products" data-track-action="our products" data-track-context="unified footer" data-track-label="link">Our products</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/librarians" data-track="nav_librarians" data-track-action="librarians" data-track-context="unified footer" data-track-label="link">Librarians</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/societies" data-track="nav_societies" data-track-action="societies" data-track-context="unified footer" data-track-label="link">Societies</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/partners" data-track="nav_partners_and_advertisers" data-track-action="partners and advertisers" data-track-context="unified footer" data-track-label="link">Partners and advertisers</a></li>
					
				</ul>
			</div>
			
			<div class="eds-c-footer__group">
				<h3 class="eds-c-footer__heading">Our brands</h3>
				<ul class="eds-c-footer__list">
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springer.com/" data-track="nav_imprint_Springer" data-track-action="Springer" data-track-context="unified footer" data-track-label="link">Springer</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.nature.com/" data-track="nav_imprint_Nature_Portfolio" data-track-action="Nature Portfolio" data-track-context="unified footer" data-track-label="link">Nature Portfolio</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/brands/bmc" data-track="nav_imprint_BMC" data-track-action="BMC" data-track-context="unified footer" data-track-label="link">BMC</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.palgrave.com/" data-track="nav_imprint_Palgrave_Macmillan" data-track-action="Palgrave Macmillan" data-track-context="unified footer" data-track-label="link">Palgrave Macmillan</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.apress.com/" data-track="nav_imprint_Apress" data-track-action="Apress" data-track-context="unified footer" data-track-label="link">Apress</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/brands/discover" data-track="nav_imprint_Discover" data-track-action="Discover" data-track-context="unified footer" data-track-label="link">Discover</a></li>
					
				</ul>
			</div>
			
		</div>
	</div>

		
		
		<div class="eds-c-footer__container">
	
		<nav aria-label="footer navigation">
			<ul class="eds-c-footer__links">
				
					<li class="eds-c-footer__item">
						
						
							<button class="eds-c-footer__link" data-cc-action="preferences"
								 data-track="dialog_manage_cookies" data-track-action="Manage cookies" data-track-context="unified footer" data-track-label="link"><span class="eds-c-footer__button-text">Your privacy choices/Manage cookies</span></button>
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://www.springernature.com/gp/legal/ccpa"
								 data-track="nav_california_privacy_statement" data-track-action="california privacy statement" data-track-context="unified footer" data-track-label="link">Your US state privacy rights</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://link.springer.com/accessibility"
								 data-track="nav_accessibility_statement" data-track-action="accessibility statement" data-track-context="unified footer" data-track-label="link">Accessibility statement</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://link.springer.com/termsandconditions"
								 data-track="nav_terms_and_conditions" data-track-action="terms and conditions" data-track-context="unified footer" data-track-label="link">Terms and conditions</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://link.springer.com/privacystatement"
								 data-track="nav_privacy_policy" data-track-action="privacy policy" data-track-context="unified footer" data-track-label="link">Privacy policy</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://support.springernature.com/en/support/home"
								 data-track="nav_help_and_support" data-track-action="help and support" data-track-context="unified footer" data-track-label="link">Help and support</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://link.springer.com/legal-notice"
								 data-track="nav_legal_notice" data-track-action="legal notice" data-track-context="unified footer" data-track-label="link">Legal notice</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://support.springernature.com/en/support/solutions/articles/6000255911-subscription-cancellations"
								 data-track-action="cancel contracts here">Cancel contracts here</a>
						
						
					</li>
				
			</ul>
		</nav>
	
	
		
			<div class="eds-c-footer__user">
				<p class="eds-c-footer__user-info">
					
					<span data-test="footer-user-ip">37.16.98.2</span>
				</p>
				<p class="eds-c-footer__user-info" data-test="footer-business-partners">Not affiliated</p>
			</div>
		
	
	
		<a href="https://www.springernature.com/" class="eds-c-footer__link">
			<img src="/oscar-static/images/logo-springernature-white-19dd4ba190.svg" alt="Springer Nature" loading="lazy" width="200" height="20"/>
		</a>
	
	<p class="eds-c-footer__legal" data-test="copyright">&copy; 2025 Springer Nature</p>
</div>

	</div>
</footer>
</div>


    </body>
</html>


