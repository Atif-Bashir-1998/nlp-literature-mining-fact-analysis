paperId,title,year,venue,citationCount,referenceCount,abstract,authors,n_authors
a6dc30b886c5132aaa716a0494d043b9ebb8cb07,A Review of Explainable Recommender Systems Utilizing Knowledge Graphs and Reinforcement Learning,2024,IEEE Access,7,102,"This review paper addresses the research question of the significance of explainability in AI and the role of integrating KG and RL to enhance Explainable Recommender Systems (XRS). It surveys articles published from January 2015 to March 2024 on XRS, focusing on knowledge graphs (KGs) and reinforcement learning (RL) for achieving explainability in recommender systems. Employing a systematic methodology, it introduces a custom Python-based web scraper to efficiently navigate and extract relevant academic research papers from IEEE, ScienceDirect (Elsevier), ACM, and Springer online databases. The study encompasses the PRISMA methodology to conduct a thorough analysis and identify pertinent research works. This systematic literature review aims to provide a unified view of the field by reviewing eight existing XRS literature reviews and 29 pertinent XRS studies involving KG and RL from the specified period. It categorizes and analyses relevant research papers based on their implementation methodologies and explores significant contributions, encompassing perspectives on model-agnostic and model-intrinsic explanations.","[{'authorId': '2294449489', 'name': 'Neeraj Tiwary'}, {'authorId': '118523123', 'name': 'Shahrul Azman Mohd Noah'}, {'authorId': '2294423020', 'name': 'Fariza Fauzi'}, {'authorId': '9287076', 'name': 'Tan Siok Yee'}]",4
75841526f5f793aabc1c8895fda6201043fb46e8,Mitigating filter bubbles: Diverse and explainable recommender systems,2024,Journal of Intelligent &amp; Fuzzy Systems,2,11,"In recent years, the surge in online content has necessitated the development of intelligent recommender systems capable of offering personalized suggestions to users. However, these systems often encapsulate users within a “filter bubble”, limiting their exposure to a narrow range of content. This study introduces a novel approach to address this issue by integrating a novel diversity module into a knowledge graph-based explainable recommender system. Utilizing the Movie Lens 1M dataset, this research pioneers in fostering a more nuanced and transparent user experience, thereby enhancing user trust and broadening the spectrum of recommendations. Looking ahead, we aim to further refine this system by incorporating an explicit feedback loop and leveraging Natural Language Processing (NLP) techniques to provide users with insightful explanations of recommendations, including a comprehensive analysis of filter bubbles. This initiative marks a significant stride towards creating a more inclusive and informed recommendation landscape, promising users not only a wider array of content but also a deeper understanding of the recommendation mechanisms at play.","[{'authorId': '2298681384', 'name': 'Umar Tahir Kidwai'}, {'authorId': '2220166482', 'name': 'Nadeem Akhtar'}, {'authorId': '2298651522', 'name': 'Mohammad Nadeem'}, {'authorId': '49004436', 'name': 'Roobaea Alroobaea'}]",4
481442415a7610e9da7ee3c8603d07b8f6b2df3d,A Comparative Analysis of Text-Based Explainable Recommender Systems,2024,ACM Conference on Recommender Systems,3,51,"One way to increase trust among users towards recommender systems is to provide the recommendation along with a textual explanation. In the literature, extraction-based, generation-based, and, more recently, hybrid solutions based on retrieval-augmented generation have been proposed to tackle the problem of text-based explainable recommendation. However, the use of different datasets, preprocessing steps, target explanations, baselines, and evaluation metrics complicates the reproducibility and state-of-the-art assessment of previous work among different model categories for successful advancements in the field. Our aim is to provide a comprehensive analysis of text-based explainable recommender systems by setting up a well-defined benchmark that accommodates generation-based, extraction-based, and hybrid approaches. Also, we enrich the existing evaluation of explainability and text quality of the explanations with a novel definition of feature hallucination. Our experiments on three real-world datasets unveil hidden behaviors and confirm several claims about model patterns. Our source code and preprocessed datasets are available at https://github.com/alarca94/text-exp-recsys24.","[{'authorId': '2240551453', 'name': 'Alejandro Ariza-Casabona'}, {'authorId': '1824224', 'name': 'Ludovico Boratto'}, {'authorId': '2035668', 'name': 'Maria Salamó'}]",3
b11668ea517548fdac1fdc4e3ec9e50164de4c4c,Whom do Explanations Serve? A Systematic Literature Survey of User Characteristics in Explainable Recommender Systems Evaluation,2024,Trans. Recomm. Syst.,4,156,"Adding explanations to recommender systems is said to have multiple benefits, such as increasing user trust or system transparency. Previous work from other application areas suggests that specific user characteristics impact the users’ perception of the explanation. However, we rarely find this type of evaluation for recommender systems explanations. This paper addresses this gap by surveying 124 papers in which recommender systems explanations were evaluated in user studies. We analyzed their participant descriptions and study results where the impact of user characteristics on the explanation effects was measured. Our findings suggest that the results from the surveyed studies predominantly cover specific users who do not necessarily represent the users of recommender systems in the evaluation domain. This may seriously hamper the generalizability of any insights we may gain from current studies on explanations in recommender systems. We further find inconsistencies in the data reporting, which impacts the reproducibility of the reported results. Hence, we recommend actions to move toward a more inclusive and reproducible evaluation.","[{'authorId': '2325155946', 'name': 'Kathrin Wardatzky'}, {'authorId': '2692909', 'name': 'Oana Inel'}, {'authorId': '2266468283', 'name': 'Luca Rossetto'}, {'authorId': '2159592493', 'name': 'Abraham Bernstein'}]",4
1cb0d0397404f2d94e4b2d201034f108ec7fde6a,Interaction Tasks for Explainable Recommender Systems,2023,Eurographics Conference on Visualization,1,28,,"[{'authorId': '33828586', 'name': 'M. Krone'}, {'authorId': '2220681380', 'name': 'S. Lenti'}, {'authorId': '2217797563', 'name': 'I. Al-Hazwani'}, {'authorId': '41078761', 'name': 'Turki S Alahmadi'}, {'authorId': '2325155946', 'name': 'Kathrin Wardatzky'}, {'authorId': '2692909', 'name': 'Oana Inel'}, {'authorId': '1401917601', 'name': 'Mennatallah El-Assady'}, {'authorId': '2052846593', 'name': 'J. Bernard'}]",8
5bb0150f1a73e174642675610038cc89bd4b3ca6,A graph-based approach for minimising the knowledge requirement of explainable recommender systems,2023,Knowledge and Information Systems,7,64,"Traditionally, recommender systems use collaborative filtering or content-based approaches based on ratings and item descriptions. However, this information is unavailable in many domains and applications, and recommender systems can only tackle the problem using information about interactions or implicit knowledge. Within this scenario, this work proposes a novel approach based on link prediction techniques over graph structures that exclusively considers interactions between users and items to provide recommendations. We present and evaluate two alternative recommendation methods: one item-based and one user-based that apply the edge weight , common neighbours , Jaccard neighbours , Adar/Adamic , and Preferential Attachment link prediction techniques. This approach has two significant advantages, which are the novelty of our proposal. First, it is suitable for minimal knowledge scenarios where explicit data such as ratings or preferences are not available. However, as our evaluation demonstrates, this approach outperforms state-of-the-art techniques using a similar level of interaction knowledge. Second, our approach has another relevant feature regarding one of the most significant concerns in current artificial intelligence research: the recommendation methods presented in this paper are easily interpretable for the users, improving their trust in the recommendations.","[{'authorId': '1410523548', 'name': 'Marta Caro-Martínez'}, {'authorId': '83898594', 'name': 'Guillermo Jiménez-Díaz'}, {'authorId': '1389962946', 'name': 'Juan A. Recio-García'}]",3
618ed65eee6f1e67a60f6213431f4a6cf261c4ff,RecoXplainer: A Library for Development and Offline Evaluation of Explainable Recommender Systems,2022,IEEE Computational Intelligence Magazine,12,0,"Since recommender systems play an important role in our online experience today and are involved in a wide range of decisions, multiple stakeholders are requesting explanations for the corresponding algorithmic predictions. These demands—together with the benefits of explanations (e.g., trust, efficiency, and sometimes even persuasion)—have triggered significant interest from researchers in academia and in industry. Nonetheless, to the best of our knowledge, no comprehensive toolkit for development and evaluation of explainable recommender systems is available to the community yet. Instead, researchers are frequently faced with the challenge of re-implementing prior algorithms when creating and evaluating new approaches. This paper introduces recoXplainer, an easy-to-use, unified and extendable library that supports the development and evaluation of explainable recommender systems. recoXplainer includes several state-of-the-art black box algorithms, model-based and post-hoc explainability techniques, as well as offline evaluation metrics in order to assess the quality of the explanation algorithms.","[{'authorId': '51161531', 'name': 'Ludovik Çoba'}, {'authorId': '144833154', 'name': 'R. Confalonieri'}, {'authorId': '1694617', 'name': 'M. Zanker'}]",3
c82f338abf4a981c8e884dd048dfce927bec9f87,Metrics for Evaluating Explainable Recommender Systems,2023,EXTRAAMAS,5,0,,"[{'authorId': '145539910', 'name': 'J. Hulstijn'}, {'authorId': '51221370', 'name': 'I. Tchappi'}, {'authorId': '2297347', 'name': 'A. Najjar'}, {'authorId': '2060698', 'name': 'Reyhan Aydoğan'}]",4
1be7b168f9921134be97640c474f748a71dc25ad,Hands on Explainable Recommender Systems with Knowledge Graphs,2022,ACM Conference on Recommender Systems,7,26,"The goal of this tutorial is to present the RecSys community with recent advances on explainable recommender systems with knowledge graphs. We will first introduce conceptual foundations, by surveying the state of the art and describing real-world examples of how knowledge graphs are being integrated into the recommendation pipeline, also for the purpose of providing explanations. This tutorial will continue with a systematic presentation of algorithmic solutions to model, integrate, train, and assess a recommender system with knowledge graphs, with particular attention to the explainability perspective. A practical part will then provide attendees with concrete implementations of recommender systems with knowledge graphs, leveraging open-source tools and public datasets; in this part, tutorial participants will be engaged in the design of explanations accompanying the recommendations and in articulating their impact. We conclude the tutorial by analyzing emerging open issues and future directions. Website: https://explainablerecsys.github.io/recsys2022/.","[{'authorId': '2163452228', 'name': 'Giacomo Balloccu'}, {'authorId': '1824224', 'name': 'Ludovico Boratto'}, {'authorId': '40433308', 'name': 'G. Fenu'}, {'authorId': '28922901', 'name': 'Mirko Marras'}]",4
1379d7b6bab33147c2cf4e1e55fc5f73b4a00dcb,Tower Bridge Net (TB-Net): Bidirectional Knowledge Graph Aware Embedding Propagation for Explainable Recommender Systems,2022,IEEE International Conference on Data Engineering,7,24,"Recently, neural networks based models have been widely used for recommender systems (RS). Unfortunately, the existing neural network based RS solutions are often treated as black-boxes, which gain little trust and confidence from users. Thus, there is an increasing demand of explainability. Several explainable recommendation methods have been introduced to RS. However, there is a trade-off between explainability and performance among these methods. In this paper, we propose a novel framework, the Tower Bridge Net (TB-Net), using the proposed bidirectional embedding propagation approach to achieve both superior recommendation and explainability performances. Extensive validation on three public datasets shows that the performance of TB-Net dominates the state-of-the-art models. We quantitatively evaluate the explainability by using numerical metrics and experimentally prove that TB-Net achieves a significant improvement on explainability compared with existing methods. More importantly, TB-Net has been deployed and offers explainable recommendation service for the largest bank in China, Industrial and Commercial Bank of China Limited (ICBC). Results on a billion-scale dataset (1.2 billion nodes and edges) from ICBC show that TB-Net can provide both accurate recommendations and semantic explanations, and is very effective and deployable in practice.","[{'authorId': '2151225434', 'name': 'Shendi Wang'}, {'authorId': '2382824304', 'name': 'Haoyang Li'}, {'authorId': '3151540', 'name': 'Caleb Chen Cao'}, {'authorId': '2141165316', 'name': 'Xiao-Hui Li'}, {'authorId': '101750640', 'name': 'Ng Ngai Fai'}, {'authorId': '2180326741', 'name': 'Jianxin Liu'}, {'authorId': '2111808280', 'name': 'Xun Xue'}, {'authorId': '2115232192', 'name': 'H. Song'}, {'authorId': '2180539149', 'name': 'Jinyu Li'}, {'authorId': '2180234851', 'name': 'Guangye Gu'}, {'authorId': '2593276', 'name': 'Lei Chen'}]",11
0a6f0e30cca5fa3a291a55a4ddff44e569c89e97,Conceptual Modeling of Explainable Recommender Systems: An Ontological Formalization to Guide Their Design and Development,2021,Journal of Artificial Intelligence Research,13,85,"With the increasing importance of e-commerce and the immense variety of products, users need help to decide which ones are the most interesting to them. This is one of the main goals of recommender systems. However, users’ trust may be compromised if they do not understand how or why the recommendation was achieved. Here, explanations are essential to improve user confidence in recommender systems and to make the recommendation useful. 
Providing explanation capabilities into recommender systems is not an easy task as their success depends on several aspects such as the explanation’s goal, the user’s expectation, the knowledge available, or the presentation method. Therefore, this work proposes a conceptual model to alleviate this problem by defining the requirements of explanations for recommender systems. Our goal is to provide a model that guides the development of effective explanations for recommender systems as they are correctly designed and suited to the user’s needs. Although earlier explanation taxonomies sustain this work, our model includes new concepts not considered in previous works. Moreover, we make a novel contribution regarding the formalization of this model as an ontology that can be integrated into the development of proper explanations for recommender systems.","[{'authorId': '1410523548', 'name': 'Marta Caro-Martínez'}, {'authorId': '83898594', 'name': 'Guillermo Jiménez-Díaz'}, {'authorId': '1389962946', 'name': 'Juan A. Recio-García'}]",3
177427d07f2e7b54dbf9657d565cd51734eb0d10,Explainable Recommender Systems via Resolving Learning Representations,2020,International Conference on Information and Knowledge Management,24,57,"Recommender systems play a fundamental role in web applications in filtering massive information and matching user interests. While many efforts have been devoted to developing more effective models in various scenarios, the exploration on the explainability of recommender systems is running behind. Explanations could help improve user experience and discover system defects. In this paper, after formally introducing the elements that are related to model explainability, we propose a novel explainable recommendation model through improving the transparency of the representation learning process. Specifically, to overcome the representation entangling problem in traditional models, we revise traditional graph convolution to discriminate information from different layers. Also, each representation vector is factorized into several segments, where each segment relates to one semantic aspect in data. Different from previous work, in our model, factor discovery and representation learning are simultaneously conducted, and we are able to handle extra attribute information and knowledge. In this way, the proposed model can learn interpretable and meaningful representations for users and items. Unlike traditional methods that need to make a trade-off between explainability and effectiveness, the performance of our proposed explainable model is not negatively affected after considering explainability. Finally, comprehensive experiments are conducted to validate the performance of our model as well as explanation faithfulness.","[{'authorId': '47717322', 'name': 'Ninghao Liu'}, {'authorId': '144143228', 'name': 'Yong Ge'}, {'authorId': '2156060357', 'name': 'Li Li'}, {'authorId': '2283100826', 'name': 'X. Hu'}, {'authorId': '1562383795', 'name': 'Rui Chen'}, {'authorId': '2108645988', 'name': 'Soo-Hyun Choi'}]",6
a1eb6dd5bf57162f365f98717b9fc3882fe94025,Transparency in Fair Machine Learning: the Case of Explainable Recommender Systems,2018,Human and Machine Learning,68,48,,"[{'authorId': '4175388', 'name': 'B. Abdollahi'}, {'authorId': '2423522', 'name': 'O. Nasraoui'}]",2
6b82110ecae66a8bc32ff3f928cd2c1e40294db0,On Explainable Recommender Systems Based on Fuzzy Rule Generation Techniques,2019,International Conference on Artificial Intelligence and Soft Computing,8,21,,"[{'authorId': '2057189914', 'name': 'Tomasz Rutkowski'}, {'authorId': '2127620', 'name': 'Krystian Lapa'}, {'authorId': '145652012', 'name': 'R. Nowicki'}, {'authorId': '1726788', 'name': 'R. Nielek'}, {'authorId': '2455940', 'name': 'K. Grzanek'}]",5
903542ef41d5e33c82aceef692c85437e5dcafe5,Towards a knowledge based Explainable Recommender Systems,2019,International Conference on Big Data and Internet of Things,27,15,"Most current Machine Learning based recommender systems act like black boxes, not offering the user any insight into the system logic or justification for the recommendations. Thus, risking losing trust with users and failing to achieve acceptance. The goal of this work is to improve the explainability of recommender systems by using a knowledge extraction method.","[{'authorId': '2342422936', 'name': 'Amina Samih'}, {'authorId': '9139705', 'name': 'Amina Adadi'}, {'authorId': '50487490', 'name': 'M. Berrada'}]",3
4fbc63683b4d79c53488a8f98aa1e397e89426ed,Knowledge-aware Autoencoders for Explainable Recommender Systems,2018,DLRS@RecSys,42,25,"Recommender Systems have been widely used to help users in finding what they are looking for thus tackling the information overload problem. After several years of research and industrial findings looking after better algorithms to improve accuracy and diversity metrics, explanation services for recommendation are gaining momentum as a tool to provide a human-understandable feedback to results computed, in most of the cases, by black-box machine learning techniques. As a matter of fact, explanations may guarantee users satisfaction, trust, and loyalty in a system. In this paper, we evaluate how different information encoded in a Knowledge Graph are perceived by users when they are adopted to show them an explanation. More precisely, we compare how the use of categorical information, factual one or a mixture of them both in building explanations, affect explanatory criteria for a recommender system. Experimental results are validated through an A/B testing platform which uses a recommendation engine based on a Semantics-Aware Autoencoder to build users profiles which are in turn exploited to compute recommendation lists and to provide an explanation.","[{'authorId': '31614629', 'name': 'Vito Bellini'}, {'authorId': '10668128', 'name': 'Angelo Schiavone'}, {'authorId': '1737962', 'name': 'T. D. Noia'}, {'authorId': '1738932', 'name': 'Azzurra Ragone'}, {'authorId': '1738818', 'name': 'E. Sciascio'}]",5
adb465db0731e405c7c7d7a588cf2ea1e48074e1,PhD position in Explainable Recommender Systems,2020,,0,0,,"[{'authorId': '2024649509', 'name': 'A. Chauvin'}]",1
04304f657d01c09ff0f59f02de1d8e116c296450,An explainable content-based approach for recommender systems: a case study in journal recommendation for paper submission,2024,User Model. User Adapt. Interact.,10,50,"Explainable artificial intelligence is becoming increasingly important in new artificial intelligence developments since it enables users to understand and consequently trust system output. In the field of recommender systems, explanation is necessary not only for such understanding and trust but also because if users understand why the system is making certain suggestions, they are more likely to consume the recommended product. This paper proposes a novel approach for explaining content-based recommender systems by specifically focusing on publication venue recommendation. In this problem, the authors of a new research paper receive recommendations about possible journals (or other publication venues) to which they could submit their article based on content similarity, while the recommender system simultaneously explains its decisions. The proposed explanation ecosystem is based on various elements that support the explanation (topics, related articles, relevant terms, etc.) and is fully integrated with the underlying recommendation model. The proposed method is evaluated through a user study in the biomedical field, where transparency, satisfaction, trust, and scrutability are assessed. The obtained results suggest that the proposed approach is effective and useful for explaining the output of the recommender system to users.","[{'authorId': '2314816151', 'name': 'Luis M. de Campos'}, {'authorId': '1397395901', 'name': 'J. M. Fernández-Luna'}, {'authorId': '1690193', 'name': 'J. Huete'}]",3
4e5385d6d4afd2f2b00a8d5998b6b61339e191f9,Explainable Multi-Stakeholder Job Recommender Systems,2024,ACM Conference on Recommender Systems,4,19,"Public opinion on recommender systems has become increasingly wary in recent years. In line with this trend, lawmakers have also started to become more critical of such systems, resulting in the introduction of new laws focusing on aspects such as privacy, fairness, and explainability for recommender systems and AI at large. These concepts are especially crucial in high-risk domains such as recruitment. In recruitment specifically, decisions carry substantial weight, as the outcomes can significantly impact individuals’ careers and companies’ success. Additionally, there is a need for a multi-stakeholder approach, as these systems are used by job seekers, recruiters, and companies simultaneously, each with its own requirements and expectations. In this paper, I summarize my current research on the topic of explainable, multi-stakeholder job recommender systems and set out a number of future research directions.","[{'authorId': '2187199259', 'name': 'Roan Schellingerhout'}]",1
72986cada4a8269041f1b153b08c51f8707afd91,Towards Explainable Conversational Recommender Systems,2023,Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,21,47,"Explanations in conventional recommender systems have demonstrated benefits in helping the user understand the rationality of the recommendations and improving the system's efficiency, transparency, and trustworthiness. In the conversational environment, multiple contextualized explanations need to be generated, which poses further challenges for explanations. To better measure explainability in CRS, we propose ten evaluation perspectives based on the concepts from conventional recommender systems together with the characteristics of CRS. We assess five existing CRS benchmark datasets using these metrics and observe the necessity of improving the explanation quality of CRS. To achieve this, we conduct manual and automatic approaches to extend these dialogues and construct a new CRS dataset, namely Explainable Recommendation Dialogues (E-ReDial). It includes 756 dialogues with over 2,000 high-quality rewritten explanations. We compare two baseline approaches to perform explanation generation based on E-ReDial. Experimental results suggest that models trained on E-ReDial can significantly improve explainability while introducing knowledge into the models can further improve the performance. GPT-3 in the in-context learning setting can generate more realistic and diverse movie descriptions. In contrast, T5 training on E-Redial can better generate clear reasons for recommendations based on user preferences. E-ReDial is available at https://github.com/Superbooming/E-ReDial.","[{'authorId': '2119112577', 'name': 'Shuyu Guo'}, {'authorId': '2108032328', 'name': 'Shuo Zhang'}, {'authorId': '2153198380', 'name': 'Weiwei Sun'}, {'authorId': '1749477', 'name': 'Pengjie Ren'}, {'authorId': '1721165', 'name': 'Zhumin Chen'}, {'authorId': '2780667', 'name': 'Z. Ren'}]",6
5f7e543af1fa5f5e836e9789335f9b23e576d2bc,Using folk theories of recommender systems to inform human-centered explainable AI (HCXAI),2023,Canadian journal of information and library science,3,0,"This study uses folk theories of the Spotify music recommender system to inform the principles of human-centered explainable AI (HCXAI). The results show that folk theories can reinforce, challenge, and augment these principles facilitating the development of more transparent and explainable recommender systems for the non-expert, lay public.","[{'authorId': '2264939653', 'name': 'Michael Ridley'}]",1
bfc22ef20dee9cbbeabe1363c044cb498e417880,Explainable Meta-Path Based Recommender Systems,2023,Trans. Recomm. Syst.,10,56,"Meta-paths have been popularly used to provide explainability in recommendations. Although long/complicated meta-paths could represent complex user-item connectivity, they are not easy to interpret. This work tackles this problem by introducing a meta-path translation task. The objective is to translate a meta-path to its comparable explainable meta-paths that perform similarly in terms of recommendation but have higher explainability compared to the given one. We propose a definition of meta-path explainability to determine comparable explainable meta-paths and a meta-path grammar that allows comparable explainable meta-paths to be formed in a similar way as sentences in human languages. Based on this grammar, we propose a meta-path translation model, a sequence-to-sequence (Seq2Seq) model to translate a long and complicated meta-path to its comparable explainable meta-paths. Two novel datasets for meta-path translation were generated based on two real-world recommendation datasets. The experiments were conducted on these generated datasets. The results show that our model outperformed state-of-the-art Seq2Seq baselines regarding meta-path translation and maintained a better trade-off between accuracy and diversity/readability in predicting comparable explainable meta-paths. These results indicate that our model can effectively generate a group of explainable meta-paths as alternative explanations for those recommendations based on any given long/complicated meta-path.","[{'authorId': '52196310', 'name': 'Thanet Markchom'}, {'authorId': '2266015545', 'name': 'Huizhi Liang'}, {'authorId': '2248238572', 'name': 'James Ferryman'}]",3
7491eb4fa2cb27daa4dbd3d0ec838cc7b6863692,Scalable and explainable visually-aware recommender systems,2023,Knowledge-Based Systems,14,44,,"[{'authorId': '52196310', 'name': 'Thanet Markchom'}, {'authorId': '34727566', 'name': 'Huizhi Liang'}, {'authorId': '113273965', 'name': 'James T Ferryman'}]",3
63a632afd5027eeb23fb18357c6eec414c4f4818,A Survey of Explainable E-Commerce Recommender Systems,2022,2022 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),2,44,"Since the growing information overload has become more and more serious on the Web, especially in the field of e-commerce. The explainable recommender system plays a crucial role in providing users with explanations of recommendations to enhance customer satisfaction and loyalty. In recent years, various explainable recommender approaches have been proposed and applied in e-commerce. This survey will review the development of explainable recommender systems, existing methods to generate explainable recommendations, applications in the e-commerce field, and further discuss future directions that can be incorporated and implemented to improve the quality of explainable recommender systems.","[{'authorId': '1832848446', 'name': 'Huaqi Gao'}, {'authorId': '2215370366', 'name': 'Shunke Zhou'}]",2
e00e06683dba1b5bda23b8cdb78a31f1d345c018,"Measuring ""Why"" in Recommender Systems: a Comprehensive Survey on the Evaluation of Explainable Recommendation",2022,arXiv.org,38,69,"Explainable recommendation has shown its great advantages for improving recommendation persuasiveness, user satisfaction, system transparency, among others. A fundamental problem of explainable recommendation is how to evaluate the explanations. In the past few years, various evaluation strategies have been proposed. However, they are scattered in different papers, and there lacks a systematic and detailed comparison between them. To bridge this gap, in this paper, we comprehensively review the previous work, and provide different taxonomies for them according to the evaluation perspectives and evaluation methods. Beyond summarizing the previous work, we also analyze the (dis)advantages of existing evaluation methods and provide a series of guidelines on how to select them. The contents of this survey are based on more than 100 papers from top-tier conferences like IJCAI, AAAI, TheWebConf, Recsys, UMAP, and IUI, and their complete summarization are presented at https://shimo.im/sheets/VKrpYTcwVH6KXgdy/MODOC/. With this survey, we finally aim to provide a clear and comprehensive review on the evaluation of explainable recommendation.","[{'authorId': '49795005', 'name': 'Xu Chen'}, {'authorId': '1591136873', 'name': 'Yongfeng Zhang'}, {'authorId': '2113341875', 'name': 'Jingxuan Wen'}]",3
1767850776facae214e643dcb3d1a1aa99aea8c4,"A Comparative Review of Expert Systems, Recommender Systems, and Explainable AI",2022,2022 IEEE 7th International conference for Convergence in Technology (I2CT),10,0,"Previously Expert Systems (ES) dominated Artificial Intelligence (AI) applications and various ES were developed in multiple domains. However, due to knowledge acquisition bottlenecks, these systems fell out of use. With the rise in Machine Learning (ML) and Deep Learning (DL) approaches, another category of systems called Recommender Systems (RS) is now developed for various application domains. As ML/DL systems acted like black boxes, explainable AI (XAI) came into the picture to provide explanations for the recommendations or predictions made. In this paper, we review the architectural similarities and differences between these three approaches along with applications and future directions. It is important to study these to predict the future of RS and any possible resurgence of ES, developments in XAI and application domains.","[{'authorId': '152866889', 'name': 'M. Ravi'}, {'authorId': '1728262', 'name': 'A. Negi'}, {'authorId': '2155000143', 'name': 'Sanjay Chitnis'}]",3
21be2210c4a4de7a9ce0cb59e4d0fc28921d68a5,Leveraging ChatGPT for Automated Human-centered Explanations in Recommender Systems,2024,International Conference on Intelligent User Interfaces,33,24,"The adoption of recommender systems (RSs) in various domains has become increasingly popular, but concerns have been raised about their lack of transparency and interpretability. While significant advancements have been made in creating explainable RSs, there is still a shortage of automated approaches that can deliver meaningful and contextual human-centered explanations. Numerous researchers have evaluated explanations based on human-generated recommendations and explanations to address this gap. However, such approaches do not scale for real-world systems. Building on recent research that exploits Large Language Models (LLMs) for RSs, we propose leveraging the conversational capabilities of ChatGPT to provide users with personalized, human-like, and meaningful explanations for recommended items. Our paper presents one of the first user studies that measure users’ perceptions of ChatGPT-generated explanations while acting as an RS. Regarding recommendations, we assess whether users prefer ChatGPT over random (but popular) recommendations. Concerning explanations, we assess users’ perceptions of personalization, effectiveness, and persuasiveness. Our findings reveal that users tend to prefer ChatGPT-generated recommendations over popular ones. Additionally, personalized rather than generic explanations prove to be more effective when the recommended item is unfamiliar.","[{'authorId': '2269452111', 'name': 'Ítallo Silva'}, {'authorId': '2159497665', 'name': 'L. Marinho'}, {'authorId': '2186119813', 'name': 'Alan Said'}, {'authorId': '1918235', 'name': 'M. Willemsen'}]",4
c25dd5b15f2225322148c0e29d406c9957f72674,"A Systematic Review of Deep Knowledge Graph-Based Recommender Systems, with Focus on Explainable Embeddings",2022,International Conference on Data Technologies and Applications,8,93,"Recommender systems (RS) have been developed to make personalized suggestions and enrich users’ preferences in various online applications to address the information explosion problems. However, traditional recommender-based systems act as black boxes, not presenting the user with insights into the system logic or reasons for recommendations. Recently, generating explainable recommendations with deep knowledge graphs (DKG) has attracted significant attention. DKG is a subset of explainable artificial intelligence (XAI) that utilizes the strengths of deep learning (DL) algorithms to learn, provide high-quality predictions, and complement the weaknesses of knowledge graphs (KGs) in the explainability of recommendations. DKG-based models can provide more meaningful, insightful, and trustworthy justifications for recommended items and alleviate the information explosion problems. Although several studies have been carried out on RS, only a few papers have been published on DKG-based methodologies, and a review in this new research direction is still insufficiently explored. To fill this literature gap, this paper uses a systematic literature review framework to survey the recently published papers from 2018 to 2022 in the landscape of DKG and XAI. We analyze how the methods produced in these papers extract essential information from graph-based representations to improve recommendations’ accuracy, explainability, and reliability. From the perspective of the leveraged knowledge-graph related information and how the knowledge-graph or path embeddings are learned and integrated with the DL methods, we carefully select and classify these published works into four main categories: the Two-stage explainable learning methods, the Joint-stage explainable learning methods, the Path-embedding explainable learning methods, and the Propagation explainable learning methods. We further summarize these works according to the characteristics of the approaches and the recommendation scenarios to facilitate the ease of checking the literature. We finally conclude by discussing some open challenges left for future research in this vibrant field.","[{'authorId': '70326112', 'name': 'R. Doh'}, {'authorId': '2110616186', 'name': 'Conghua Zhou'}, {'authorId': '2059615608', 'name': 'John Kingsley Arthur'}, {'authorId': '2379399679', 'name': 'Isaac Tawiah'}, {'authorId': '1404440022', 'name': 'Benjamin Doh'}]",5
229ac921a1ef8a9683821f024a45163f6dcfda94,Expressive Latent Feature Modelling for Explainable Matrix Factorisation-based Recommender Systems,2022,ACM Trans. Interact. Intell. Syst.,3,43,"The traditional matrix factorisation (MF)-based recommender system methods, despite their success in making the recommendation, lack explainable recommendations as the produced latent features are meaningless and cannot explain the recommendation. This article introduces an MF-based explainable recommender system framework that utilises the user-item rating data and the available item information to model meaningful user and item latent features. These features are exploited to enhance the rating prediction accuracy and the recommendation explainability. Our proposed feature-based explainable recommender system framework utilises these meaningful user and item latent features to explain the recommendation without relying on private or outer data. The recommendations are explained to the user using text message and bar chart. Our proposed model has been evaluated in terms of the rating prediction accuracy and the reasonableness of the explanation using six real-world benchmark datasets for movies, books, video games, and fashion recommendation systems. The results show that the proposed model can produce accurate explainable recommendations.","[{'authorId': '114468283', 'name': 'Abdullah Alhejaili'}, {'authorId': '2164040526', 'name': 'Shaheen Fatima'}]",2
32566e07da8ae95325238eea068ed8c7fe01578a,RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems,2024,The Web Conference,16,6,"This paper introduces RecAI, a practical toolkit designed to augment or even revolutionize recommender systems with the advanced capabilities of Large Language Models (LLMs). RecAI provides a suite of tools, including Recommender AI Agent, Recommendation-oriented Language Models, Knowledge Plugin, RecExplainer, and Evaluator, to facilitate the integration of LLMs into recommender systems from multifaceted perspectives. The new generation of recommender systems, empowered by LLMs, are expected to be more versatile, explainable, conversational, and controllable, paving the way for more intelligent and user-centric recommendation experiences. We hope the open-source of RecAI can help accelerate evolution of new advanced recommender systems. The source code of RecAI is available at https://github.com/microsoft/RecAI.","[{'authorId': '2813328', 'name': 'Jianxun Lian'}, {'authorId': '2226475380', 'name': 'Yuxuan Lei'}, {'authorId': '2236672137', 'name': 'Xu Huang'}, {'authorId': '2237129499', 'name': 'Jing Yao'}, {'authorId': '2267336219', 'name': 'Wei Xu'}, {'authorId': '2261275112', 'name': 'Xing Xie'}]",6
491d4e2f0449f1dfd377a8936f2ccb01a17e6f56,Recommender Systems: An Explainable AI Perspective,2021,International Symposium on INnovations in Intelligent SysTems and Applications,31,0,"In recent years, in the era of information overload development, the need for recommender systems that make personalized suggestion systems has become a very exciting field for researchers. To develop models that generate high-quality recommendations, the explainable recommendation has been introduced, proposing to develop intuitive and trustworthy explanations. The problem that the explainable recommendation wants to solve is to let people understand why certain elements rather than other are recommended by the system. This paper briefly overviews the short history of explainable AI and then it presents its role and applicability in the domain of recommender systems. Our work contributes to understanding the concept of explainable recommendation and what it should accomplish to increase its acceptability and to enable its accurate evaluation.","[{'authorId': '2091486093', 'name': 'Alexandra Vultureanu-Albişi'}, {'authorId': '1740341', 'name': 'C. Bǎdicǎ'}]",2
0cfdd655100055f234fd23ebecd915504b8e00e3,Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System,2023,arXiv.org,342,28,"Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks. However, traditional recommender systems continue to face great challenges such as poor interactivity and explainability, which actually also hinder their broad deployment in real-world systems. To address these limitations, this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augments LLMs for building conversational recommender systems by converting user profiles and historical interactions into prompts. Chat-Rec is demonstrated to be effective in learning user preferences and establishing connections between users and products through in-context learning, which also makes the recommendation process more interactive and explainable. What's more, within the Chat-Rec framework, user's preferences can transfer to different products for cross-domain recommendations, and prompt-based injection of information into LLMs can also handle the cold-start scenarios with new items. In our experiments, Chat-Rec effectively improve the results of top-k recommendations and performs better in zero-shot rating prediction task. Chat-Rec offers a novel approach to improving recommender systems and presents new practical scenarios for the implementation of AIGC (AI generated content) in recommender system studies.","[{'authorId': '1390781327', 'name': 'Yunfan Gao'}, {'authorId': '2169937292', 'name': 'Tao Sheng'}, {'authorId': '2152318443', 'name': 'Youlin Xiang'}, {'authorId': '2212411539', 'name': 'Yun Xiong'}, {'authorId': '2256769434', 'name': 'Haofen Wang'}, {'authorId': '2144138716', 'name': 'Jiawei Zhang'}]",6
c4318ad89e8fe85674c2f6e10fe2d2446b7c7578,User Feedback in Controllable and Explainable Social Recommender Systems: a Linguistic Analysis,2020,IntRS@RecSys,5,60,,"[{'authorId': '40166850', 'name': 'Chun-Hua Tsai'}, {'authorId': '1804693', 'name': 'Peter Brusilovsky'}]",2
413a87342b5a7dfb1ba2877549aad80e338f89da,"AdaReX: Cross-Domain, Adaptive, and Explainable Recommender System",2023,SIGIR-AP,5,43,"Explainability is an inherent issue of recommender systems and has received a lot of attention recently. Generative explainable recommendation, which provides personalized explanations by generating textual rationales, is emerging as an effective solution. Despite promising, current methods face limitations in their reliance on dense training data, which hinders the generalizability of explainable recommender systems. Our work tackles a novel problem of cross-domain explainable recommendation aiming to extend the generalizability of explainable recommender systems. To solve this, we propose a novel approach that models aspects extracted from past reviews, to empower the explainable recommender systems by leveraging knowledge from other domains. Specifically, we propose AdaReX (Adaptive eXplainable Recommendation), to model auxiliary and target domains simultaneously. By performing specific tasks in respective domains and their interconnection via a discriminator model, AdaReX allows the aspect sequences to learn common knowledge across different domains and tasks. Furthermore, through our proposed optimization objective, the learning of aspect sequence is deeply cross-interacted with in-domain users and items’ latent factors, enabling the enhanced sharing of knowledge between domains. Our extensive experiments on real datasets demonstrate that our approach not only generates better explanations and recommendations for sparse users but also improves performance for general users.","[{'authorId': '2268284436', 'name': 'Yi Yu'}, {'authorId': '2268098628', 'name': 'Kazunari Sugiyama'}, {'authorId': '2239340856', 'name': 'Adam Jatowt'}]",3
1aeb8cce292705824987983a20dd29584f722749,Explainable Recommender With Geometric Information Bottleneck,2023,IEEE Transactions on Knowledge and Data Engineering,2,57,"Explainable recommender systems can explain their recommendation decisions, enhancing user trust in the systems. Most explainable recommender systems either rely on human-annotated rationales to train models for explanation generation or leverage the attention mechanism to extract important text spans from reviews as explanations. The extracted rationales are often confined to an individual review and may fail to identify the implicit features beyond the review text. To avoid the expensive human annotation process and to generate explanations beyond individual reviews, we propose to incorporate a geometric prior learnt from user-item interactions into a variational network which infers latent factors from user-item reviews. The latent factors from an individual user-item pair can be used for both recommendation and explanation generation, which naturally inherit the global characteristics encoded in the prior knowledge. Experimental results on three e-commerce datasets show that our model significantly improves the interpretability of a variational recommender using the Wasserstein distance while achieving performance comparable to existing content-based recommender systems in terms of recommendation behaviours.","[{'authorId': '1830443015', 'name': 'Hanqi Yan'}, {'authorId': '145096580', 'name': 'Lin Gui'}, {'authorId': '2381913941', 'name': 'Menghan Wang'}, {'authorId': '2156563711', 'name': 'Kun Zhang'}, {'authorId': '1390509967', 'name': 'Yulan He'}]",5
0f2d11ac3202997b914efafaedb56b564ff9d3a7,Leveraging Large Language Models in Conversational Recommender Systems,2023,arXiv.org,110,118,"A Conversational Recommender System (CRS) offers increased transparency and control to users by enabling them to engage with the system through a real-time multi-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to converse naturally and incorporate world knowledge and common-sense reasoning into language understanding, unlocking the potential of this paradigm. However, effectively leveraging LLMs within a CRS introduces new technical challenges, including properly understanding and controlling a complex conversation and retrieving from external sources of information. These issues are exacerbated by a large, evolving item corpus and a lack of conversational data for training. In this paper, we provide a roadmap for building an end-to-end large-scale CRS using LLMs. In particular, we propose new implementations for user preference understanding, flexible dialogue management and explainable recommendations as part of an integrated architecture powered by LLMs. For improved personalization, we describe how an LLM can consume interpretable natural language user profiles and use them to modulate session-level context. To overcome conversational data limitations in the absence of an existing production CRS, we propose techniques for building a controllable LLM-based user simulator to generate synthetic conversations. As a proof of concept we introduce RecLLM, a large-scale CRS for YouTube videos built on LaMDA, and demonstrate its fluency and diverse functionality through some illustrative example conversations.","[{'authorId': '2217253911', 'name': 'Luke Friedman'}, {'authorId': '2416263', 'name': 'Sameer Ahuja'}, {'authorId': '2217251792', 'name': 'David Allen'}, {'authorId': '6840539', 'name': 'Zhenning Tan'}, {'authorId': '39041127', 'name': 'Hakim Sidahmed'}, {'authorId': '2217232763', 'name': 'Changbo Long'}, {'authorId': '2109935759', 'name': 'Jun Xie'}, {'authorId': '2370193442', 'name': 'Gabriel Schubiner'}, {'authorId': '2109171018', 'name': 'Ajay Patel'}, {'authorId': '2196537401', 'name': 'Harsh Lara'}, {'authorId': '2055638940', 'name': 'Brian Chu'}, {'authorId': '2211981539', 'name': 'Zexiang Chen'}, {'authorId': '2074227766', 'name': 'Manoj Tiwari'}]",13
4cc1093e13fb03aa844e9d2e9d8793d9c10f9453,Neural Explainable Collective Non-negative Matrix Factorization for Recommender Systems,2018,International Conference on Web Information Systems and Technologies,2,20,"Explainable recommender systems aim to generate explanations for users according to their predicted scores, the user’s history and their similarity to other users. Recently, researchers have proposed explainable recommender models using topic models and sentiment analysis methods providing explanations based on user’s reviews. However, such methods have neglected improvements in natural language processing, even if these methods are known to improve user satisfaction. In this paper, we propose a neural explainable collective nonnegative matrix factorization (NECoNMF) to predict ratings based on users’ feedback, for example, ratings and reviews. To do so, we use collective non-negative matrix factorization to predict user preferences according to different features and a natural language model to explain the prediction. Empirical experiments were conducted in two datasets, showing the model’s efficiency for predicting ratings and generating explanations. The results present that NECoNMF improves the accuracy for explainable recommendations in comparison with the state-of-art method in 18.3% for NDCG@5, 12.2% for HitRatio@5, 17.1% for NDCG@10, and 12.2% for HitRatio@10 in the Yelp dataset. A similar performance has been observed in the Amazon dataset 7.6% for NDCG@5, 1.3% for HitRatio@5, 7.9% for NDCG@10, and 3.9% for HitRatio@10.","[{'authorId': '145331063', 'name': 'F. Costa'}, {'authorId': '1805892', 'name': 'Peter Dolog'}]",2
afbd4b202dcd599a6906bfc0dd219b5abbf9f0b8,The Dark Side of Explanations: Poisoning Recommender Systems with Counterfactual Examples,2023,Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,28,33,"Deep learning-based recommender systems have become an integral part of several online platforms. However, their black-box nature emphasizes the need for explainable artificial intelligence (XAI) approaches to provide human-understandable reasons why a specific item gets recommended to a given user. One such method is counterfactual explanation (CF). While CFs can be highly beneficial for users and system designers, malicious actors may also exploit these explanations to undermine the system's security. In this work, we propose H-CARS, a novel strategy to poison recommender systems via CFs. Specifically, we first train a logical-reasoning-based surrogate model on training data derived from counterfactual explanations. By reversing the learning process of the recommendation model, we thus develop a proficient greedy algorithm to generate fabricated user profiles and their associated interaction records for the aforementioned surrogate model. Our experiments, which employ a well-known CF generation method and are conducted on two distinct datasets, show that H-CARS yields significant and successful attack performance.","[{'authorId': '2157281262', 'name': 'Ziheng Chen'}, {'authorId': '2192306989', 'name': 'Fabrizio Silvestri'}, {'authorId': '2144547216', 'name': 'Jia Wang'}, {'authorId': '1591136873', 'name': 'Yongfeng Zhang'}, {'authorId': '2651748', 'name': 'Gabriele Tolomei'}]",5
56cbac15c07f6a52b0bdd59c1d2f968aaea52395,Recommender systems for sustainability: overview and research issues,2023,Frontiers Big Data,24,133,"Sustainability development goals (SDGs) are regarded as a universal call to action with the overall objectives of planet protection, ending of poverty, and ensuring peace and prosperity for all people. In order to achieve these objectives, different AI technologies play a major role. Specifically, recommender systems can provide support for organizations and individuals to achieve the defined goals. Recommender systems integrate AI technologies such as machine learning, explainable AI (XAI), case-based reasoning, and constraint solving in order to find and explain user-relevant alternatives from a potentially large set of options. In this article, we summarize the state of the art in applying recommender systems to support the achievement of sustainability development goals. In this context, we discuss open issues for future research.","[{'authorId': '2264307908', 'name': 'Alexander Felfernig'}, {'authorId': '2097794289', 'name': 'Manfred Wundara'}, {'authorId': '13278227', 'name': 'Thi Ngoc Trang Tran'}, {'authorId': '2135115901', 'name': 'Seda Polat-Erdeniz'}, {'authorId': '2182737348', 'name': 'Sebastian Lubos'}, {'authorId': '2264438563', 'name': 'Merfat El Mansi'}, {'authorId': '2233477799', 'name': 'Damian Garber'}, {'authorId': '144335284', 'name': 'Viet-Man Le'}]",8
f6e936ee147639b3d694e076896ea6756fcc399e,Interactive Explanation with Varying Level of Details in an Explainable Scientific Literature Recommender System,2023,International journal of human computer interactions,16,86,"Abstract Explainable recommender systems (RS) have traditionally followed a one-size-fits-all approach, delivering the same explanation level of detail to each user, without considering their individual needs and goals. Further, explanations in RS have so far been presented mostly in a static and non-interactive manner. To fill these research gaps, we aim in this paper to adopt a user-centered, interactive explanation model that provides explanations with different levels of detail and empowers users to interact with, control, and personalize the explanations based on their needs and preferences. We followed a user-centered approach to design interactive explanations with three levels of detail (basic, intermediate, and advanced) and implemented them in the transparent Recommendation and Interest Modeling Application (RIMA). We conducted a qualitative user study (N = 14) to investigate the impact of providing interactive explanations with varying level of details on the users’ perception of the explainable RS. Our study showed qualitative evidence that fostering interaction and giving users control in deciding which explanation they would like to see can meet the demands of users with different needs, preferences, and goals, and consequently can have positive effects on different crucial aspects in explainable recommendation, including transparency, trust, satisfaction, and user experience.","[{'authorId': '1729232457', 'name': 'Mouadh Guesmi'}, {'authorId': '1724546', 'name': 'Mohamed Amine Chatti'}, {'authorId': '2114534153', 'name': 'Shoeb Joarder'}, {'authorId': '2167427642', 'name': 'Qurat Ul Ain'}, {'authorId': '2023361368', 'name': 'R. Alatrash'}, {'authorId': '2214581453', 'name': 'Clara Siepmann'}, {'authorId': '2219861226', 'name': 'Tannaz Vahidi'}]",7
b20c87e2925a81f57d9c913423e9e74c14de5341,A Survey on Trustworthy Recommender Systems,2022,Trans. Recomm. Syst.,90,372,"Recommender systems (RS), serving at the forefront of Human-centered AI, are widely deployed in almost every corner of the web and facilitate the human decision-making process. However, despite their enormous capabilities and potential, RS may also lead to undesired effects on users, items, producers, platforms, or even the society at large, such as compromised user trust due to non-transparency, unfair treatment of different consumers, or producers, privacy concerns due to extensive use of user’s private data for personalization, just to name a few. All of these create an urgent need for Trustworthy Recommender Systems (TRS) so as to mitigate or avoid such adverse impacts and risks. In this survey, we will introduce techniques related to trustworthy recommendation, including but not limited to explainable recommendation, fairness in recommendation, privacy-aware recommendation, robustness in recommendation, user-controllable recommendation, as well as the relationship between these different perspectives in terms of trustworthy recommendation. Through this survey, we hope to deliver readers with a comprehensive view of the research area and raise attention to the community about the importance, existing research achievements, and future research directions on trustworthy recommendation.","[{'authorId': '152988336', 'name': 'Yingqiang Ge'}, {'authorId': '50152132', 'name': 'Shuchang Liu'}, {'authorId': '2011378', 'name': 'Zuohui Fu'}, {'authorId': '2110449137', 'name': 'Juntao Tan'}, {'authorId': '2109968285', 'name': 'Zelong Li'}, {'authorId': '2111044480', 'name': 'Shuyuan Xu'}, {'authorId': '48515097', 'name': 'Yunqi Li'}, {'authorId': '2885287', 'name': 'Yikun Xian'}, {'authorId': '2145038716', 'name': 'Yongfeng Zhang'}]",9
41f052ccaa811731f68f0abbef2c8a978a374f83,"Post Processing Recommender Systems with Knowledge Graphs for Recency, Popularity, and Diversity of Explanations",2022,Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,42,41,"Existing explainable recommender systems have mainly modeled relationships between recommended and already experienced products, and shaped explanation types accordingly (e.g., movie ""x"" starred by actress ""y"" recommended to a user because that user watched other movies with ""y"" as an actress). However, none of these systems has investigated the extent to which properties of a single explanation (e.g., the recency of interaction with that actress) and of a group of explanations for a recommended list (e.g., the diversity of the explanation types) can influence the perceived explaination quality. In this paper, we conceptualized three novel properties that model the quality of the explanations (linking interaction recency, shared entity popularity, and explanation type diversity) and proposed re-ranking approaches able to optimize for these properties. Experiments on two public data sets showed that our approaches can increase explanation quality according to the proposed properties, fairly across demographic groups, while preserving recommendation utility. The source code and data are available at https://github.com/giacoballoccu/explanation-quality-recsys.","[{'authorId': '2163452228', 'name': 'Giacomo Balloccu'}, {'authorId': '1824224', 'name': 'Ludovico Boratto'}, {'authorId': '40433308', 'name': 'G. Fenu'}, {'authorId': '28922901', 'name': 'Mirko Marras'}]",4
d39b71bd3bcf53e12129323f8521f25ed604efab,A survey on effects of adding explanations to recommender systems,2022,Concurrency and Computation,24,64,"Explainable recommendations become essential when we need to improve the performance of recommendations and to increase user confidence. Explanations are effective when end users can build a complete and correct mental representation of the inferential process of a recommender system. This paper presents our view on the background regarding the implications of explainability applied to recommender systems. Our work contributes to the better understanding of the concept of explainable recommendation and it offers a broader picture of the development of further research in this field. Additionally, we contribute by providing a better understanding of the concept of human‐centered evaluation of explainable recommender systems.","[{'authorId': '2091486093', 'name': 'Alexandra Vultureanu-Albişi'}, {'authorId': '1740341', 'name': 'C. Bǎdicǎ'}]",2
d4bec7afedeb15bd1a3b45e369a86c8eeebbe384,A Multi-Dimensional Conceptualization Framework for Personalized Explanations in Recommender Systems 11-23,2022,IUI Workshops,8,47,,"[{'authorId': '2167427642', 'name': 'Qurat Ul Ain'}, {'authorId': '2163104962', 'name': 'Mohamed Anime Chati'}, {'authorId': '1729232457', 'name': 'Mouadh Guesmi'}, {'authorId': '2114534153', 'name': 'Shoeb Joarder'}]",4
f6e64ef1094c9a6d546ca05b431556521d38b689,Addressing Hiccups in Conversations with Recommender Systems,2022,Conference on Designing Interactive Systems,5,89,"Conversational Agents (CAs) employing voice as their main interaction mode produce natural language utterances with the aim of mimicking human conversations. To unveil hiccups in conversations with recommender systems, we observed users interacting with CAs. Our findings suggest that those occur as users struggle to start the session, as CAs do not appear exploratory, and as CAs remained silent after offering recommendation(s) or after reporting errors. Users enacted mental models derived from years of experience with Graphical User Interfaces, but also expected human-like characteristics such as explanations and proactivity. Anchoring on these, we designed a dialogue model for a multimodal Conversational Recommender System (CRS) mimicking humans and GUIs. We probed the state of hiccups further with a Wizard-of-Oz prototype implementing this dialogue model. Our findings suggest that participants rapidly adopted GUI mimicries, cooperated for error resolution, appreciated explainable recommendations, and provided insights to improve persisting hiccups in proactivity and navigation. Based on these, we provide implications for design to address hiccups in CRS.","[{'authorId': '1678382', 'name': 'Sruthi Viswanathan'}, {'authorId': '1811419139', 'name': 'Fabien Guillot'}, {'authorId': '10741225', 'name': 'Minsuk Chang'}, {'authorId': '2167612', 'name': 'A. Grasso'}, {'authorId': '2822140', 'name': 'J. Renders'}]",5
9ef432c5432c98587af495b819a594f1c4989711,A Novel Explainable and Health-aware Food Recommender System,2022,International Conference on Knowledge Discovery and Information Retrieval,3,21,": Food recommendation systems are increasingly being used by online food services to make recommendations. Health factors are often ignored in most of these systems, despite the fact that unhealthy diets are connected to a wide range of non-communicable diseases. Furthermore, if users do not receive compelling explanations about the recommended healthy foods, they may become hesitant to try them. In this paper, a novel explainable and health-aware food recommender system is developed to address these challenges. For this purpose, user’s preferences and food health factors are taken into account simultaneously and then a rule-based mechanism is employed for ﬁnal healthy and explainable recommendations. Five performance metrics were used to compare our system with different new recommender systems. Using a dataset crawled from ”Allrecipes.com”, the proposed model is shown to perform best.","[{'authorId': '2189179175', 'name': 'Merhrdad Rostami'}, {'authorId': '2189179927', 'name': 'Vahid Farahi'}, {'authorId': '52024680', 'name': 'K. Berahmand'}, {'authorId': '3103483', 'name': 'Saman Forouzandeh'}, {'authorId': '1901767', 'name': 'S. Ahmadian'}, {'authorId': '2051449930', 'name': 'M. Oussalah'}]",6
4da28c66713a5038114911b387b549b7ce9e0090,A Survey on LLM-powered Agents for Recommender Systems,2025,arXiv.org,15,48,"Recommender systems are essential components of many online platforms, yet traditional approaches still struggle with understanding complex user preferences and providing explainable recommendations. The emergence of Large Language Model (LLM)-powered agents offers a promising approach by enabling natural language interactions and interpretable reasoning, potentially transforming research in recommender systems. This survey provides a systematic review of the emerging applications of LLM-powered agents in recommender systems. We identify and analyze three key paradigms in current research: (1) Recommender-oriented approaches, which leverage intelligent agents to enhance the fundamental recommendation mechanisms; (2) Interaction-oriented approaches, which facilitate dynamic user engagement through natural dialogue and interpretable suggestions; and (3) Simulation-oriented approaches, which employ multi-agent frameworks to model complex user-item interactions and system dynamics. Beyond paradigm categorization, we analyze the architectural foundations of LLM-powered recommendation agents, examining their essential components: profile construction, memory management, strategic planning, and action execution. Our investigation extends to a comprehensive analysis of benchmark datasets and evaluation frameworks in this domain. This systematic examination not only illuminates the current state of LLM-powered agent recommender systems but also charts critical challenges and promising research directions in this transformative field.","[{'authorId': '152405445', 'name': 'Qiyao Peng'}, {'authorId': '2108972050', 'name': 'Hongtao Liu'}, {'authorId': '2334596658', 'name': 'Hua Huang'}, {'authorId': '2260805189', 'name': 'Qing Yang'}, {'authorId': '19066746', 'name': 'Minglai Shao'}]",5
960f194d1b129843de4a99c5eea5a535f142fcef,A Survey on Reinforcement Learning for Recommender Systems,2021,IEEE Transactions on Neural Networks and Learning Systems,53,206,"Recommender systems have been widely applied in different real-life scenarios to help us find useful information. In particular, reinforcement learning (RL)-based recommender systems have become an emerging research topic in recent years, owing to the interactive nature and autonomous learning ability. Empirical results show that RL-based recommendation methods often surpass supervised learning methods. Nevertheless, there are various challenges in applying RL in recommender systems. To understand the challenges and relevant solutions, there should be a reference for researchers and practitioners working on RL-based recommender systems. To this end, we first provide a thorough overview, comparisons, and summarization of RL approaches applied in four typical recommendation scenarios, including interactive recommendation, conversational recommendation, sequential recommendation, and explainable recommendation. Furthermore, we systematically analyze the challenges and relevant solutions on the basis of existing literature. Finally, under discussion for open issues of RL and its limitations of recommender systems, we highlight some potential research directions in this field.","[{'authorId': '113310851', 'name': 'Y. Lin'}, {'authorId': '2144385006', 'name': 'Yong Liu'}, {'authorId': '145957468', 'name': 'Fan Lin'}, {'authorId': '2240533680', 'name': 'Lixin Zou'}, {'authorId': '2111194421', 'name': 'Pengcheng Wu'}, {'authorId': '3265605', 'name': 'Wenhua Zeng'}, {'authorId': '2257328854', 'name': 'Huanhuan Chen'}, {'authorId': '1679209', 'name': 'C. Miao'}]",8
e77ca077480349e8f8cb2a12c9e64e724b4001bd,From Intrinsic to Counterfactual: On the Explainability of Contextualized Recommender Systems,2021,arXiv.org,15,51,"With the prevalence of deep learning based embedding approaches, recommender systems have become a proven and indispensable tool in various information filtering applications. However, many of them remain difficult to diagnose what aspects of the deep models' input drive the final ranking decision, thus, they cannot often be understood by human stakeholders. In this paper, we investigate the dilemma between recommendation and explainability, and show that by utilizing the contextual features (e.g., item reviews from users), we can design a series of explainable recommender systems without sacrificing their performance. In particular, we propose three types of explainable recommendation strategies with gradual change of model transparency: whitebox, graybox, and blackbox. Each strategy explains its ranking decisions via different mechanisms: attention weights, adversarial perturbations, and counterfactual perturbations. We apply these explainable models on five real-world data sets under the contextualized setting where users and items have explicit interactions. The empirical results show that our model achieves highly competitive ranking performance, and generates accurate and effective explanations in terms of numerous quantitative metrics and qualitative visualizations.","[{'authorId': '2110362600', 'name': 'Yao Zhou'}, {'authorId': None, 'name': 'Haonan Wang'}, {'authorId': '31108652', 'name': 'Jingrui He'}, {'authorId': '2109590665', 'name': 'Haixun Wang'}]",4
baf0bc1609a866c226e8ec8cbefbf399ca489910,Conversational review-based explanations for recommender systems: Exploring users’ query behavior,2021,International Conference on Conversational User Interfaces,19,69,"Providing explanations based on user reviews in recommender systems (RS) can increase users’ perception of system transparency. While static explanations are dominant, interactive explanatory approaches have emerged in explainable artificial intelligence (XAI), so that users are more likely to examine system decisions and get more arguments supporting system assertions. However, little attention has been paid to conversational approaches for explanations targeting end users. In this paper we explore how to design a conversational interface to provide explanations in a review-based RS, and present the results of a Wizard of Oz (WoOz) study that provided insights into the type of questions users might ask in such a context, as well as their perception of a system simulating such a dialog. Consequently, we propose a dialog management policy and user intents for explainable review-based RS, taking as an example the hotels domain.","[{'authorId': '1573207580', 'name': 'Diana C. Hernandez-Bocanegra'}, {'authorId': '145758499', 'name': 'J. Ziegler'}]",2
3986307b4a015ad4f6eac004d28e3bf4c77480db,AgroXAI: Explainable AI-Driven Crop Recommendation System for Agriculture 4.0,2024,BigData Congress [Services Society],7,29,"Today, crop diversification in agriculture is a critical issue to meet the increasing demand for food and to improve food safety and quality. This issue is considered to be the most important challenge for the next generation of agriculture due to diminishing natural resources, limited arable land and unpredictable climatic conditions caused by climate change. In this paper, we employ emerging technologies such as the Internet of Things (IoT), machine learning (ML) and explainable artificial intelligence (XAI) to improve operational efficiency and productivity in the agricultural sector. Specifically, we propose an edge computing-based explainable crop recommendation system, AgroXAI, which suggests suitable crops for a region based on weather and soil conditions. In this system, we provide local and global explanations of ML model decisions with methods such as ELI5, LIME, SHAP, which we integrate into ML models. More importantly, we provide regional alternative crop recommendations with the Counterfactual explainability method. In this way, we envision that our proposed AgroXAI system will be a platform that provides regional crop diversity in the next generation agriculture.","[{'authorId': '2233329512', 'name': 'Özlem Turgut'}, {'authorId': '35305688', 'name': 'Ibrahim Kok'}, {'authorId': '2056426673', 'name': 'Suat Özdemi̇r'}]",3
596be14ea711f225964cf760d3f7453be433c057,Enhancing Agricultural Decision-Making through an Explainable AI-Based Crop Recommendation System,2024,2024 International Conference on Signal Processing and Advance Research in Computing (SPARC),2,19,"Agriculture is essential for maintaining food security and promoting economic sustainability; however, farmers frequently encounter difficulties in choosing the most appropriate crops for their land due to diverse environmental, soil, and market conditions. This paper proposes an innovative crop recommendation system powered by Explainable Artificial Intelligence (XAI), designed to enhance agricultural decision-making. The system utilizes machine-learning models to analyze diverse data inputs—such as soil properties, weather conditions, and market trends—to recommend optimal crops for specific regions. What sets this approach apart is its explainability: the XAI framework provides transparent insights into how and why certain recommendations are made, enabling farmers to trust and understand the decision-making process. The incorporation of eXplainable AI into crop recommendation systems has transformed agriculture, facilitating data-driven decision-making that leads to improved crop production and more efficient resource management. However, these models’ lack of transparency and interpretability frequently constrains their practical implementation. We explore XAI based approaches, such as LIME and SHAP, to interpret model outputs and highlight key features influencing predictions. Our experiments demonstrate that XAI improves the transparency of ML models and aids in refining model performance through informed feature selection and model adjustments. Our proposed models achieve the highest accuracy $\mathbf{9 9. 3 9 \%}$ using the KNN algorithm and BiLSTM model achieve the $\mathbf{9 5. 9 1 \%}$ accuracy.","[{'authorId': '2310318563', 'name': 'Surendra Kumar'}, {'authorId': '2249066577', 'name': 'Mohit Kumar'}]",2
938a96df8752749057746741fc7f40986d275bf8,Smart Crop Recommendation System Using Machine Learning and Explainable AI,2024,2024 27th International Conference on Computer and Information Technology (ICCIT),0,20,"In many countries of the world, especially in South Asian countries, most of the people's livelihood and food habits are based on agriculture, so it is very important to produce different types of crops profitably and in large quantities. However many farmers are still using primitive methods of cropping and do not understand exactly which crop is best for which land at which time due to climate variability, soil degradation, and environmental instability. The AI-based automated system is a crying need for the farmers for this problem. This study proposes a smart crop recommendation model using Machine Learning (ML) and Explainable AI to help farmers make data-driven decisions on what crops are ideal to grow. This investigation utilizes a standard crop recommendation dataset from Kaggle, which includes key soil characteristics such as nitrogen, phosphorus, potassium, and pH level; and climate parameters including temperature, humidity, and rainfall. The proposed work makes the best use of several machine learning algorithms including Support Vector Machine, Decision Tree, Random Forest, Gaussian Naive Bayes, and a boosting ensemble classifier called Adaptive Boosting (AdaBoost). Among these models, AdaBoost reveals the best performance of 99.35% with a Decision Tree oriented base learner ID3. In addition, the SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME) from explainable AI are used to detect the responsible soil and climate features for the classification of the AdaBoost ensemble model.","[{'authorId': '2366236444', 'name': 'Syed Imran Ertaza'}, {'authorId': '2366236300', 'name': 'SK Alamgir Hossain'}]",2
7a5f710d7ace5aaa60fd92dceab13875a0f83d81,Mooc Course Recommendation System Model with Explainable AI (XAI) Using Content Based Filtering Method,2024,"2024 11th International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)",0,6,"Massive Open Online Course (MOOC) is a type of online course that has been designed and can be accessed by all individuals via the internet. The problem that is often found in MOOCs is the lack of a recommendation system provided by the algorithm of the MOOC. This research is conducted to analyze a recommendation system that applies the Content Based Filtering approach in order to solve the problems that occur. The recommendation system analyzed will function as a media that provides recommendations to users based on their preferences. By utilizing content-based methods, the recommendations given are expected to be exactly what the user wants. The level of explainability of the recommendation system is further emphasized by XAI using ELI5. By getting a concise explanation when a recommendation is given, the system will gain more trust from users for providing an appropriate recommendation. The assessment of the accuracy of the recommendation system model is measured using MAE. By researching this recommendation system using XAI, it is hoped that it can help future systems to improve the quality of the course recommendation system.","[{'authorId': '116143268', 'name': 'Mugi Praseptiawan'}, {'authorId': '2335126297', 'name': 'M. F. D. Muchtarom'}, {'authorId': '2335584303', 'name': 'Nabila Muthia Putri'}, {'authorId': '3161326', 'name': 'A. N. C. Pee'}, {'authorId': '2241629440', 'name': 'Mohd Hafiz Zakaria'}, {'authorId': '32471431', 'name': 'M. C. Untoro'}]",6
15fb60600202fe4a9baf1f6f6dfbdee373e30fe6,Towards Automated Human-Centered Recommendation of Explainable AI Solutions,2024,MAI-XAI@ECAI,0,0,,"[{'authorId': '2329741857', 'name': 'Nils Ole Breuer'}]",1
ea21bfa00ed7718a774334e33edd9f5187e09e5f,ICARE: the principles of Explainable AI in a Context-aware Recommendation APP,2024,EDBT/ICDT Workshops,0,0,,"[{'authorId': '2183783740', 'name': 'Anna Dalla Vecchia'}, {'authorId': '1728772', 'name': 'Barbara Oliboni'}, {'authorId': '2294723982', 'name': 'Elisa Quintarelli'}]",3
edd867ee721a0e91cc0311235b29a295ebb256b2,Cancer omic data based explainable AI drug recommendation inference: A traceability perspective for explainability,2023,Biomedical Signal Processing and Control,21,61,,"[{'authorId': '3442932', 'name': 'Jianing Xi'}, {'authorId': '2155683568', 'name': 'Daniel X M Wang'}, {'authorId': '1860612', 'name': 'Xuebing Yang'}, {'authorId': '2108167911', 'name': 'Wensheng Zhang'}, {'authorId': '2148946979', 'name': 'Qinghua Huang'}]",5
5ef74f7db4d5643ea87e931a6b4d00afb3c26d5e,Role of Explainable AI in Crop Recommendation Technique of Smart Farming,2025,International Journal of Intelligent Systems and Applications,5,56,"Smart farming is undergoing a transformation with the integration of machine learning (ML) and artificial intelligence (AI) to improve crop recommendations. Despite the advancements, a critical gap exists in opaque ML models that need to explain their predictions, leading to a trust deficit among farmers. This research addresses the gap by implementing explainable AI (XAI) techniques, specifically focusing on the crop recommendation technique in smart farming.
An experiment was conducted using a Crop recommendation dataset, applying XAI algorithms such as Local Interpretable Model-agnostic Explanations (LIME), Differentiable InterCounterfactual Explanations (dice_ml), and SHapley Additive exPlanations (SHAP). These algorithms were used to generate local and counterfactual explanations, enhancing model transparency in compliance with the General Data Protection Regulation (GDPR), which mandates the right to explanation.
The results demonstrated the effectiveness of XAI in making ML models more interpretable and trustworthy. For instance, local explanations from LIME provided insights into individual predictions, while counterfactual scenarios from dice_ml offered alternative crop cultivation suggestions. Feature importance from SHAP gave a global perspective on the factors influencing the model's decisions. The study's statistical analysis revealed that the integration of XAI increased the farmers' understanding of the AI system's recommendations, potentially reducing food insufficiency by enabling the cultivation of alternative crops on the same land.","[{'authorId': '2203839392', 'name': 'Yaganteeswarudu Akkem'}, {'authorId': '2291195807', 'name': 'S. K. Biswas'}, {'authorId': '2280233508', 'name': 'Aruna Varanasi'}]",3
a5cfbc209e3387b5e5c9956f347f7d0cbcc2de42,A Data-Driven Crop Recommendation System with Explainable AI for Precision Agriculture,2025,2025 Third International Conference on Augmented Intelligence and Sustainable Systems (ICAISS),0,6,"In this paper, we presented a crop recommendation system for precision agriculture that is based on the machine learning models providing crop recommendations based on the environmental and soil context. Introducing Explainable AI (XAI) in the designed system using LIME, we hope to enhance the level of trust to its recommendations through the understanding of the process which goes into each of them by the farmers. This has been made possible by the system’s architecture that consists of backend in Flask and frontend in React. The system was subjected to a rigorous assessment of its reliability and efficiency once developed to obviate measurement errors and ensure its effectiveness and efficiency. Moreover, the feedback on the integration of XAI also reveals an enhancement of interpretability by demonstrating the explanations of such features as pH of the soil, temperature, and moisture of the soil to have a positive effect to the system. Besides improving the chances of accurate crop yields estimation, this approach enables an accurate assessment of farming inputs with regards to sustainability.","[{'authorId': '2368459566', 'name': 'Moduguri Karthik'}, {'authorId': '2368458301', 'name': 'Ankenapalle Nandini'}, {'authorId': '2368459363', 'name': 'Pochamreddy Venkata Vedha'}, {'authorId': '2368457731', 'name': 'A. Raaga Latha'}, {'authorId': '2368457901', 'name': 'Balaji K V'}, {'authorId': '9425580', 'name': 'Akey Sungheetha'}]",6
bae1214526f2dc712546fa4b887683bc0baafa3f,An Explainable AI Framework for Diabetic Foot Ulcer Detection and Personalized Footwear Recommendation,2025,"2025 2nd International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)",0,14,"Diabetic Foot Ulcers (DFUs) are a major complication of diabetes, often leading to infections, hospitalizations, and amputations if not detected early. This study proposes an integrated deep learning and IoT-based system for the early detection, classification, and prevention of DFUs. An EfficientNetB0 model is utilized to classify DFU images with high accuracy, while plantar foot type classification is performed using WaveNet, ResNet, and an Attention Mechanism to analyze plantar pressure data. For personalized footwear recommendation, a Decision Tree model is employed, enhanced with Explainable AI techniques (SHAP, LIME, and Grad-CAM) to provide transparency in decision-making. The system captures real-time foot health parameters using multiple IoT sensors (pressure, temperature, heart rate, gyroscope) embedded in smart footwear, enabling continuous monitoring and risk prediction. The developed models show promising results in improving early DFU diagnosis, personalized prevention strategies, and clinical decision support. Future enhancements aim to scale the solution with larger datasets, real-time cloud integration, and clinical validation. This research provides a novel, explainable, and wearable AI-driven framework for improving diabetic foot care outcomes.","[{'authorId': '2242945328', 'name': 'P. Velvizhy'}, {'authorId': '2368460223', 'name': 'Pavishek Tv'}, {'authorId': '2335125910', 'name': 'V. A'}, {'authorId': '2368461174', 'name': 'Naveen Kumar T'}, {'authorId': '2368450138', 'name': 'Sharath J'}, {'authorId': '2350021677', 'name': 'Matan P'}]",6
3c97967c62cffe70d3880c03d05917596a7f59a1,Interpretability-driven Course Recommendation: A Random Forest Approach with Explainable AI,2025,Recent Advances in Electrical &amp; Electronic Engineering (Formerly Recent Patents on Electrical &amp; Electronic Engineering),0,0,"

In the current educational landscape, universities provide a vast array of
diverse and obscure courses. However, the primary challenge students face is decision overload,
as the abundance of options can make it difficult to choose courses that align with their interests,
career aspirations, and strengths. The development and application of proper course suggestion
systems might assist students in overcoming the challenges associated with choosing the right
courses. However, these recommendation systems still require improvement to address explainability
issues, security issues, and cold start. Our analysis reveals that there is limited research addressing
how course recommendation systems can assist 12th-grade students in selecting courses
for higher education. Therefore, this study presents a novel personalized recommendation system,
namely “Interpretability-Driven Course Recommendation: A Random Forest Approach with Explainable
AI”, that chooses the top-3 courses for students based on their intermediate class grades/-
marks. This research provides accurate and interpretable recommendations using the Random Forest
algorithm with Explainable AI that ensures transparency by explaining why a particular course
is recommended.



The system uses a Random Forest classifier with SHAP (SHapley Additive exPlanations)
to forecast the top-3 courses with the greatest expected scores for appropriateness. The system
uses SHAP (SHapley Additive exPlanations) values to integrate Explainable AI (XAI) to guarantee
openness and trust. The proposed model solves the cold start problem and provides data security.



In terms of precision (0.90), recall (0.92), and F1-score (0.92), Random Forest fared better
than any other classifier. By combining predictions from several decision trees, its ensemble
learning technique increases stability, decreases overfitting, and improves generalization across a
broad range of input data patterns.



A major drawback of conventional black-box machine learning models is their lack
of transparency, which is addressed by the incorporation of SHAP into the course recommendation
system. In this model, students get suggestions in addition to information on which topic
scores had the biggest influence on their choice.



This study presents a practical and interpretable course recommendation system designed
for 12th-grade students transitioning into higher education. Through the use of Explainable
AI-enhanced Random Forest, the model provides precise, transparent, and customized recommendations.
It addresses the main drawbacks of conventional systems, such as their lack of explainability
and their cold start problems. The model's excellent precision, recall, and F1-score indicate its
superior performance, which makes it a useful tool for student guidance and academic advice. For
even more thorough recommendations, future research may consider combining long-term academic
objectives, aptitude test results, and student interests.
","[{'authorId': '117082887', 'name': 'Tasleem Nizam'}, {'authorId': '2320928600', 'name': 'Sherin Zafar'}, {'authorId': '2576047', 'name': 'S. Biswas'}, {'authorId': '2320926588', 'name': 'Imran Hussain'}]",4
d4dc21b3d90eb75474a3d19b123ca08e067484ec,Personalized Cancer Drug Recommendation using GAN and Explainable AI,2025,"2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)",0,15,"Through more accurate medication recommendations and increased treatment efficacy, machine learning has improved personalized cancer therapy. To support the treatment recommendation system for lung cancer, the study uses a Conditional Tabular Generative Adversarial Network (CTGAN) to generate synthetic patient data. Machine learning algorithms process data to recommend appropriate medications while LightGBM demonstrates superior performance to XGBoost by achieving 0.82 accuracy. Explainable AI (XAI) techniques produce greater transparency while supplying doctors with expanded treatment information. This approach aims to enhance patient treatment outcomes while establishing trust in AI-driven medical advice.","[{'authorId': '2364206831', 'name': 'A. Apoorva'}, {'authorId': '2364207517', 'name': 'Farha Afreen I'}, {'authorId': '2364195216', 'name': 'E. Suganya'}]",3
d206a64c96e72f06c7178b029e4c45febfaaf548,Improving Course Recommendation Systems with Explainable AI: LLM-Based Frameworks and Evaluations,2025,Educational Data Mining,0,0,,"[{'authorId': '2380412991', 'name': 'Jiawei Li'}, {'authorId': '2380388771', 'name': 'Qianru Lyu'}, {'authorId': '2260654913', 'name': 'Wei Qiu'}, {'authorId': '2192484611', 'name': 'Andy W. H. Khong'}]",4
1f79e0f9b805dd312ba02326eea88ce3490685a9,Sentiment-Aware and Explainable AI-Based Cross-Domain Recommendation System,2025,IJARCCE,0,0,,[],0
a617f4d620192b8c1ae64876f6c1a8d2fffdf1f4,Soil-Specific Crop Recommendation with Explainable AI: The XGB-FuzzyIncNet Approach,2025,Communications in Soil Science and Plant Analysis,0,15,,"[{'authorId': '2375604029', 'name': 'S. Shenbagavadivu'}, {'authorId': '2375606467', 'name': 'M. Senthil Kumar'}]",2
567816e81dc457e982147f2ca44b771070e1380c,Deep Learning Based Plant Disease Classification With Explainable AI and Mitigation Recommendation,2021,IEEE Symposium Series on Computational Intelligence,6,0,"Plants show visible symptoms of getting infected with a disease. Presently an experienced plant pathologist can diagnose the condition through visual inspection of disease-affected plants. However, manual visualization is time-consuming and depends on the plant pathologist's expertise in identifying plant disease. Hence this problem can be solved by a computer-aided diagnostic system with artificial intelligence (CADS-AI). This system will aid in improving and protecting the yield of the plant, but it lacks trust as the existing system is not flawless. Hence, in this research work, a plant disease classification with an explainable AI pipeline is developed which ensures trust in the CADS solution. Furthermore, an expert recommendation system will act as an alternative to expert plant pathologists. Tomato leaf diseases data from the PlantVillage dataset is used in the proposed solution. Transfer learning technique was adopted in training deep neural network models with original and augmented data of 16,684 and 53,476 images respectively. The best model for the dataset was efficientNet B5 with best F1 score accuracy of 0.9842 and 0.9930. The predicted output of B5 was interpreted with explainable AI techniques and validated using YOLOv4. Inference of the proposed solution was a client-server interface where end-users can upload infected leaf images via mobile phones or web browsers. This entire system was tested in real-time with 250 volunteers with 4G mobile network or 100 MBPS wifi. The average throughput time of the system is around 4.3 seconds.","[{'authorId': '2285338084', 'name': 'C. Arvind'}, {'authorId': '2044359387', 'name': 'Aditi Totla'}, {'authorId': '153116789', 'name': 'Tanisha Jain'}, {'authorId': '144122679', 'name': 'Nandini Sinha'}, {'authorId': '47686112', 'name': 'R. Jyothi'}, {'authorId': '2151210320', 'name': 'K. Aditya'}, {'authorId': '2151209363', 'name': 'Keerthan'}, {'authorId': '2341490993', 'name': 'Mohammed Farhan'}, {'authorId': '2151210243', 'name': 'G. Sumukh'}, {'authorId': '2151208174', 'name': 'Guruprasad Ak'}]",10
328e83f14b3258eef00605b086a335e663a17bd7,Analyzing and assessing explainable AI models for smart agriculture environments,2024,Multim. Tools Appl.,28,17,"We analyze a case study in the field of smart agriculture exploiting Explainable AI (XAI) approach, a field of study that aims to provide interpretations and explanations to the behaviour of AI systems. The study regards a multiclass classification problem on the Crop Recommendation dataset. The original task is the prediction of the most adequate crop, according to seven features. In addition to the predictions, two of the most well-known XAI approaches have been used in order to obtain explanations and interpretations of the behaviour of the models: SHAP ( SH apley A dditive Ex P lanations), and LIME (Local Interpretable Model-Agnostic Explanations). Both packages provide easy-to-understand visualizations that allow common users to understand explanations of single predictions even without going into the mathematical details of the algorithms. Within the scientific community criticisms have been raised against these approaches, and recently some papers brought to light some weaknesses. However, the two algorithms are among the most popular in XAI and are still considered points of reference for this field of study.","[{'authorId': '2182628938', 'name': 'Andrea Cartolano'}, {'authorId': '2237999539', 'name': 'Alfredo Cuzzocrea'}, {'authorId': '2237999771', 'name': 'Giovanni Pilato'}]",3
9784066a2acf67fcb3aa4eecddfe02415dec8c20,Explainable AI In E-Commerce: Enhancing Trust And Transparency In AI-Driven Decisions,2024,Innovatech Engineering Journal,13,0,"This study explores the transformative role of Explainable Artificial Intelligence (XAI) in e-commerce, focusing on its potential to enhance consumer trust, transparency, and regulatory compliance. Through a systematic review of 42 peer-reviewed articles, this research examines the applications, challenges, and limitations of XAI techniques such as SHAP (Shapley Additive Explanations), LIME (Local Interpretable Model-Agnostic Explanations), and other interpretability frameworks in consumer-facing AI systems. The findings reveal that XAI significantly improves user trust and satisfaction by providing interpretable explanations for AI-driven decisions in areas like recommendation engines, fraud detection, and dynamic pricing. However, critical gaps remain, including the scalability of XAI methods for handling large datasets, their limited capacity to address systemic biases, and the need for personalized, user-centric explanations tailored to diverse audiences. The study also highlights the role of XAI in ensuring compliance with regulations such as GDPR and CCPA, showcasing its dual impact on operational transparency and legal adherence. By identifying these strengths and gaps, this research contributes to a deeper understanding of XAI’s potential and provides valuable insights for its effective integration into e-commerce platforms. These findings underscore the necessity of advancing XAI methodologies to meet the evolving demands of the digital marketplace.","[{'authorId': '2341010971', 'name': 'Malay Sarkar'}]",1
1d4acddc281471cc1f363c9d1f402d19e7ded77a,AI Trust: Can Explainable AI Enhance Warranted Trust?,2023,Human Behavior and Emerging Technologies,23,39,"Explainable artificial intelligence (XAI), known to produce explanations so that predictions from AI models can be understood, is commonly used to mitigate possible AI mistrust. The underlying premise is that the explanations of the XAI models enhance AI trust. However, such an increase may depend on many factors. This article examined how trust in an AI recommendation system is affected by the presence of explanations, the performance of the system, and the level of risk. Our experimental study, conducted with 215 participants, has shown that the presence of explanations increases AI trust, but only in certain conditions. AI trust was higher when explanations with feature importance were provided than with counterfactual explanations. Moreover, when the system performance is not guaranteed, the use of explanations seems to lead to an overreliance on the system. Lastly, system performance had a stronger impact on trust, compared to the effects of other factors (explanation and risk).","[{'authorId': '2264821615', 'name': 'Regina de Brito Duarte'}, {'authorId': '144106225', 'name': 'Filipa Correia'}, {'authorId': '2252416777', 'name': 'P. Arriaga'}, {'authorId': '2264824312', 'name': 'Ana Paiva'}]",4
65076655c75c7f9664841817a3e8db3fb1421705,CLARUS: An Interactive Explainable AI Platform for Manual Counterfactuals in Graph Neural Networks,2023,bioRxiv,38,62,"Background Lack of trust in artificial intelligence (AI) models in medicine is still the key blockage for the use of AI in clinical decision support systems (CDSS). Although AI models are already performing excellently in systems medicine, their black-box nature entails that patient-specific decisions are incomprehensible for the physician. This is especially true for very complex models such as graph neural networks (GNNs), a common state-of-the-art approach to model biological networks such as protein-protein-interaction graphs (PPIs) to predict clinical outcomes. The aim of explainable AI (XAI) algorithms is to “explain” to a human domain expert, which input features, such as genes, influenced a specific recommendation. However, in the clinical domain, it is essential that these explanations lead to some degree of causal understanding by a clinician in the context of a specific application. Results We developed the CLARUS platform, aiming to promote human understanding of GNN predictions by allowing the domain expert to validate and improve the decision-making process. CLARUS enables the visualisation of the patient-specific biological networks used to train and test the GNN model, where nodes and edges correspond to gene products and their interactions, for instance. XAI methods, such as GNNExplainer, compute relevance values for genes and interactions. The CLARUS graph visualisation highlights gene and interaction relevances by color intensity and line thickness, respectively. This enables domain experts to gain deeper insights into the biological network by identifying the most influential sub-graphs and molecular pathways crucial for the decision-making process. More importantly, the expert can interactively alter the patient-specific PPI network based on the acquired understanding and initiate re-prediction or retraining. This interactivity allows to ask manual counterfactual questions and analyse the resulting effects on the GNN prediction. Conclusion To the best of our knowledge, we present the first interactive XAI platform prototype, CLARUS, that allows not only the evaluation of specific human counterfactual questions based on user-defined alterations of patient PPI networks and a re-prediction of the clinical outcome but also a retraining of the entire GNN after changing the underlying graph structures. The platform is currently hosted by the GWDG on https://rshiny.gwdg.de/apps/clarus/.","[{'authorId': '2142485909', 'name': 'J. Beinecke'}, {'authorId': '1947785', 'name': 'Anna Saranti'}, {'authorId': '2162998870', 'name': 'Alessa Angerschmid'}, {'authorId': '2152300', 'name': 'B. Pfeifer'}, {'authorId': '2192361873', 'name': 'Vanessa Klemt'}, {'authorId': '47596587', 'name': 'Andreas Holzinger'}, {'authorId': '1731430', 'name': 'Anne-Christin Hauschild'}]",7
576844456ba24457771ad4dbfe0f9c41ca94dbb5,Eye tracking insights into physician behaviour with safe and unsafe explainable AI recommendations,2024,npj Digit. Medicine,5,60,"We studied clinical AI-supported decision-making as an example of a high-stakes setting in which explainable AI (XAI) has been proposed as useful (by theoretically providing physicians with context for the AI suggestion and thereby helping them to reject unsafe AI recommendations). Here, we used objective neurobehavioural measures (eye-tracking) to see how physicians respond to XAI with N = 19 ICU physicians in a hospital’s clinical simulation suite. Prescription decisions were made both pre- and post-reveal of either a safe or unsafe AI recommendation and four different types of simultaneously presented XAI. We used overt visual attention as a marker for where physician mental attention was directed during the simulations. Unsafe AI recommendations attracted significantly greater attention than safe AI recommendations. However, there was no appreciably higher level of attention placed onto any of the four types of explanation during unsafe AI scenarios (i.e. XAI did not appear to ‘rescue’ decision-makers). Furthermore, self-reported usefulness of explanations by physicians did not correlate with the level of attention they devoted to the explanations reinforcing the notion that using self-reports alone to evaluate XAI tools misses key aspects of the interaction behaviour between human and machine.","[{'authorId': '50600402', 'name': 'M. Nagendran'}, {'authorId': '2854788', 'name': 'Paul Festor'}, {'authorId': '2297296338', 'name': 'Matthieu Komorowski'}, {'authorId': '2242617182', 'name': 'Anthony C. Gordon'}, {'authorId': '2286113515', 'name': 'A. A. Faisal'}]",5
b768705712fe81c7b4bef94dce957552361b82f6,Explainable AI: Definition and attributes of a good explanation for health AI,2024,AI and Ethics,1,82,"
 Proposals of artificial intelligence (AI) solutions based on more complex and accurate predictive models are becoming ubiquitous across many disciplines. As the complexity of these models increases, there is a tendency for transparency and users’ understanding to decrease. This means accurate prediction alone is insufficient to make an AI-based solution truly useful. For the development of healthcare systems, this raises new issues for accountability and safety. How and why an AI system made a recommendation may necessitate complex explanations of the inner workings and reasoning processes. While research on explainable AI (XAI) has grown significantly in recent years, and the demand for XAI in medicine is high, determining what constitutes a good explanation is ad hoc and providing adequate explanations remains a challenge. To realise the potential of AI, it is critical to shed light on two fundamental questions of explanation for safety–critical AI such as health-AI that remain unanswered: (1) What is an explanation in health-AI? And (2) What are the attributes of a good explanation in health-AI? In this study and possibly for the first time we studied published literature, and expert opinions from a diverse group of professionals reported from a two-round Delphi study. The research outputs include (1) a proposed definition of explanation in health-AI, and (2) a comprehensive set of attributes that characterize a good explanation in health-AI.","[{'authorId': '3444731', 'name': 'E. Kyrimi'}, {'authorId': '2322503083', 'name': 'Scott McLachlan'}, {'authorId': '2239205423', 'name': 'Jared M. Wohlgemut'}, {'authorId': '2335206', 'name': 'Zane Perkins'}, {'authorId': '1953585', 'name': 'D. Lagnado'}, {'authorId': '2242974990', 'name': 'William Marsh'}, {'authorId': '2326110289', 'name': 'ExAIDSS Expert Group'}]",7
f203ca98725217c2df8b429bf33289a24f59f807,Explainable AI in E-Commerce: Seller Recommendations with Ethnocentric Transparency,2024,Journal of Electrical Systems,0,19,"Personalized seller recommendations are fundamental to enhancing user experiences and increasing sales in e-commerce platforms. Traditional recommendation systems, however, often function as black-box models, offering limited interpretability. This paper explores the integration of Explainable AI (XAI) techniques, particularly Integrated Gradients (IG) and DeepLIFT (Deep Learning Important FeaTures), into a hybrid recommendation system. The proposed approach combines Matrix Factorization (MF) and Graph Neural Networks (GNNs) to deliver personalized and interpretable seller recommendations. Using real-world e-commerce datasets, the study evaluates how different features, such as user interaction history, social connections, and seller reputation contribute to recommendation outcomes. The system addresses the trade-offs between recommendation accuracy and interpretability, ensuring that insights are both actionable and trustworthy. Experimental results demonstrate that the hybrid model achieves substantial improvements in precision, recall, and F1-score compared to standalone MF and GNN-based approaches. Moreover, Integrated Gradients and DeepLIFT provide users with clear and intuitive explanations of the recommendation process, fostering trust in the system. This paper also introduces a comprehensive feature attribution analysis to quantify the impact of key factors, including behavioral patterns and network influence, on recommendation decisions. A comparative evaluation with state-of-the-art neural recommendation models highlights the effectiveness of the proposed system in balancing performance with interpretability. Finally, the study discusses future enhancements, such as incorporating explainability techniques tailored for multimodal data, employing reinforcement learning for adaptive personalization, and extending the model to handle dynamic user preferences. These findings underscore the importance of transparent, user-focused AI in driving innovation in e-commerce recommendation systems.","[{'authorId': '2362775215', 'name': 'Tauqeer Akhtar'}]",1
b451f6dde2e4b9b6d72b892a4768228c577ec4ac,Recommender Systems in Financial Trading: Using machine-based conviction analysis in an explainable AI investment framework,2024,,2,39,"Traditionally, assets are selected for inclusion in a portfolio (long or short) by human analysts. Teams of human portfolio managers (PMs) seek to weigh and balance these securities using optimisation methods and other portfolio construction processes. Often, human PMs consider human analyst recommendations against the backdrop of the analyst's recommendation track record and the applicability of the analyst to the recommendation they provide. Many firms regularly ask analysts to provide a""conviction""level on their recommendations. In the eyes of PMs, understanding a human analyst's track record has typically come down to basic spread sheet tabulation or, at best, a""virtual portfolio""paper trading book to keep track of results of recommendations. Analysts' conviction around their recommendations and their""paper trading""track record are two crucial workflow components between analysts and portfolio construction. Many human PMs may not even appreciate that they factor these data points into their decision-making logic. This chapter explores how Artificial Intelligence (AI) can be used to replicate these two steps and bridge the gap between AI data analytics and AI-based portfolio construction methods. This field of AI is referred to as Recommender Systems (RS). This chapter will further explore what metadata that RS systems functionally supply to downstream systems and their features.","[{'authorId': '2296991022', 'name': 'Alicia Vidler'}]",1
fde09ebed89633eaed16c857147bb2123c77d58a,When Post Hoc Explanation Knocks: Consumer Responses to Explainable AI Recommendations,2023,Journal of Interactive Marketing,18,49,"Artificial intelligence (AI) recommendations are becoming increasingly prevalent, but consumers are often reluctant to trust them, in part due to the “black-box” nature of algorithm-facilitated recommendation agents. Despite the acknowledgment of the vital role of interpretability in consumer trust in AI recommendations, it remains unclear how to effectively increase interpretability perceptions and consequently enhance positive consumer responses. The current research addresses this issue by investigating the effects of the presence and type of post hoc explanations in boosting positive consumer responses to AI recommendations in different decision-making domains. Across four studies, the authors demonstrate that the presence of post hoc explanations increases interpretability perceptions, which in turn fosters positive consumer responses (e.g., trust, purchase intention, and click-through) to AI recommendations. Moreover, they show that the facilitating effect of post hoc explanations is stronger in the utilitarian (vs. hedonic) decision-making domain. Further, explanation type modulates the effectiveness of post hoc explanations such that attribute-based explanations are more effective in enhancing trust in the utilitarian decision-making domain, whereas user-based explanations are more effective in the hedonic decision-making domain.","[{'authorId': '2145780744', 'name': 'Changdong Chen'}, {'authorId': '2271898801', 'name': 'Allen Ding Tian'}, {'authorId': '2271916495', 'name': 'Ruochen Jiang'}]",3
88bd41850a54c95e4b35a58f3f11ea4053a33bd2,ERS: An Explainable Research Paper Recommendation System for User-Centric Discovery with LIME,2024,2024 5th IEEE Global Conference for Advancement in Technology (GCAT),0,29,"In the era of growing academic research, the challenge of efficiently discovering relevant and valuable papers persists, exacerbated by the ever-growing volume of publications. Traditional recommendation systems, while offering assistance, often lack transparency, leaving users in the dark about the rationale behind suggestions. This research proposes an Explainable Research Paper Recommendation System (ERS) that leverages the Locally Interpretable Model-agnostic Explanations (LIME) algorithm. Our ERS personalizes recommendations by analyzing user profiles, research interests, and citation history. Uniquely, ERS offers transparent explanations for each recommendation, utilizing LIME to highlight the key factors influencing each suggested paper. This fosters user trust and empowers researchers with a deeper understanding of the recommendation process. Furthermore, the paper explores the potential of ERS to enhance academic collaboration, knowledge-sharing, and scalability, while contributing to the advancement of Explainable AI research. Ultimately, by delivering a positive user experience through relevant recommendations and clear explanations, such as systems can empower researchers to navigate the vast ocean of academic literature.","[{'authorId': '2305785220', 'name': 'Parvathy P Nair'}, {'authorId': '2305783684', 'name': 'Surabhi Sudhan'}, {'authorId': '2305787515', 'name': 'M. Thushara'}]",3
ea76dce92072b9c84d22d94460e91e8d14b4b72c,A Reusable Model-agnostic Framework for Faithfully Explainable Recommendation and System Scrutability,2023,ACM Trans. Inf. Syst.,14,98,"State-of-the-art industrial-level recommender system applications mostly adopt complicated model structures such as deep neural networks. While this helps with the model performance, the lack of system explainability caused by these nearly blackbox models also raises concerns and potentially weakens the users’ trust in the system. Existing work on explainable recommendation mostly focuses on designing interpretable model structures to generate model-intrinsic explanations. However, most of them have complex structures, and it is difficult to directly apply these designs onto existing recommendation applications due to the effectiveness and efficiency concerns. However, while there have been some studies on explaining recommendation models without knowing their internal structures (i.e., model-agnostic explanations), these methods have been criticized for not reflecting the actual reasoning process of the recommendation model or, in other words, faithfulness. How to develop model-agnostic explanation methods and evaluate them in terms of faithfulness is mostly unknown. In this work, we propose a reusable evaluation pipeline for model-agnostic explainable recommendation. Our pipeline evaluates the quality of model-agnostic explanation from the perspectives of faithfulness and scrutability. We further propose a model-agnostic explanation framework for recommendation and verify it with the proposed evaluation pipeline. Extensive experiments on public datasets demonstrate that our model-agnostic framework is able to generate explanations that are faithful to the recommendation model. We additionally provide quantitative and qualitative study to show that our explanation framework could enhance the scrutability of blackbox recommendation model. With proper modification, our evaluation pipeline and model-agnostic explanation framework could be easily migrated to existing applications. Through this work, we hope to encourage the community to focus more on faithfulness evaluation of explainable recommender systems.","[{'authorId': '2284763197', 'name': 'Zhichao Xu'}, {'authorId': '2029235362', 'name': 'Hansi Zeng'}, {'authorId': '2110449137', 'name': 'Juntao Tan'}, {'authorId': '2011378', 'name': 'Zuohui Fu'}, {'authorId': '1591136873', 'name': 'Yongfeng Zhang'}, {'authorId': '144922928', 'name': 'Qingyao Ai'}]",6
b4151feb1dd2f06888027a6db96369cf0b483bd9,Integration of Explainable Artificial Intelligence (XAI) in the Development of Disease Prediction and Medicine Recommendation System,2024,2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI),1,11,"The contemporary healthcare landscape necessitates innovative solutions to improve transparency and understanding in medical decision-making. This paper proposes an advanced medicine recommendation system and a robust disease prediction model integrating explainable AI (XAI). Recognizing the prevalence of misinformation and the urgent need for user-friendly applications, the system empowers users to manage their health proactively. It can be extended beyond common diseases, encompassing rare diseases, and employs XAI algorithms, specifically SHAP and LIME, to enhance transparency. The system incorporates Random Forest Classifier and Decision Tree models, showcasing high accuracy and robustness. The explanation models contribute to user understanding, while performance metrics offer insights into model strengths and generalization abilities. Figures depict SHAP outputs for Decision Tree and Random Forest models, emphasizing transparency in medical predictions. This proposed system addresses critical healthcare challenges, fostering informed decision-making and user trust.","[{'authorId': '2298064946', 'name': 'Joel Mathew'}, {'authorId': '2298062803', 'name': 'Richie Suresh Koshy'}, {'authorId': '2298090592', 'name': 'Dr. R. Chitra'}, {'authorId': '2298061499', 'name': 'Caleb Stephen'}]",4
62635461b44d9ff7901fbce982a5c1bbccfaf2b0,"Explainable AI: A Brief Survey on History, Research Areas, Approaches and Challenges",2019,Natural Language Processing and Chinese Computing,518,24,,"[{'authorId': '2724114', 'name': 'Feiyu Xu'}, {'authorId': '1781790', 'name': 'H. Uszkoreit'}, {'authorId': '3165307', 'name': 'Yangzhou Du'}, {'authorId': '2088400146', 'name': 'Wei Fan'}, {'authorId': '144060462', 'name': 'Dongyan Zhao'}, {'authorId': '145254043', 'name': 'Jun Zhu'}]",6
173ee66ebf96b6767aa1999bc4310dc7c39d40f0,Explainable AI at Work! What Can It Do for Smart Agriculture?,2022,IEEE International Conference on Multimedia Big Data,8,39,"Explainable AI (XAI) is gaining the momentum, at now. While the idea is to apply it in different scenarios, including medicine, business analytics, genomics computing and so forth, in this paper we focus the attention on another emerging case, represented by so called smart agricolture. In this paper, we propose the application of some well-known XAI tools on top of the Crop Recommendation dataset. Our research efforts also involve the sensitivity analysis of retrieved results.","[{'authorId': '2182628938', 'name': 'Andrea Cartolano'}, {'authorId': '145046124', 'name': 'A. Cuzzocrea'}, {'authorId': '1708205', 'name': 'G. Pilato'}, {'authorId': '32715613', 'name': 'G. Grasso'}]",4
ea590185f9661c311f28b9b71d6902e6ca651a2a,Developing Explainable AI Models for Personalized Treatment Recommendations Using LLMs and EHR Data,2025,2025 International Conference on Computing Technologies & Data Communication (ICCTDC),0,36,"Personalized treatment recommendations remain critical for better outcomes and optimization of healthcare delivery. This paper discusses an explainable AI framework for personalized treatment recommendation using large language models in conjunction with EHR data. Using the publicly available dataset MIMIC-III, this approach integrates into a unique analysis advanced natural language processing capabilities of LLMs, interpreting complex clinical narratives along with structured data. We implement explainability techniques that provide insight into the decision-making process of the AI model to ensure transparency and trust. The results show improved accuracy and clinician satisfaction compared to traditional models, underlining the potential of explainable AI in personalized medicine. This work emphasizes the need to combine large datasets with state-of-the-art AI technologies to enable trustworthy and effective healthcare solutions.","[{'authorId': '2332938567', 'name': 'Sukanth Korkanti'}]",1
230de5e607efa30416bab29db654f9e14415b6ad,An Explainable AI Framework for Online Diabetes Risk Prediction with a Personalized Chatbot Assistant,2025,Electronics,0,14,"Background and Objective: Diabetes is a prevalent chronic disease that presents considerable health risks, making prompt diagnosis and treatment essential to avert complications. Traditional Artificial Intelligence (AI) models for diabetes prediction often operate as black boxes. A major issue caused by this is that black boxes lack interpretability, which impacts their effectiveness in clinical use cases. We introduce a novel online recommendation framework using explainable AI (XAI) to predict type II diabetes risk and provide clear, actionable analyses with a personalized chatbot assistant. Methods: To make the model, we chose the CatBoost classifier and SHapley Additive exPlanations (SHAP) due to their ability to provide accurate predictions. Using those tools, we analyzed 16 individual risk factors from a dataset of 520 patients. We applied the Synthetic Minority Over-sampling Technique (SMOTE) to reduce the effect of data imbalance. We also developed an interactive interface that allows users to input data, visualize personalized risk profiles, and understand the driving factors behind predictions. Finally, large language models (LLMs) were integrated into the interface for patient-specific recommendations for improving health and lifestyle through a personalized chatbot assistant. Results: The model demonstrated great predictive performance, with an Area Under the ROC Curve (AUC) of 0.99, a Cohen Kappa score of 0.978, and an F1 score of 0.99. For the minority class, SMOTE application improved performance metrics, resulting in an AUC of 0.98 and an F1 score of 0.91 for female patients. Conclusions: This study proposes an explainable AI framework for predicting diabetes risk online and providing patient-specific advice through a personalized chatbot assistant. This will help to facilitate better decision-making and improved management of diabetes risk.","[{'authorId': '2335970300', 'name': 'Ehesan Maimaitijiang'}, {'authorId': '2382197947', 'name': 'Muyesaier Aihaiti'}, {'authorId': '2335035043', 'name': 'Yasin Mamatjan'}]",3
f9f4cdb0ff91640d0fb28025e91b89758010da74,A Methodological Framework for Business Decisions with Explainable AI and the Analytic Hierarchical Process,2025,Processes,3,37,"In today’s data-driven business landscape, effective and transparent decision making becomes relevant to maintain a competitive advantage over the competition, especially in customer service in B2B environments. This study presents a methodological framework that integrates Explainable Artificial Intelligence (XAI), C-means clustering, and the Analytic Hierarchical Process (AHP) to improve strategic decision making in business environments. The framework addresses the need to obtain interpretable information from predictions based on machine learning processes and the prioritization of key factors for decision making. C-means clustering enables flexible customer segmentation based on interaction patterns, while XAI provides transparency into model outputs, allowing support teams to understand the factors influencing each recommendation. The AHP is then applied to prioritize criteria within each customer segment, aligning support actions with organizational goals. Tested with real customer interaction data, this integrated approach proved effective in accurately segmenting customers, predicting support needs, and optimizing resource allocation. The combined use of XAI and the AHP ensures that business decisions are data-driven, interpretable, and aligned with the company’s strategic objectives, making this framework relevant for companies seeking to improve their customer service in complex B2B contexts. Future research will explore the application of the proposed model in different business processes.","[{'authorId': '2137515097', 'name': 'Gabriel Marín Díaz'}, {'authorId': '2320628073', 'name': 'Raquel Gómez Medina'}, {'authorId': '2320631933', 'name': 'José Alberto Aijón Jiménez'}]",3
77c014c618465a90e00db4347eb04ce6d5f1dbf3,Optimizing AI System Security: An Ecosystem Recommendation to Socio-Technical Risk Management,2024,AHFE International,1,0,"Given the sophistication of adversarial machine learning (ML) attacks on Artificial Intelligence (AI) systems, enhanced security frameworks that integrate human factors into risk assessments are critical. This paper presents a comprehensive methodology combining cybersecurity, cyberpsychology, and AI to address human-related aspects of these attacks. It introduces an AI system security optimization ecosystem to help security officers protect AI systems against various attacks, including poisoning, evasion, extraction, and inference. The risk management approach enhances NIST and ENISA frameworks by incorporating socio-technical aspects of adversarial ML threats. By creating digital clones and using explainable AI (XAI) techniques, the human elements of attackers are integrated into security risk management. An innovative conversational agent is proposed to include defenders’ perspectives, advancing the design and deployment of secure AI systems and guiding future certification schemes.","[{'authorId': '51193905', 'name': 'Kitty Kioskli'}, {'authorId': '2329775487', 'name': 'Antonios Ramfos'}, {'authorId': '2330059715', 'name': 'Steve Taylor'}, {'authorId': '2140688864', 'name': 'Leandros A. Maglaras'}, {'authorId': '2310530389', 'name': 'Ricardo Lugo'}]",5
77a09effafd8dcdfe44a709f51c12e86992e6cce,Challenges and Future Perspectives in Explainable AI A Roadmap for New Scholars,2025,"2025 3rd International Conference on Intelligent Systems, Advanced Computing and Communication (ISACC)",0,35,"Machine learning is transforming industries with its capabilities in prediction, recommendation, and decision support. However, the complexity of these systems often hides their inner workings, earning them the label ""black box"" models. This lack of transparency makes it challenging for users to understand both processes and outcomes, highlighting the need for Explainable Artificial Intelligence (XAI) to build trustworthy AI systems.This paper reviews existing literature and contributions on XAI, focusing on core concepts such as interpretability, exploitability, and intelligibility. We emphasize the importance of these aspects for fostering trust, explore the XAI lifecycle, and analyze taxonomies of XAI methods. Additionally, we discuss challenges in the field and propose future research directions, stressing responsible AI development. Our aim is to help novice researchers understand the XAI framework and apply it effectively to their work.","[{'authorId': '2356859639', 'name': 'Chumki Sil'}, {'authorId': '2261924161', 'name': 'Dr. Papri Ghosh'}, {'authorId': '2353404768', 'name': 'Subhram Das'}, {'authorId': '122078277', 'name': 'Md Ashifuddin Mondal'}]",4
01dedd96bd2d696e3d93582438f5e44a174064bd,Personality-based tailored explainable recommendation for trustworthy smart learning system in the age of artificial intelligence,2023,Smart Learning Environments,7,52,"In the age of artificial intelligence (AI), trust in AI systems is becoming more important. Explainable recommenders, which explain why an item is recommended, have recently been proposed in the field of learning technology to improve transparency, persuasiveness, and trustworthiness. However, the methods for generating explanations are limited and do not consider the learner’s cognitive perceptions or personality. This study draws inspiration from tailored intervention research in public health and investigates the effectiveness of personality-based tailored explanations by implementing them for the recommended quizzes in an explainable recommender system. High school students (n = 217) were clustered into three distinct profiles labeled Diligent (n = 77), Fearful (n = 72), and Agreeable (n = 68), based on the Big Five personality traits. The students were divided into a tailored intervention group (n = 106) and a control group (n = 111). In the tailored intervention group, personalized explanations for recommended quizzes were provided based on student profiles, with explanations based on quiz characteristics. In the control group, only non-personalized explanations based on quiz characteristics were provided. An 18-day A/B experiment showed that the tailored intervention group had significantly higher recommendation usage than the control group. These results suggest that personality-based tailored explanations with a recommender approach are effective for e-learning engagement and imply improved trustworthiness of AI learning systems.","[{'authorId': '144664473', 'name': 'Kyosuke Takami'}, {'authorId': '145667538', 'name': 'B. Flanagan'}, {'authorId': '114887272', 'name': 'Yiling Dai'}, {'authorId': '2240147388', 'name': 'Hiroaki Ogata'}]",4
8dd0727601d40d437a182f05548da563ac60d4b3,Explainable Fairness in Recommendation,2022,Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,44,79,"Existing research on fairness-aware recommendation has mainly focused on the quantification of fairness and the development of fair recommendation models, neither of which studies a more substantial problem--identifying the underlying reason of model disparity in recommendation. This information is critical for recommender system designers to understand the intrinsic recommendation mechanism and provides insights on how to improve model fairness to decision makers. Fortunately, with the rapid development of Explainable AI, we can use model explainability to gain insights into model (un)fairness. In this paper, we study the problem ofexplainable fairness, which helps to gain insights about why a system is fair or unfair, and guides the design of fair recommender systems with a more informed and unified methodology. Particularly, we focus on a common setting with feature-aware recommendation and exposure unfairness, but the proposed explainable fairness framework is general and can be applied to other recommendation settings and fairness definitions. We propose a Counterfactual Explainable Fairness framework, called CEF, which generates explanations about model fairness that can improve the fairness without significantly hurting the performance. The CEF framework formulates an optimization problem to learn the ""minimal'' change of the input features that changes the recommendation results to a certain level of fairness. Based on the counterfactual recommendation result of each feature, we calculate an explainability score in terms of the fairness-utility trade-off to rank all the feature-based explanations, and select the top ones as fairness explanations. Experimental results on several real-world datasets validate that our method is able to effectively provide explanations to the model disparities and these explanations can achieve better fairness-utility trade-off when using them for recommendation than all the baselines.","[{'authorId': '152988336', 'name': 'Yingqiang Ge'}, {'authorId': '2110449137', 'name': 'Juntao Tan'}, {'authorId': '2153095583', 'name': 'Yangchun Zhu'}, {'authorId': '35846319', 'name': 'Yinglong Xia'}, {'authorId': '33642939', 'name': 'Jiebo Luo'}, {'authorId': '50152132', 'name': 'Shuchang Liu'}, {'authorId': '2011378', 'name': 'Zuohui Fu'}, {'authorId': '1947101', 'name': 'Shijie Geng'}, {'authorId': '2109968285', 'name': 'Zelong Li'}, {'authorId': '2145038716', 'name': 'Yongfeng Zhang'}]",10
42bfda85ef5e2baf171fce33523e7923f80ba827,Learning to Rank Rationales for Explainable Recommendation,2022,arXiv.org,3,48,"State-of-the-art recommender system (RS) mostly rely on complex deep neural network (DNN) model structure, which makes it difficult to provide explanations along with RS decisions. Previous researchers have proved that providing explanations along with recommended items can help users make informed decisions and improve their trust towards the uninterpretable blackbox system. In model-agnostic explainable recommendation, system designers deploy a separate explanation model to take as input from the decision model, and generate explanations to meet the goal of persuasiveness. In this work, we explore the task of ranking textual rationales (supporting evidences) for model-agnostic explainable recommendation. Most of existing rationales ranking algorithms only utilize the rationale IDs and interaction matrices to build latent factor representations; and the semantic information within the textual rationales are not learned effectively. We argue that such design is suboptimal as the important semantic information within the textual rationales may be used to better profile user preferences and item features. Seeing this gap, we propose a model named Semantic-Enhanced Bayesian Personalized Explanation Ranking (SE-BPER) to effectively combine the interaction information and semantic information. SE-BPER first initializes the latent factor representations with contextualized embeddings generated by transformer model, then optimizes them with the interaction data. Extensive experiments show that such methodology improves the rationales ranking performance while simplifying the model training process (fewer hyperparameters and faster convergence). We conclude that the optimal way to combine semantic and interaction information remains an open question in the task of rationales ranking.","[{'authorId': '2284763197', 'name': 'Zhichao Xu'}, {'authorId': '1475721571', 'name': 'Yi Han'}, {'authorId': '1958895984', 'name': 'Tao Yang'}, {'authorId': '2077430797', 'name': 'Anh Tran'}, {'authorId': '144922928', 'name': 'Qingyao Ai'}]",5
d9ec78a71231900b76b33472937db53189247953,Explainable Rules and Heuristics in AI Algorithm Recommendation Approaches—A Systematic Literature Review and Mapping Study,2023,Computer Modeling in Engineering &amp; Sciences,5,0,,"[{'authorId': '2204989581', 'name': 'Francisco Jos�Garc韆-Pe馻lvo'}, {'authorId': '2204999384', 'name': 'Andrea V醶quez-Ingelmo'}, {'authorId': '2204989567', 'name': 'Alicia Garc韆-Holgado'}]",3
e632f1191b3c261cdae48698c97eb2e9a5a45c78,Explainable recommendation: when design meets trust calibration,2021,World wide web (Bussum),32,105,"Human-AI collaborative decision-making tools are being increasingly applied in critical domains such as healthcare. However, these tools are often seen as closed and intransparent for human decision-makers. An essential requirement for their success is the ability to provide explanations about themselves that are understandable and meaningful to the users. While explanations generally have positive connotations, studies showed that the assumption behind users interacting and engaging with these explanations could introduce trust calibration errors such as facilitating irrational or less thoughtful agreement or disagreement with the AI recommendation. In this paper, we explore how to help trust calibration through explanation interaction design. Our research method included two main phases. We first conducted a think-aloud study with 16 participants aiming to reveal main trust calibration errors concerning explainability in AI-Human collaborative decision-making tools. Then, we conducted two co-design sessions with eight participants to identify design principles and techniques for explanations that help trust calibration. As a conclusion of our research, we provide five design principles: Design for engagement, challenging habitual actions, attention guidance, friction and support training and learning. Our findings are meant to pave the way towards a more integrated framework for designing explanations with trust calibration as a primary goal.","[{'authorId': '2122587261', 'name': 'Mohammad Naiseh'}, {'authorId': '1403767870', 'name': 'Dena Al-Thani'}, {'authorId': '144623186', 'name': 'Nan Jiang'}, {'authorId': '2114136995', 'name': 'Raian Ali'}]",4
a88055f98ea9d5171e8c32ca3199cececf772fdc,A Personalized and Explainable News Recommendation Framework Leveraging User Clickstreams and Content Semantics,2025,Journal of Machine and Computing,0,18,"MindReader introduces an interpretable and personalized news recommender system that integrates clickstream data to summarize transformer-based content semantics and provide token-level biased attribution. Unlike classical recommender systems, which often operate as black boxes, MindReader offers actionable interpretability by utilizing Shapley Additive explanations (SHAP) to reveal the word-level contributions behind each recommendation decision. This model combines both user reading history and article content extraction through a unified framework, incorporating temporal patterns and semantic embeddings. MindReader demonstrates state-of-the-art AUC and coherence scores on real-world news datasets, outperforming several strong baselines, including TF-IDF and neural content models. Human evaluation confirms its superiority. SHAP-based overlays closely align with user attention patterns, while error case analysis highlights resilience against linguistic noise and clickbait content. A key differentiator of MindReader lies in its commitment to not only achieving high performance but also ensuring transparency and trust qualities vital for the deployment of AI in sensitive areas such as journalism, education, and civic communication. This transparency allows users to not only see what content is recommended but also understand the reasoning behind it. In alignment with the UN Sustainable Development Goals (SDG 4 – Quality Education, SDG 9 – Innovation and Infrastructure, and SDG 16 – Strong Institutions), MindReader advocates for an interpretable AI framework for public information dissemination. The architecture proposed in this work offers a scalable, user-centric, and SDG-compliant approach to the implementation of explainable recommended systems.","[{'authorId': '2341762828', 'name': 'Kalyan Chakravarthy N S'}, {'authorId': '2381275892', 'name': 'Muthuramya C'}, {'authorId': '2381719466', 'name': 'Soujanya M'}, {'authorId': '2212450613', 'name': 'Jafar Ali Ibrahim Syed Masood'}, {'authorId': '9116000', 'name': 'Raenu A. L. Kolandaisamy'}]",5
8eac5a4671f8e797e1f7abfae2be4e6c6c99e2bd,A Personalized and Explainable Federated Learning Approach for Recommendation Systems,2025,International Conference on Edge Computing [Services Society],0,31,"The growing adoption of wearable fitness devices and health applications has led to an exponential increase in fitness recommendations. However, privacy concerns remain significant barriers to user trust and regulatory compliance. Federated Learning (FL) offers a privacy-preserving paradigm by training models across decentralized devices without exposing raw data. However, FL introduces new challenges, including data heterogeneity, computational overhead, and the need for explainable AI (XAI). This work presents XFL, an integrated, explainable FL approach for personalized fitness recommendation systems. Our approach integrates FL with XAI techniques, SHAP, and LIME, to enhance transparency and interpretability while preserving privacy. By leveraging global and client-specific explanations, our framework empowers users to understand the rationale behind personalized recommendations, fostering trust and usability. Experimental results demonstrate that XFL performs better than centralized models while maintaining strong privacy guarantees. Furthermore, we evaluated the computational impact of integrating XAI in FL environments, providing insights into the efficiency of different explainability techniques. Our findings contribute to developing user-centric, privacy-aware, and interpretable AI-driven fitness solutions.","[{'authorId': '12147027', 'name': 'Sadi Alawadi'}, {'authorId': '17663383', 'name': 'Feras M. Awaysheh'}, {'authorId': '2376343805', 'name': 'Thambugala Athukoralalage Jayani Sandunka Athukorala'}, {'authorId': '2376345646', 'name': 'Saket Gande'}, {'authorId': '11033180', 'name': 'Fahed Alkhabbas'}]",5
e978c8f33e75917be10447c8c889983755012cc6,AI-Enhanced Cross-Modal Anime Recommendation System with Explainable Deep Learning,2025,2024 International Conference on IT Innovation and Knowledge Discovery (ITIKD),0,21,"This paper presents an innovative approach to anime recommendation systems by integrating multi-modal deep learning with explainable AI techniques. We propose a novel framework that combines visual features, textual content, and user interaction data to create more accurate and interpretable recommendations. Our system addresses key challenges in ex-isting recommendation systems, including the cold-start problem and limited content understanding, through a hybrid architecture that leverages BERT-based natural language processing and convolutional neural networks for visual analysis. Experimental results demonstrate a 27% improvement in recommendation accuracy compared to traditional methods, while providing transparent explanations for recommendations through attention visualization.","[{'authorId': '2321383040', 'name': 'Jayabhaduri Radhakrishnan'}, {'authorId': '2362622459', 'name': 'H. Naga'}, {'authorId': '2362594356', 'name': 'R. G. Vardhan'}, {'authorId': '9425580', 'name': 'Akey Sungheetha'}, {'authorId': '2321686830', 'name': 'K. Dinesh'}, {'authorId': '2353334012', 'name': 'K.Indu Reddy,'}, {'authorId': '2363147176', 'name': 'K. D. Kumar'}, {'authorId': '2362640828', 'name': 'B. C. Reddy'}]",8
af92abaa9460d414559982b07d37ddc582654d6c,"Development of an artificial intelligence-generated, explainable treatment recommendation system for urothelial carcinoma and renal cell carcinoma to support multidisciplinary cancer conferences",2025,medRxiv,5,19,"Background and Objective: Decisions on the best available treatment in clinical oncology are based on expert opinions in multidisciplinary cancer conferences (MCC). Artificial intelligence (AI) could increase evidence-based treatment by generating additional treatment recommendations (TR). We aimed to develop such an AI system for urothelial carcinoma (UC) and renal cell carcinoma (RCC). Methods: Comprehensive data of patients with histologically confirmed UC and RCC who received MCC recommendations in the years 2015 to 2022 were transformed into machine readable representations. Development of a two-step process to train a classifier to mimic TR. Identification of superordinate categories of recommendations followed by specification of detailed TR. Machine learning (CatBoost, XGBoost, Random Forest) and deep learning (TabPFN, TabNet, SoftOrdering CNN, FCN) techniques were trained. Results were measured by F1-scores for accuracy weights. Additionally, clinical trial data for drugs were included. Key Findings and Limitations: AI training was performed with 1617 (UC) and 880 (RCC) MCC recommendations (77 and 76 patient input parameters). AI system generated fully automated TR with excellent F1-scores for UC (e.g. Surgery 0.81, Anti-cancer drug 0.83, Gemcitabine/Cisplatin 0.88) and RCC (e.g. Anti-cancer drug 0.92 Nivolumab 0.78, Pembrolizumab/Axitinib 0.89). Explainability is provided by clinical features and their importance score. TR and explainability were visualized on a dashboard. Main limitations: single-centre and retrospective study. Conclusions and Clinical Implications: First AI-generated explainable TR in UC and RCC with excellent performance results. Potential support tool for high-quality, evidence-based TR in MCC. Study sets global reference standards for AI development in MCC recommendations in clinical oncology.","[{'authorId': '2238763867', 'name': 'G. Duwe'}, {'authorId': '2321582460', 'name': 'D. Mercier'}, {'authorId': '2308244131', 'name': 'V. Kauth'}, {'authorId': '2308244200', 'name': 'K. Moench'}, {'authorId': '1696614145', 'name': 'Vikas Rajashekar'}, {'authorId': '2308244191', 'name': 'Markus Junker'}, {'authorId': '2205389703', 'name': 'A. Dengel'}, {'authorId': '2238765725', 'name': 'A. Haferkamp'}, {'authorId': '6942928', 'name': 'T. Höfner'}]",9
71a2503540229315188d53acd4b2bb1e2f61242c,Towards Explainable AI Using Similarity: An Analogues Visualization System,2019,Interacción,2,8,,"[{'authorId': '33713332', 'name': 'Vinicius Segura'}, {'authorId': '2076970127', 'name': 'Bruna Brandão'}, {'authorId': '8763940', 'name': 'Ana Fucs'}, {'authorId': '2178217', 'name': 'E. V. Brazil'}]",4
e42b9cf5367c882381746dac1767ee747a97217a,Evolution of recommendation systems in the age of Generative AI,2025,International Journal of Science and Research Archive,2,15,"This article examines the transformative evolution of recommendation systems in the era of Generative AI, exploring how these advanced technologies have revolutionized user experience and business outcomes across digital platforms. The article investigates the transition from traditional rule-based approaches to sophisticated model-based systems, highlighting the impact of deep learning technologies, explainable AI mechanisms, and multimodal integration. Through comprehensive analysis of recent developments, the article demonstrates how Generative AI has enhanced personalization capabilities, improved recommendation accuracy, and enabled more contextually relevant suggestions while addressing crucial aspects of user privacy and system transparency. The article encompasses various domains, including e-commerce, content streaming, and digital marketplaces, offering insights into both technical advancements and practical implementations of modern recommendation systems.","[{'authorId': '2383992695', 'name': 'Ankur Aggarwal'}]",1
eccaae7d657b436a44c0c2a5693c591454b16c98,"Issues in Explainable AI: Blackboxes, Recommendations, and Levels of Explanation",2019,,0,0,,"[{'authorId': '80378192', 'name': 'Rune Nyrup'}]",1
5c8aec51a60d078b526a55e5b941984ef5111184,AI-Driven Diagnosis and Treatment Recommendation in Healthcare: A Hybrid Deep Learning Framework,2025,International Journal of Scientific Research in Computer Science Engineering and Information Technology,0,8,"The healthcare industry faces persistent challenges in delivering accurate, scalable, and privacy-compliant diagnosis and treatment recommendations. This paper presents an innovative hybrid AI framework that integrates deep learning, federated learning, and reinforcement learning to provide an automated, explainable, and secure medical decision-making system. Unlike traditional AI models, our approach ensures data privacy through differential privacy and secure multi-party computation, while an explainable AI (XAI) module enhances clinician trust by making predictions interpretable. The system dynamically personalizes treatment recommendations using hierarchical reinforcement learning, optimizing chronic disease management. We validated the framework on real-world multi-modal medical datasets, including MIMIC-IV, NIH X-ray, and PubMed Genetics. The system processes multiple data modalities, such as electronic health records (EHRs), medical imaging (X-rays, MRIs), and genetic data, achieving an 18% reduction in diagnostic errors and a 25% improvement in patient outcomes. This research sets a new benchmark in AI-driven healthcare by demonstrating a scalable, privacy-preserving, and clinically interpretable system that can be seamlessly integrated into modern hospital infrastructures.","[{'authorId': '2345852551', 'name': 'Satyanarayana Murthy Polisetty'}]",1
a65b027ff2a70c336f1abbb164ec4bb8f8e5c670,NUTRIC AI: an AI-Assisted Personalized Nutrition Recommendation System,2025,"International Conference on Cryptography, Security and Privacy",0,15,"In this paper, NUTRIC AI is explained as a cutting-edge AI-assisted personal nutrition advisory system that provides personalized dietary guidelines through the review of individual clinical histories, current medical records, and AI-assisted meal planning. Real-time food recognition is performed by the system via Convolutional Neural Networks (CNNs), and Forecasting of Dietary Patterns is achieved using Long Short-Term Memory (LSTM) networks on past records of health information. Generative Networks are utilized to design personalized meal plans as per the specified dietary needs. For transparency and user confidence induction, the system utilizes Explainable AI (XAI) techniques, i.e., SHAP, to incorporate interpretability in its recommendations. Experimental trials depict substantial gains at a 95% accurate meal planning rate and a 92% user acceptance rate, better than existing available nutrition management platforms. The real-time food scoring system, user-specific dietary advice tailored to a user's own health record, and AI-facilitated chatbot interface are the distinguishing strengths of NUTRIC AI. The convergence of these three features places NUTRIC AI at the forefront of the art of personalized nutritional management.","[{'authorId': '2137829175', 'name': 'H. N'}, {'authorId': None, 'name': 'Kavitha S'}, {'authorId': None, 'name': 'Surya Prakash G'}, {'authorId': None, 'name': 'Sandhiya G'}]",4
